{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/services/aas/csw-aas-http.html","text":"","title":"Akka HTTP Adapter (csw-aas-http)"},{"location":"/services/aas/csw-aas-http.html#akka-http-adapter-csw-aas-http-","text":"This library is a security adapter for Akka HTTP server applications. csw-aas uses OpenId Connect for authentication and authorization. The authentication server used by AAS is Keycloak. We recommended that you get familiar with Keycloak’s documentation and configurations to fully leverage this adapter’s features.\nThis adapter provides authentication via security directives such as sGet, sPost, sPut, etc. These directives are used in routing and replace the default get, post, put, etc. directives from Akka HTTP. This allows custom policies to be enforced at the routing level. For authorization, these secure directives accept a wide range of policy expressions. The usage of these directives are described below.","title":"Akka HTTP Adapter (csw-aas-http)"},{"location":"/services/aas/csw-aas-http.html#dependencies","text":"To use the Akka HTTP Adapter (csw-aas-http), add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-aas-http\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/aas/csw-aas-http.html#prerequisites","text":"To run an Akka HTTP server app, which uses this adapter, we need\nThe Location Service running An AAS instance running and registered with the Location Service\nThe Location Service and AAS can be running on different machines. To start the Location Service and AAS server on a local machine, you can make use of the csw-services.sh script.","title":"Prerequisites"},{"location":"/services/aas/csw-aas-http.html#application-configurations","text":"All auth related configurations go inside an auth-config block. There are three configurations applicable for an Akka HTTP server application i.e. realm, client-id & enable-permissions.\nTHe realm has a default value of TMT, if not specified. Ideally all apps in TMT should not have to override this, however it might be useful to override this while testing your app.\nenable-permissions is optional config with a default value of false. Typically, roles are used for authorization and specific permissions are not needed. However, if your Akka HTTP server application uses permission based authorization policies, this config needs to be set to true.\nclient-id is a mandatory configuration which specifies the client ID of the app as per its registration in AAS.\ndisabled is an optional config with default value of false. This flag can be turned on for local development and testing purposes. When turned on, all http requests bypass all security policies. Clients don’t have to pass any token in the requests. It can greatly ease the process of testing business logic without having to go through the process of creating users, managing roles and logging in with user credentials to generate valid access tokens.\nCaution Please use disabled flag with caution. If accidentally turned on in production, confidential data can be compromised\nauth-config {\n  realm = TMT # DEFAULT\n  enable-permissions = false # DEFAULT\n  client-id = demo-cli # REQUIRED\n  disabled = false # DEFAULT\n}","title":"Application Configurations"},{"location":"/services/aas/csw-aas-http.html#building-a-secure-akka-http-server-application","text":"The core of this adapter is the SecurityDirectives class. The recommended way to instantiate SecurityDirectives is as shown below.\nScala object SampleHttpApp extends HttpApp with App {\n\n  implicit val actorSystem: ActorSystem[SpawnProtocol.Command] = typed.ActorSystem(SpawnProtocol(), \"sample-http-app\")\n  implicit val ec: ExecutionContext                            = actorSystem.executionContext\n\n  val locationService = HttpLocationServiceFactory.makeLocalClient\n  val directives      = SecurityDirectives(locationService)\n  import directives._\n\n  override protected def routes: Route = pathPrefix(\"api\") {\n    get {\n      complete(\"SUCCESS\")\n    } ~\n    sPost(RealmRolePolicy(\"admin\")) {\n      complete(\"SUCCESS\")\n    }\n  }\n\n  private val host = \"0.0.0.0\"\n  private val port = 9003\n\n  startServer(host, port)\n}\nImporting everything from security directives is recommended as it imports some implicit methods along with all security directives.\nIn the above example,\nGET http://localhost:9003/api does not use any security directive and hence is accessible to all. POST http://localhost:9003/api uses sPost which is secure directive. This directive takes care of authentication (access token signature and expiration validation). For authorization, it needs an authorization policy. The authorizing policy specifies one or more conditions for request validation.\nIn this instance, the sPost directive has been given a RealmRolePolicy policy with the parameter value admin.\nThis results into following sequence of actions when a request arrives for a secure directive route:\nCheck request header to look for an access token Validate the token signature and expiry Check the token for roles and validate that it has the admin realm role After all the above checks/validations pass, execute the route logic\nIf any of the validations fails, an appropriate HTTP status code is returned to the requester. For authentication failure, 401 is sent and for authorization failure, 403 is sent.\nNote To know more about realm roles, check out the Keycloak documentation","title":"Building a Secure Akka HTTP server application"},{"location":"/services/aas/csw-aas-http.html#authorization-policies","text":"An authorization policy is a way to provide filtering on incoming HTTP requests based on standard rules. Following policies can be applied to protect routes.\nReamRolePolicy ClientRolePolicy PermissionPolicy CustomPolicy CustomPolicyAsync EmptyPolicy","title":"Authorization Policies"},{"location":"/services/aas/csw-aas-http.html#realmrolepolicy","text":"This policy filters requests based on Realm Role. A Realm Role is global and is applicable for all clients within a realm.\nIn the following example, the policy will authorize a request if the user has been assigned the admin role\nScala val routeWithRealmRolePolicy: Route = sGet(RealmRolePolicy(\"admin\")) {\n  complete(\"OK\")\n}","title":"RealmRolePolicy"},{"location":"/services/aas/csw-aas-http.html#clientrolepolicy","text":"Client roles are basically a namespace dedicated to a client. Each client gets its own namespace.\nThis policy filters requests based on Client Role. In the following example, the policy will authorize amrequest if user has been assigned the accounts-admin role for the clientId specified in the configuration.\nScala val routeWithClientRolePolicy: Route = sGet(ClientRolePolicy(\"accounts-admin\")) {\n  complete(\"OK\")\n}","title":"ClientRolePolicy"},{"location":"/services/aas/csw-aas-http.html#permissionpolicy","text":"This policy filters requests based on permissions. It expects the name of the scope and the name of the resource on which the permission is created in AAS.\nIn the following example policy will authorize a request if the user has the appropriate permission associated in AAS which specifies the delete scope for the account resource.\nScala val routeWithPermissions: Route = sDelete(PermissionPolicy(\"delete\", \"account\")) {\n  complete(\"OK\")\n}","title":"PermissionPolicy"},{"location":"/services/aas/csw-aas-http.html#custompolicy","text":"This policy allows custom request filtering based on access token properties. It expects a predicate function which accepts an access token and returns a boolean. If the predicate returns true, it indicates the user is authorized.\nIn the following example, the policy will authorize a request if the user’s given name contains test-user.\nScala val routeWithCustomPolicy: Route = sPost(CustomPolicy(token => token.given_name.contains(\"test-user\"))) {\n  complete(\"OK\")\n}","title":"CustomPolicy"},{"location":"/services/aas/csw-aas-http.html#custompolicyasync","text":"This policy is similar to CustomPolicy, with only difference being that it expects a predicate which returns a Future of Boolean instead of a Boolean. This could be very useful for custom validations which need to make an IO call. For example,\nScala //GET http://[host]:[port]/files?fileId=[fileId]\nval route: Route =\n  path(\"files\" / LongNumber) { fileId =>\n    sGet(CustomPolicyAsync(token => Database.doesUserOwnFile(token.preferred_username, fileId))) {\n      complete(Database.getFileContents(fileId))\n    }\n  }\nThis forms an HTTP route for a secure GET request for the path /files/[fileId] and expects a path parameter of type Long. The async custom policy makes an async database call to check whether the file being requested belongs to the user who made the HTTP request.","title":"CustomPolicyAsync"},{"location":"/services/aas/csw-aas-http.html#emptypolicy","text":"This policy is used this when only authentication is needed but not authorization. EmptyPolicy is an object and not a class like other policies and it does not need any parameters.\nScala val authenticationOnlyRoute: Route = // GET http://[host]:[post]/api\n  path(\"api\") {\n    sGet(EmptyPolicy) {\n      complete(\"OK\")\n    }\n  }","title":"EmptyPolicy"},{"location":"/services/aas/csw-aas-http.html#security-directives","text":"The csw-aas-http adapter supports following secure HTTP verbs:\nName Description sGet Rejects all unauthorized and non-GET requests sPost Rejects all unauthorized and non-POST requests sPut Rejects all unauthorized and non-PUT requests sDelete Rejects all unauthorized and non-DELETE requests sHead Rejects all unauthorized and non-HEAD requests sConnect Rejects all unauthorized and non-CONNECT requests","title":"Security Directives"},{"location":"/services/aas/csw-aas-http.html#using-access-token","text":"A handle of the access token type is given to all secure routes. It is optional to define a parameter for it.\nFor example:\nScala val routeExampleWithToken: Route = sDelete(EmptyPolicy) { token =>\n  parameter(\"entityId\".as[Long]) { entityId =>\n    ThirdPartyService.deleteEntity(entityId, token.preferred_username)\n    complete(s\"user ${token.given_name} ${token.family_name} deleted entity $entityId\")\n  }\n}\n\nval routeExampleWithoutToken: Route = sDelete(EmptyPolicy) {\n  parameter(\"entityId\".as[Long]) { entityId =>\n    ThirdPartyService.deleteEntity(entityId)\n    complete(s\"entity $entityId deleted\")\n  }\n}\nBoth of the above approaches compile and are valid. The access token holds basic information about the user or the client who has made the request.\nNote When disabled flag is true in auth-config, all fields of access token will be set to None\nPlease go through the API documentation to know more about Access Tokens.","title":"Using Access Token"},{"location":"/services/aas/csw-aas-http.html#policy-expressions","text":"So far, we have seen that security directives can accept an authorization policy. It can however also accept an expression of multiple authorization policies. This could be useful to express complex authorization logic. For example:\nScala val routes: Route =\n  sGet(RealmRolePolicy(\"admin\") | CustomPolicy(_.email.contains(\"super-admin@tmt.org\"))) {\n    complete(\"OK\")\n  } ~\n    sPost(ClientRolePolicy(\"finance_user\") & PermissionPolicy(\"edit\")) {\n      complete(\"OK\")\n    }\nNote the | , & operators which help compose an expression. A Policy expression could be more complex than this and can contain braces to group more expressions. For example:\nval policy = policy1 | (policy2 & (policy3 | policy4)) | policy5","title":"Policy Expressions"},{"location":"/services/aas/csw-aas-http.html#directive-composition","text":"Since security directives extend from akka.http.scaladsl.server.Directive, they give you all the benefits of a usual directive. These benefits include being able to label and compose higher level directives.\nWith the help of directive labeling you could write a route like below:\nScala sGet(RealmRolePolicy(\"admin\")) & sGet(ClientRolePolicy(\"sales_admin\"))\nThe same can be achieved via Policy Expressions as shown below\nScala sGet(RealmRolePolicy(\"admin\") & ClientRolePolicy(\"sales_admin\"))\nIf you want to combine two directives and both of them are CSW security directives, we strongly recommend that you use Policy Expressions. The reason for this is that when you combine two CSW security directives, the authentication check happens twice (or multiple times based on how many CSW security directives are combined). Since this was meant to happen only once, it causes performance slowdown. You can however combine CSW security directives with other directives freely without worrying about performance.","title":"Directive Composition"},{"location":"/services/aas/csw-aas-http.html#source-code-for-above-examples","text":"Example http server","title":"Source code for above examples"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/prefix.html","text":"","title":"Prefix in CSW 2"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/prefix.html#prefix-in-csw-2","text":"The use of prefixes has been made more consistent in CSW 2.0.0. Before this update, Prefix was a dot-separated String starting with a subsystem, where the component name was considered to be the last item. The Subsystem was extracted from the String on creation.\ne.g IRIS.imager.filterWheelAssembly => Subsystem: IRIS Component Name: filterWheelAssembly\nThere was some confusion about what was the Prefix: the entire string (IRIS.imager.filterWheelAssembly), or everything except the component name (IRIS.imager)?\nWith CSW 2, we expect this to be less confusing. Prefix is the whole string and is made up of exactly two parts: the subsystem (explicitly as a Subsystem type) and a String component name, where everything after the subsystem is considered to be the component name.\ne.g. IRIS.imager.filterWheelAssembly => Subsystem: IRIS Component Name: imager.filterWheelAssembly\nPrefixes can be still be constructed using a String, but it must have a part at the beginning before the first dot that matches one of the valid TMT subsystems as specified in the Subsystem.scala.\nThis change will have the following effects on your code:","title":"Prefix in CSW 2"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/prefix.html#component-creation","text":"ComponentInfo files now take a Prefix as a String instead of an ambiguous prefix and a component name. Similarly, Container configuration files also reference their components using Prefix instead of component name.\nFor example, in CSW 1:\nname = \"SampleHcd\"\ncomponentType = hcd\nbehaviorFactoryClassName = \"org.tmt.nfiraos.samplehcd.SampleHcdBehaviorFactory\"\nprefix = \"nfiraos.samplehcd\"\nlocationServiceUsage = RegisterOnly\nbecomes in CSW 2:\nprefix = \"nfiraos.samplehcd\"\ncomponentType = hcd\nbehaviorFactoryClassName = \"org.tmt.nfiraos.samplehcd.SampleHcdBehaviorFactory\"\nlocationServiceUsage = RegisterOnly","title":"Component Creation"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/prefix.html#location-service","text":"A ComponentId is now constructed from a Prefix and ComponentType instead of a component name String and a ComponentType. This allows registration of components from different subsystems with the same component name.\nFor example, in CSW 1:\nval hcdConnection = AkkaConnection(ComponentId(\"hcd1\"), ComponentType.HCD)\nbecomes in CSW 2:\nval hcdConnection = AkkaConnection(ComponentId(Prefix(Subsystem.NFIRAOS, \"hcd1\"), ComponentType.HCD))","title":"Location Service"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/prefix.html#alarm-service","text":"Alarms are defined in the Alarm Configuration file using a Prefix instead of a subsystem and component name.\nFor example, in CSW 1:\nalarms: [\n  {\n    subsystem = nfiraos\n    component = tromboneAssembly\n    name = tromboneAxisLowLimitAlarm\n    description = \"Warns when trombone axis has reached the low limit\"\n    location = \"south side\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major, Critical]\n    probableCause = \"the trombone software has failed or the stage was driven into the low limit\"\n    operatorResponse = \"go to the NFIRAOS engineering user interface and select the datum axis command\"\n    isAutoAcknowledgeable = false\n    isLatchable = true\n    activationStatus = Active\n  }\n]\nbecomes in CSW 2:\nalarms: [\n  {\n    prefix = nfiraos.tromboneAssembly\n    name = tromboneAxisLowLimitAlarm\n    description = \"Warns when trombone axis has reached the low limit\"\n    location = \"south side\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major, Critical]\n    probableCause = \"the trombone software has failed or the stage was driven into the low limit\"\n    operatorResponse = \"go to the NFIRAOS engineering user interface and select the datum axis command\"\n    isAutoAcknowledgeable = false\n    isLatchable = true\n    activationStatus = Active\n  }\n]\nAlarmKey also takes Prefix instead of a subsystem and a component name:\nCSW 1:\nval alarmKey = AlarmKey(NFIRAOS, \"trombone\", \"tromboneAxisLowLimitAlarm\")\nbecomes in CSW 2:\nval alarmKey = AlarmKey(Prefix(NFIRAOS, \"trombone\"), \"tromboneAxisLowLimitAlarm\")","title":"Alarm Service"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/prefix.html#logging-service","text":"The construction of a Logging Service is typically done using a LoggerFactory. LoggerFactory now takes a Prefix instead of just a component name String. For components, this is done automatically for you in the framework. This has the additional effect that the Subsystem and Component Name are now added to the log messages.\nFor example, in CSW 1:\n{\"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"LocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\": \"akka://csw-examples-locationServiceClient@10.131.23.195:53618/user/$a\",\n \"class\":\"csw.location.LocationServiceExampleClient\",\n \"file\":\"LocationServiceExampleClientApp.scala\",\n \"line\":149,\n \"message\":\"Result of the find call: None\",\n \"timestamp\":\"2017-11-30T10:58:03.102Z\" }\nbecomes in CSW 2:\n{\"@prefix\":\"CSW.my-component-name\",\n \"@subsystem\":\"CSW\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"LocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53618/user/$a\",\n \"class\":\"csw.location.LocationServiceExampleClient\",\n \"file\":\"LocationServiceExampleClientApp.scala\",\n \"line\":149,\n \"message\":\"Result of the find call: None\",\n \"timestamp\":\"2017-11-30T10:58:03.102Z\"\n }\nAlso, configuring default log levels in application configuration files must be specified using the entire Prefix.\nCSW 1:\ncomponent-log-levels {\n    trombonehcd = debug\n    tromboneassembly = error\n  }\nbecomes in CSW 2:\ncomponent-log-levels {\n    TCS.trombonehcd = debug\n    TCS.tromboneassembly = error\n  }\nor alternatively:\ncomponent-log-levels {\n    TCS {\n        trombonehcd = debug\n        tromboneassembly = error\n    }\n  }\nIn addition, the Prefix and Subsystem classes have been moved from csw.params.core.models to csw.prefix.models, so imports will have be updated.","title":"Logging Service"},{"location":"/technical/location/location-server.html","text":"","title":"Location Server"},{"location":"/technical/location/location-server.html#location-server","text":"","title":"Location Server"},{"location":"/technical/location/location-server.html#introduction","text":"The csw-location-server project contains the main implementation of the Location Service. Think of it as a agent which is running on every machine. Normally one instance of the Location Server will run on each host that is running CSW services (although clients can be configured to use a remote host).","title":"Introduction"},{"location":"/technical/location/location-server.html#design","text":"Main building blocks of location service are captured below, we will go through each one of them in following sections:\nAkka Cluster Conflict Free Replicated Data Types (CRDTs): Shares location information within the network. Akka HTTP DeathWatch Actor\nAbove diagram shows different parts of Location Service and how it will look like in TMT environment. On a single physical machine, we can have multiple JVM’s (Java Virtual Machines) running. Roughly these JVM’s can be categorized into two types:\nContainer: It can have single or multiple components (HCD, Assembly etc.) running inside it. Location Service: Think of it as a agent which is running on all the machines in TMT environment.\nNote Here onwards, we will refer to Location Service as agent or server interchangeably. Do not confuse it with csw-location-agent.\nLet’s discuss different components of Location Server in following sections:","title":"Design"},{"location":"/technical/location/location-server.html#cluster-member","text":"Location Service JVM (precisely Actor System) takes part in Akka Cluster. By default, this actor system binds to port 3552. Initially when there is no member in Akka cluster, node joins itself. Such a node is referred as seed node (introducer) and the location of this node needs to be known so that other nodes can join to this known address and form a larger cluster. After the joining process is complete, seed nodes are not special and they participate in the cluster in exactly the same way as other nodes.\nAkka Cluster provides cluster membership service using gossip protocols and an automatic failure detector.\nDeath watch uses the cluster failure detector for nodes in the cluster, i.e. it detects network failures and JVM crashes, in addition to graceful termination of watched actor. Death watch generates the Terminated message to the watching actor when the unreachable cluster node has been downed and removed. Hence we have kept auto-down-unreachable-after = 10s so that in case of failure, interested parties get the death watch notification for the location in around 10s.","title":"Cluster Member"},{"location":"/technical/location/location-server.html#distributed-data-replicator-","text":"We use Akka Distributed Data to share CSW component locations between nodes in an Akka Cluster. These locations are accessed with an actor called as replicator providing a key-value store like API. We store the following data in this key-value store (distributed data):\nAllServices: This uses LWWMap CRDT from Connection to Location. Connection and Location can be one of Akka, Tcp or HTTP type. At any point in time, the value of this map represents all the locations registered with Location Service in a TMT environment. Service: This uses LWWRegister which holds location of CSW component against unique connection name.","title":"Distributed Data (Replicator)"},{"location":"/technical/location/location-server.html#consistency-guarantees","text":"WriteMajority: All the write API’s (register, unregister etc.) updates registry (distributed data) with consistency level of WriteMajority which means value will immediately be written to a majority of replicas, i.e. at least N/2 + 1 replicas, where N is the number of nodes in the cluster ReadLocal: All the get API’s (find, resolve, list etc.): retrieves value from registry (distributed data) with consistency level of ReadLocal which means value will only be read from the local replica\nIn TMT environment, we do not want two components to be registered with same connection name. This is achieved by configuring consistency level of WriteMajority for register API. Register API guarantees that a component is registered with Location Service and its entry is replicated to at least N/2 + 1 replicas.\nBased on above configuration, it is always guaranteed that only one location of a component will exist at any point in time in registry. Hence it is safe to read location just from local replica with consistency level of ReadLocal with the assumption that eventually location will get replicated on this replica if not present when queried.","title":"Consistency Guarantees"},{"location":"/technical/location/location-server.html#death-watch-actor","text":"Death watch actor registers interest in change notifications for AllServices key. Hence on every addition or removal of location, death watch actor receives Changed[LWWMap[Connection, Location]] message from where it gets all the current locations.\nDeath watch actor then starts watching all the new locations. When it receives Terminated signal for any of the watched location precisely for actor ref, then it unregister that particular connection from Location Service.\nNote Death watch actor only supports Akka locations and filters out tcp and http locations.","title":"Death Watch Actor"},{"location":"/technical/location/location-server.html#http-server","text":"Location Service provides HTTP routes to get, register, unregister and track locations. Only one instance of location server is started on port 7654 on evey machine. Client from same machine running in different processes can connect to localhost:7654 to access Location Service. In most of the cases, you will not directly talk to this address. You will always use Location Service client provided by CSW which internally connects to localhost:7654 to access Location Service.","title":"HTTP Server"},{"location":"/technical/location/location-server.html#how-location-tracking-works","text":"Below diagram illustrate Assembly tracking HCD. Use case shown in diagram is when Assembly starts tracking, before HCD is registered with Location Service. It also shows the abrupt shutdown of HCD and how Assembly gets notification of that.\nLet us go through each action step by step as shown in diagram:\nAssembly starts tracking HCD by sending HTTP track request using location client to location server. On receiving track request, location server internally subscribes to the replicator using Service key as explained in previous section and generates stream of TrackingEvent Server then maps this stream of TrackingEvent to Websocket Server also keeps sending ServerSentEvent.heartbeat every 2 seconds to keep connection alive HCD registers with Location Service by sending register request to location server. On receiving register request, location server internally updates both Service and AllServices keys by sending update message to replicator Death watch actor is started with Location Service and it gets notification on every component registration. In our flow, death watch actor receives notification of HCD getting registered with Location Service from previous step and it immediately starts watching death of HCD. One of the tasks of replicator is to keep replicating CRDT's from one node to other. In this case, location of HCD gets replicated from Machine 1 to Machine 2 As soon as replicator from Machine 2 receives HCD location, it notifies all the interested parties. Remember Step 1 is interested and receives Changed(key) message from replicator which gets mapped to TrackingEvent Location server then maps it to LocationUpdated event and pushes it to Assembly via SSE Assume that after some time, HCD crashes/terminates/throws exception and shutdowns abruptly. As described in Step 3, Death watch actor is watching HCD. On HCD's shutdown, death watch actor unregisters HCD from Location Service by sending update message by removing HCD's entry from replicator. Eventually this removal of HCD gets replicated to replicator running on Machine 2. On receiving removal of HCD location, same actions gets performed as described in Step 5. In this case, LocationRemoved event gets pushed to Assembly via SSE\nNote At any point in time, Assembly can choose to cancel tracking. On cancellation, this persistent connection will be released.","title":"How location tracking works"},{"location":"/technical/location/location-server.html#location-service-with-authentication-and-authorization","text":"Note : Outside below means any machine not present in this Akka cluster.\nBelow diagram illustrate how Akka cluster will look when authentication and authorization is enabled in Location Server. By default when you start Location Server, it will start in local-only mode (Authentication and authorization Disabled) and bind to 127.0.0.1. To start Location Server in public mode (Authentication and authorization enabled) and bind to 0.0.0.0, use --publicNetwork command line option when starting location server\nWhy is this needed ?\nAs Location Server is by default bind to 127.0.0.1, no application can establish Http connection from Outside. In production environment, you may need a capability to access protected resources of Location Server and provide Authentication and Authorization for such resources. E.g. ability to register/unregister components(which has to undergo maintenance) from a system operator machine present Outside. To enable this we need to bind few instances of Location Server to 0.0.0.0, so that Outside Http connections can be made and applications with valid token, can access its protected resources.","title":"Location Service with Authentication and Authorization"},{"location":"/technical/location/location-server.html#internals","text":"The Main class delegates the job of creating the cluster actor and HTTP server instance to the ServerWiring class.\nThe default TCP ports for the actor and HTTP servers are specified in application.conf.\nNote Due to the way random port numbers are used for CSW components, firewalls should be disabled for these systems, which are assumed to be in an internal network that is protected from outside access.\nIn order to determine the correct IP address to use for the local host, it is necessary to set the INTERFACE_NAME environment variable or property to the name of the network interface to use (There could be multiple NICs). The ClusterSettings class uses that information, along with other settings when starting the cluster actor. It also needs to know the cluster seeds, a comma separated list of host:port values for at least one other actor in the cluster. This information is needed in order to join the Location Service cluster.\nThe Location Service HTTP server is implemented by the LocationHttpHandler class, LocationWebsocketHandler and talks to the cluster actor on the client’s behalf.","title":"Internals"},{"location":"/technical/location/location-server.html#java-api","text":"Since the location server is only accessed internally, there is no extra Java API for it. The location service client and API code does provide Java APIs (see below).","title":"Java API"},{"location":"/technical/location/location-server.html#tests","text":"There are numerous tests for the location server, including multi-jvm tests. The tests can be run with:\nUnit/Component Tests: sbt csw-location-server/test:test Multi-Jvm Tests: sbt integration/multi-jvm:testOnly csw.location*","title":"Tests"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/commandService.html","text":"","title":"Command Service in CSW 2"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/commandService.html#command-service-in-csw-2","text":"The Command Service got a rework in CSW 2 to make it more intuitive. One of the major differences is that the runId of a command is no longer created in the client code when creating a Setup or Observe. It is instead created by the framework when receiving a command, and then passed to the command handlers as an argument. Therefore validateCommand, onSubmit, and onOneway handlers all now take a runId in addition to the command.\nWhen a command is received via a Submit, the runId is created and passed with the command to the validateCommand handler. If the command is Accepted, the runId and command are passed to the onSubmit handler. For long running commands, a Started message is returned containing the runId. This is then registered with CommandResponseManager(CRM), which has also been streamlined in this release. The Started response is then returned to the sender of the command, which can use the included runId to query the command for a final response.\nThe query command in CSW 1 would return a QueryResponse, which would essentially be any SubmitResponse plus an additional response for the case when the Command runId being queried is not registered in the CRM. For CSW 2, this case now returns an InvalidResponse with a IdNotAvailableIssue, thus eliminating the need for a QueryResponse.\nFor final responses, the CompletedWithResult response type has been removed. Instead, all Completed responses contain a Result with it. If the command does not return a result, this result value will be an EmptyResult type.\nA default timeout of 5 seconds has been added to all commands. This timeout can be overridden in submitAndWait and queryFinal calls in Scala. For Java, the timeout is a mandatory argument to these calls.\nAs mentioned before, the CRM has been streamlined, but most improvements are internal and not visible to the developer. One API change is that the updateSubCommand method is no longer supported.\nHandlers have also been added for onDiagnosticMode and onOperationsMode. The corresponding commands have been added to the CommandService object created when using a CommandServiceFactory and the component’s location.\nAdditionally, commands can now be sent to Assemblies and HCDs using HTTP. A HTTP version of the CommandService can be obtained from the CommandServiceFactory using an HTTPLocation. If an AkkaLocation is used, the normal Akka-based CommandService is obtained.","title":"Command Service in CSW 2"},{"location":"/services/aas/csw-aas-installed.html","text":"","title":"Installed Auth Adapter (csw-aas-installed)"},{"location":"/services/aas/csw-aas-installed.html#installed-auth-adapter-csw-aas-installed-","text":"csw-aas-installed is the adapter you will use if you want to build a client application that executes on a user’s machine and talks to an AAS-protected web service application, such as a CLI application. The Configuration Service Admin API makes use of this library.\nThis is as opposed to building a web application that runs in a browser. To do that, use the csw-aas-http library.","title":"Installed Auth Adapter (csw-aas-installed)"},{"location":"/services/aas/csw-aas-installed.html#dependencies","text":"To use the Akka HTTP Adapter (csw-aas-installed), add the following to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-aas-installed\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/aas/csw-aas-installed.html#prerequisites","text":"To run a client app with AAS access, we need\nThe CSW Location Service running An AAS instance running and registered with the Location Service A protected HTTP server running\nAll of these can be running on different machines. To start a Location Service and AAS server on a local machine, you can make use of thecsw-services.sh script.","title":"Prerequisites"},{"location":"/services/aas/csw-aas-installed.html#application-configurations","text":"All AAS related configurations go inside an auth-config block in your application.conf file. There are two configurations applicable for a public client application: realm and client-id.\nrealm has a default value of TMT if not specified. Normally, all apps in TMT should not have to override this, however it might be useful to override this while testing your app.\nclient-id is a mandatory configuration which specifies the client ID of the app as per its registration in AAS.\ndisabled is an optional configuration with default value of false. This flag can be turned on for local development and testing purposes. It can greatly ease the process of testing business logic without having to go through the process of creating users, managing roles and logging in with user credentials to generate valid access tokens. For it to work, the same flag needs to turned on in the server application too.\nCaution This should not be used in production\nauth-config {\n  realm = TMT # DEFAULT\n  client-id = demo-cli # REQUIRED\n  disabled = false # DEFAULT\n}","title":"Application Configurations"},{"location":"/services/aas/csw-aas-installed.html#building-a-cli-application","text":"Let’s say that we have an existing Akka HTTP application which has some open and some protected routes, and we want to build a CLI client which accesses these routes.\nScala var data: Set[String] = Set.empty\n\nval routes: Route =\n  pathPrefix(\"data\") {\n    get {                    // un-protected route for reading data\n      pathEndOrSingleSlash { // e.g HTTP GET http://localhost:7000/data\n        complete(data)\n      }\n    } ~ sPost(RealmRolePolicy(\"admin\")) { // only users with 'admin' role is allowed for this route\n      parameter(\"value\") { value =>       // e.g POST GET localhost:7000/data?value=abc\n        data = data + value\n        complete(StatusCodes.OK)\n      }\n    }\n  }\nNote To know more about how to create secure web APIs, please go through Akka HTTP Adapter - csw-aas-http\nWe will create a CLI application that has following commands:\ncommand description login performs user authentication logout logs user out read reads data from server write {content} writes data to server\nLet’s begin with Main.scala\nScala object Main extends App {\n\n  LocationServerStatus.requireUpLocally()\n\n  implicit val actorSystem: ActorSystem[_] = ActorSystem(Behaviors.empty, \"example-system\")\n\n  val adapter: InstalledAppAuthAdapter = AdapterFactory.makeAdapter\n\n  val command: Option[AppCommand] = CommandFactory.makeCommand(adapter, args)\n\n  try {\n    command.foreach(_.run())\n  }\n  finally {\n    actorSystem.terminate()\n  }\n}\nThe statement LocationServerStatus.requireUpLocally() ensures that the Location Service is up and running before proceeding further. If it is not running, an exception will be thrown and the application will exit.\nNote In a real application, you would ideally want to use LocationServerStatus.requireUp which takes locationHost: String parameter instead of looking for the Location Service on the localhost.\nNext, we will instantiate InstalledAppAuthAdapter. There is a factory already available to create the required instance. We will create a small factory on top of this factory to keep our Main.scala clean.\nScala object AdapterFactory {\n  def makeAdapter(implicit actorSystem: typed.ActorSystem[_]): InstalledAppAuthAdapter = {\n    implicit val ec: ExecutionContextExecutor = actorSystem.executionContext\n    val locationService: LocationService      = HttpLocationServiceFactory.makeLocalClient(actorSystem)\n    val authStore                             = new FileAuthStore(Paths.get(\"/tmp/demo-cli/auth\"))\n    InstalledAppAuthAdapterFactory.make(locationService, authStore)\n  }\n}\nNote the the internal factory method we have used requires two parameters: a reference to the Location Service to resolve the AAS Server, and authStore, which is a file-based access token storage system.\nWarning In this case we have configured it to store all tokens in the “/tmp/demo-cli/auth” directory, but ideally you want this location to be somewhere in the user’s home directory. This will ensure that different users don’t have access to each other’s tokens.\nComing back to Main.scala, now we need to find out which command the user wants to execute. To parse the user input arguments, we will create a small utility.\nScala object CommandFactory {\n  def makeCommand(adapter: InstalledAppAuthAdapter, args: Array[String])(\n      implicit actorSystem: typed.ActorSystem[_]\n  ): Option[AppCommand] = {\n\n    // ============ NOTE ============\n    // We are doing hand parsing of command line arguments here for the demonstration purpose to keep things simple.\n    // However, we strongly recommend that you use one of the existing CLI libraries. CSW makes extensive use of scopt.\n    args match {\n      case Array(\"login\")          => Some(new LoginCommand(adapter))\n      case Array(\"logout\")         => Some(new LogoutCommand(adapter))\n      case Array(\"read\")           => Some(new ReadCommand)\n      case Array(\"write\", content) => Some(new WriteCommand(adapter, content))\n      case _ =>\n        println(\"invalid or no command\\nvalid commands are: login, logout, read & write\")\n        None\n    }\n  }\n}\nAll of these commands extend from a simple trait - AppCommand.\nScala trait AppCommand {\n  def run(): Unit\n}\nIts single method run is executed in our application once the arguments are parsed into an AppCommand.\nNote We could have used a command line parser library here to parse the command names and options/arguments, but since our requirements are simple and this is a demonstration, we will keep things simple. However, we strongly recommend that you use one of the existing libraries. CSW makes extensive use of scopt. There are other libraries which are equally good and easy to use.\nLet’s go through each command one by one:","title":"Building a CLI Application"},{"location":"/services/aas/csw-aas-installed.html#login","text":"Scala class LoginCommand(val installedAppAuthAdapter: InstalledAppAuthAdapter) extends AppCommand {\n  override def run(): Unit = {\n    installedAppAuthAdapter.login()\n    println(\"SUCCESS : Logged in successfully\")\n  }\n}\nHere the constructor takes an InstalledAppAuthAdapter as a parameter, and in the run method, it calls installedAppAuthAdapter.login(). This method opens a browser and redirects the user to a TMT login screen (served by AAS). In the background, it starts an HTTP server on a random port. Once the user submits the correct credentials on the login screen, AAS redirects the user to http://localhost:[SomePort] with the access and refresh tokens in a query string. The InstalledAppAuthAdapter will then save these tokens to the file system using FileAuthStore. After this, InstalledAppAuthAdapter will shut down the local server since it is no longer needed. The user can then close the browser.\nIf you want to develop a CLI app that is not dependent on a browser, you can call loginCommandLine() method instead of login(). This will prompt the user to provide credentials in the CLI instead of opening a browser.\nNote While the loginCommandLine() method is available, a browser is generally more user-friendly since it can store cookies and remember passwords.","title":"Login"},{"location":"/services/aas/csw-aas-installed.html#logout","text":"Scala class LogoutCommand(val installedAppAuthAdapter: InstalledAppAuthAdapter) extends AppCommand {\n  override def run(): Unit = {\n    installedAppAuthAdapter.logout()\n    println(\"SUCCESS : Logged out successfully\")\n  }\n}\nThe structure here is very similar to the login command. installedAppAuthAdapter.logout() clears all the tokens from the file system via FileAuthStore.","title":"Logout"},{"location":"/services/aas/csw-aas-installed.html#read","text":"Scala class ReadCommand(implicit val actorSystem: typed.ActorSystem[_]) extends AppCommand {\n  override def run(): Unit = {\n    val url      = \"http://localhost:7000/data\"\n    val response = Await.result(Http(actorSystem.toClassic).singleRequest(HttpRequest(uri = Uri(url))), 2.seconds)\n    println(convertToString(response.entity))\n  }\n}\nSince the get route is not protected by any authentication or authorization in the our example server, the read command simply sends a get request and prints the response.","title":"Read"},{"location":"/services/aas/csw-aas-installed.html#write","text":"Scala class WriteCommand(val installedAppAuthAdapter: InstalledAppAuthAdapter, value: String)(\n    implicit val actorSystem: typed.ActorSystem[_]\n) extends AppCommand {\n  override def run(): Unit = {\n\n    installedAppAuthAdapter.getAccessToken() match {\n      case Some(token) =>\n        val bearerToken = headers.OAuth2BearerToken(token.value)\n        val url         = s\"http://localhost:7000/data?value=$value\"\n        val response =\n          Await.result(\n            Http(actorSystem.toClassic).singleRequest(\n              HttpRequest(\n                method = HttpMethods.POST,\n                uri = Uri(url),\n                headers = List(headers.Authorization(bearerToken))\n              )\n            ),\n            2.seconds\n          )\n\n        response.status match {\n          case StatusCodes.OK           => println(\"Success\")\n          case StatusCodes.Unauthorized => println(\"Authentication failed\")\n          case StatusCodes.Forbidden    => println(\"Permission denied\")\n          case code                     => println(s\"Unrecognised error: http status code = ${code.value}\")\n        }\n\n      case None =>\n        println(\"you need to login before executing this command\")\n        System.exit(1)\n    }\n  }\n}\nThe WriteCommand constructor takes an InstalledAppAuthAdapter and a string value, passed in at the command line. Since the post route is protected by a realm role policy in our example server, we need to pass a bearer token in the request header.\ninstalledAppAuthAdapter.getAccessTokenString() checks the FileAuthStore and returns an Option[String]. If the Option is None, it means that user has not logged in and an error message is displayed. If the token is found, the bearer token is obtained and passed in the header of the request to the HTTP server. The HTTP server uses this token to determine whether the client has the proper permissions to perform the request.\nIf the response status code is 200, it means authentication and authorization were successful. In our example, authorization required that the user had the admin role.\nIf the response is 401 (StatusCodes.Unauthorized), there is something wrong with the token. It could indicate that token has expired or does not have a valid signature. InstalledAppAuthAdapter ensures that you don’t send a request with an expired token. If the access token is expired, it refreshes the access token with the help of a refresh token. If the refresh token has also expired, it returns None which means that user has to log in again.\nIf the response is 403 (StatusCodes.Forbidden), the token is valid but the token is not authorized to perform that action. In our example, this would occur if the user does not have the admin role.","title":"Write"},{"location":"/services/aas/csw-aas-installed.html#source-code-for-above-examples","text":"Example","title":"Source code for above examples"},{"location":"/technical/location/location-agent.html","text":"","title":"Location Agent"},{"location":"/technical/location/location-agent.html#location-agent","text":"","title":"Location Agent"},{"location":"/technical/location/location-agent.html#introduction","text":"The csw-location-agent project provides an application used to register and track non-csw services, such as Redis, which is used to implement the Event and Alarm Services.","title":"Introduction"},{"location":"/technical/location/location-agent.html#usage","text":"See here for the command line usage.","title":"Usage"},{"location":"/technical/location/location-agent.html#design","text":"Location service agent is a CLI application build using Location Client.\nAs shown in above diagram, it accepts external command, name of the services and some other optional arguments.\nOn execution, Location agent performs following actions in sequence:\nSpawns new process and executes provided command in that process Adds exit hook on this process, so that agent shutdowns gracefully on termination of child process Registers Tcp or Http connections with provided component names with location service using location http client Adds shutdown hook to unregister services on termination (this is done as a part of step 2)\nLog messages are configured in application.conf to log only to file (under $TMT_LOG_HOME/csw/logs).","title":"Design"},{"location":"/index.html","text":"","title":"TMT Common Software (CSW)"},{"location":"/index.html#tmt-common-software-csw-","text":"Common Software is the package of services and infrastructure software that integrates the TMT software systems.\nVisit TMT website to know more about Thirty Meter Telescope.","title":"TMT Common Software (CSW)"},{"location":"/index.html#common-software-architecture","text":"CSW is designed to support the Observing Mode-Oriented Architecture (OMOA). An observing mode is a well-defined instrument or engineering observing task and an associated set of owned resources, procedures, and capabilities that implement the mode. An example instrument observing mode is: IRIS multi-filter integral field spectroscopy using the NFIRAOS adaptive optics unit with AO laser guide star correction. An instrument will generally have several associated observing modes for acquisition, science objects, and calibrations. Examples of observing mode resources could be an instrument’s hardware devices, or the use of a larger system such as the Laser Guide Star Facility.\nOMOA structures the software in layers as shown in the following figure. Each layer contains components with specific responsibilities described in the following sections. OMOA bypasses the use of standalone “subsystems” (large principal systems) for a flatter system that requires less code and allows the software system for an observing mode to optionally be more flexibly composed at run-time.","title":"Common Software Architecture"},{"location":"/index.html#layer-0-obseratory-hardware","text":"Layer 0 represents the actual hardware being controlled and the hardware controllers that interface the hardware to the computer systems.","title":"Layer 0 - Obseratory Hardware"},{"location":"/index.html#layer-1-hardware-control-layer","text":"The lowest layer in the OMOA software system, the Hardware Control Layer, consists of all the controllable hardware that is available for use by higher levels of software. A sea of similar software components called Hardware Control Daemons (HCD) at layer 1 controls the low-level hardware of the telescope, adaptive optics, and instruments.\nAn HCD is similar to the device driver found in many systems. Each HCD is associated with a networked motion controller, a PLC/PAC, or other low-level hardware controller present in Layer 0. Some hardware controllers will support multiple channels. An HCD may support a highly cohesive, related set of functionality. For instance, one motion controller with 8 axes might handle all the slow moving filters and gratings of an instrument. In other cases, the channels of the controller hardware could be associated with unrelated devices. If the hardware controller has multiple channels, the HCD supports access to all the channels and must multiplex access to the controller and coordinate requests and replies among the clients.","title":"Layer 1 - Hardware Control Layer"},{"location":"/index.html#layer-2-assembly-layer","text":"The Assembly Layer exists just above the Hardware Control Layer at Layer 2. Software at this layer consists of components called Assemblies. In OMOA, an Assembly represents a device as a collection of hardware that makes sense at the user level. Examples of instrument devices are a filter wheel, a deformable mirror, or a detector controller. Assemblies often represent user-oriented devices in the software system, but it is not necessary that an Assembly control HCDs.","title":"Layer 2 - Assembly Layer"},{"location":"/index.html#layer-3-sequencing-layer","text":"The Sequencing Layer is Layer 3 in the figure above. Components at this level are called Sequencers because they take complex descriptions of tasks and control and synchronize the actions of the Assemblies to accomplish the tasks. A Sequence Component is a reusable OMOA application that can load a Script. Once the Sequence Component loads a Script, it becomes a Sequencer. This allows a Sequence Component to load a different Script based on the observing mode. Individual Sequencers and Scripts can provide a higher level of control for a set of distributed hardware (e.g., init) or can provide commands that are specific to an observing mode or engineering task.\nSequencers in this layer share a software interface that allows them to be plugged together to form a sequencing hierarchy for a specific observing mode. There can be one or many Sequencers in a hierarchy supporting a specific observing mode.","title":"Layer 3 - Sequencing Layer"},{"location":"/index.html#layer-4-monitoring-and-control-layer","text":"The Monitoring/Control Layer is the layer of software that contains the user interface programs that are used to observe with the telescope. At TMT there will be graphical user interfaces for use by observers during observing. These applications use the CSW services to control and monitor the system.","title":"Layer 4 - Monitoring and Control Layer"},{"location":"/index.html#csw-services","text":"CSW or Common Software provides a shared software infrastructure based on a set of services and associated software for integrating individual components in the large observatory software architecture. The components and client applications use a set of loosely coupled services, each autonomous with a well-defined interface that hides the service implementation and also provides a TMT-standardized communication with the services.","title":"CSW Services"},{"location":"/index.html#","text":"The Location Service of TMT Common Software handles application, component, and service registration and discovery in the distributed TMT software system. When a component (i.e. an Application, Sequencer, Assembly, HCD, Container, or Service) is initializing, it registers its name along with other information such as interface type and connection information to the Location Service. The important feature and reason for the Location Service is that details of connection information should not be hardwired, they should be discovered at runtime.\nLocation Service is most obviously needed when one component commands another component. In this case the first component uses the Location Service to get information about the second component, and uses that information to make a connection. Discovered information might include a protocol (e.g., HTTP), interface type (e.g., command), or host and port.","title":"Location Service"},{"location":"/index.html#","text":"The Configuration Service (CS) provides a centralized persistent store for “configuration files” used in the TMT Software System. In this context, a configuration file is a set of values describing state, initialization values, or other information useful to a component or set of components. The TCS provides many examples such as look-up tables of various kinds or a set of pointing model parameters or parameters for setting up a motion controller. Another is the Alarm Service Configuration File. At the applications level, the GUI used by the Observing Assistant could provide a button to save offsets between an instrument science field and its acquisition camera origin. These are the kinds of scenarios that use the Configuration Service.\nThe Configuration Service provides the added feature of storing versions of configuration files. All versions of configuration files are retained providing a historical record of changes for each configuration file. Components can save today’s version without fear that yesterday’s version will be lost. If the configuration of a component is inadvertently lost, it will be possible to easily restore to the most recently saved version or a default version.","title":"Configuration Service"},{"location":"/index.html#","text":"Logging is the ability of a software component to output a message, usually for diagnostic purposes. Common Software will provide a Logging Service. Logging should not be confused with “data logging”, which is usually collection of measured values.\nThis log message includes a time of the log message, a severity (INFO), the source of the log message as a package path in the software, and a formatted text message. The Logging Service provides the ability to log messages locally to a file or screen and optionally to a centralized logging aggregator.\nThe central logging aggregator provides the capability for all components to log diagnostic information to a central and optionally persistent store. The logging information is then aggregated and ordered by timestamp. A coordinated, centralized log can be an extremely useful tool for diagnosing many types of distributed software problems. Structured logging will be used with the central logging aggregator.\nThe Logging Service is unique because it is required early in the lifecycle of a component and most components and CSW services themselves will want the ability to log information. It is often necessary to log messages while a component starts up. This means that the implementation of distributed logging must not depend upon other services (at least if the independence of services is desired). It also means that distributed logging will need to load quickly and provide proper behavior if the aggregating logging capability is needed.\nThe logging API provides familiar features similar to available logging libraries including logging levels and the ability to dynamically change the component’s logging configuration while the component is executing. This allows the ability to interactively log more detailed messages when a component encounters problems.","title":"Logging Service"},{"location":"/index.html#","text":"In the OMOA software design, an Application or Sequencer connects to Assemblies and causes actions by submitting commands. Assemblies then connect to and command HCDs. The service that provides the command functionality is called the Command Service (CCS).\nIn the system design each observing mode has a Sequencer hierarchy that consists of one or more OMOA Sequence Components/Sequencers. Commands flow down through the Sequencers to the Assemblies, HCDs and hardware in a hierarchy.\nIn CSW, commands require peer-to-peer connections between the component sending a command and the component receiving the command. There is no reason to directly connect to a component unless that component will be commanded. The Location Service provides connection information for components sending commands with CCS.","title":"Command Service"},{"location":"/index.html#","text":"In an event-driven system, an event marks the occurrence of a state change, action, or activity that is of interest in the system. In TMT many interactions between systems are best viewed as being event-driven. For instance, Observe Events are used by a science detector to indicate when activities have occurred such as closing the shutter at the conclusion of a science observation. The TCS Pointing Assembly sends pointing demand events to mechanisms throughout the software system.\nThe Event Service is based on the publish/subscribe messaging paradigm. One component publishes an event and all components that have subscribed receive the event. The advantage of this type of message system is that publishers and subscribers are decoupled. Publishers can publish regardless of whether there are subscribers, and subscribers can subscribe even if there are no publishers. The relationship between publishers and subscribers can be one-to-one, one-to-many, many to one, or even many-to-many. Another advantage of publish-subscribe systems is that components and systems can startup and stop independently without requiring special interactions or startup sequences with other systems.\nThe publish-subscribe pattern also allows the creation of event dependencies between systems that are difficult to track but must be understood and managed. Dependencies can be passive or active. When a component subscribes to a topic, but takes no action based on the value it is called a passive dependency. For instance, a GUI display could subscribe to the current position of an instrument filter but take no action other than displaying the value. An active dependency occurs when a subscribing component uses the event value to alter its behavior. For instance, an atmospheric dispersion corrector Assembly listens to the telescope zenith angle event in order to properly correct for dispersion.","title":"Event Service"},{"location":"/index.html#","text":"An alarm is published to mark an abnormal condition that requires the attention of an operator or other user. Alarms are not errors, they are conditions that occur asynchronously while components are executing or inactive. For instance, an alarm could be published to indicate a hardware limit. An example of this kind of alarm event is a detector temperature that is too high. Alarms are most valuable to operators and observers who monitor the status of the telescope systems and the instruments. The control GUIs will include standardized ways for displaying alarms if needed. The Executive Software provides the observer and operator interfaces for the purpose of displaying alarms relevant to observing from the instruments and telescope system.\nHowever, alarms are not a suitable or approved approach to hazard control as part of a TMT safety system. Nor should alarms be used to indicate errors. Alarms should provide additional information to operators and staff about systems monitored by the OSS and can provide early warning of future hazardous conditions, but should not be a sole, primary hazard control.","title":"Alarm Service"},{"location":"/index.html#","text":"TMT has standardized on the use of Precision Time Protocol (PTP) as the basis of observatory time. The Time Service provides access to time based on the time provided by PTP. The Global Positioning System (GPS) provides the absolute time base called Observatory Time. The PTP grand master clock (a hardware device) is synchronized to Observatory Time. Each computer system participating in the PTP system synchronizes to Observatory Time using the PTP protocol. The time service also provides APIs for scheduling periodic and non-periodic tasks in the future, which are optimised for scheduling at up to 1KHz frequency. Time Service provides access to TAI and UTC time that is synchronized to Observatory Time. Time Service also provides functions for scheduling tasks based on time.","title":"Time Service"},{"location":"/index.html#","text":"The Database Service provides API to manage database connections and access data in the TMT software system. The service expects Postgres as database server. It uses Jooq library underneath to manage database access, connection pooling, etc. To describe JOOQ briefly, it is a Java library that provides an API for accessing data including DDL support, DML support, fetch, batch execution, prepared statements, etc. safety against sql injection connection pooling, etc. To know more about JOOQ and its features, please refer to this link.","title":"Database Service"},{"location":"/index.html#","text":"The framework provides templates for creating and running the kind of software components defined by OMOA as well as service access interfaces for these components. It also provides application support for running multiple components on a host machine.\nThe framework also contains the structures that are common to components, such as commands and event structures.","title":"Framework"},{"location":"/index.html#csw-roadmap","text":"The 1.0.0 release was a crucial milestone of Common Software (CSW). All the required services of Common Software are included in the 1.0.0 release and subsequent releases. Release 1.0.0 is the primary deliverable of the CSW construction work package. CSW is now in the maintenance phase. During the maintenance phase we expect twice per year maintenance releases of CSW.\nMembers of TMT work packages can add issues at the internal maintenance site.\nWhat to expect from releases after 1.0.0?\nUpgrade to Scala 2.13 Upgrade to Akka 2.6 Updates of dependencies Bug fixes and improvements based on user input Any changes needed to support ESW","title":"CSW Roadmap"},{"location":"/index.html#http-based-services-api-documentation","text":"Documentation for HTTP based services could be found here.","title":"HTTP-based services API documentation"},{"location":"/index.html#csw-javascript-adapters-documentation","text":"Documentation for CSW JS adapters could be found here","title":"CSW Javascript adapters documentation"},{"location":"/commons/getting-started.html","text":"","title":"Getting Started"},{"location":"/commons/getting-started.html#getting-started","text":"In this tutorial, you’ll see how to create a Scala/Java project using a giter8 template for CSW (csw.g8) which contains sample handlers for creating HCD and Assembly. It also contains a deploy project which is responsible for starting multiple components or containers. You can use this as a starting point for your own projects for writing component. We’ll use the sbt build tool which compiles, runs, and tests your projects among other related tasks.","title":"Getting Started"},{"location":"/commons/getting-started.html#installation","text":"Supported Operating Systems are: CentOS and MacOS\nMake sure you have the Java AdoptOpenJDK 11. Run javac -version in the command line and make sure you see javac 11._._ If you don’t have version 11 or higher, links given below will help you to reach there. Mac Linux Install sbt Mac Linux Install IntelliJ MAC Linux Install following IntelliJ Plugins Scala Scalafmt Recommended testing frameworks/tools: ScalaTest JUnit, JUnit Interface Note: These frameworks are typically downloaded and made available by the sbt tool by specifying them as dependencies. If you are using the giter8 template (see below), these dependencies are specified by default, and the sbt will resolve them when it runs.","title":"Installation"},{"location":"/commons/getting-started.html#create-project","text":"cd to an empty folder. Run the following command sbt new tmtsoftware/csw.g8. This pulls the ‘csw’ template from GitHub. Provide input details when prompted. Follow the template readme.md for detailed information about input parameters.\nTo open the project in IntelliJ, start IntelliJ and click on Import Project in the Intro dialog. If you have a project already open, click on File -> New -> Project from Existing Sources…\nThen select the directory created by the template and click Open.\nYou will then see a dialog asking how to import the project.\nBe sure the Import project from external model radio button and sbt options are selected and click Next. Then click Finish on the next dialog to accept the defaults.\nLet’s take a look at what just got generated:\nIn this example, a project was created with default parameters. The complete project structure looks like this:\nAs you can see in below snapshot, template will create three projects: sample-assembly sample-hcd sample-deploy\nsample-deploy project is used to create a concrete implementation. This allows for the construction of a complete binary package bundled with all dependencies, and a launching application.\n3. Template comes with csw and other useful library dependencies. It also includes bunch of plugins as explained in below snapshot","title":"Create project"},{"location":"/commons/getting-started.html#add-new-sbt-project-module","text":"If you want to add another component to the project, for example, with the name sample-io, you have to create a new sbt module:\nAdd external library dependencies required by sample-io in Libs.scala file, if it does not exist. val `akka-actor` = \"com.typesafe.akka\" %% \"akka-actor\" % \"2.6.1\"\n Map new/existing library dependencies in Dependencies.scala file against new project. val SampleIO = Seq( Libs.`akka-actor` )\n Include below snippet in build.sbt file, this will create new sbt project module. lazy val `sample-io` = project\n  .settings( libraryDependencies ++= Dependencies.SampleIO )\n If your new module depends on code from other modules within this project, use .dependsOn in your build.sbt file: lazy val `sample-io` = project\n  .settings( libraryDependencies ++= Dependencies.SampleIO )\n  .dependsOn(\n`sample-assembly`,\n`sample-hcd`\n  )\n Update the deployment dependencies: lazy val `sample-deploy` = project\n  .dependsOn(\n`sample-assembly`,\n`sample-hcd`,\n`sample-io`\n  )","title":"Add new sbt project module"},{"location":"/commons/create-component.html","text":"","title":"Creating a Component"},{"location":"/commons/create-component.html#creating-a-component","text":"This walk-through helps in creating a CSW component in Scala/Java. CSW components depend on the csw-framework package, which can be found here. This section discusses constructing an HCD, but the principles apply to an Assembly as well. We will be constructing the Assembly in the next section Working with Multiple Components.","title":"Creating a Component"},{"location":"/commons/create-component.html#","text":"This tutorial shows code written in Scala and Java, based on code generated by the giter8 templates with the default values.\nAsync handling in Scala and Java examples. Most of CSW is written using asynchronous programming. Therefore, in the examples, you may find constructs that deal with Futures and other asynchronous code in various ways. The following constructs are commonly used throughout this manual: Scala: The Scala async package is used extensively. async marks a block of asynchronous code and allows to await the computation until the Future is complete. For more info, please refer to: https://github.com/scala/scala-async. Sometimes, an example may use the Await.result construct. While this is often used in tests, it is a blocking call and typically should not be used in production code. Java non-blocking example: The code snippets use CompletableFuture and its thenAsync, thenApply methods. This style allows to compose multiple Futures and not block the calling thread until Futures are complete. Java blocking example: The code snippets use CompletableFuture using get blocking call. This style also blocks the calling thread until the Future is complete, and it should only be prudently used.","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#anatomy-of-component","text":"A component consists of a Supervisor actor, a Top Level Actor (TLA) that provides component handlers, and one or more worker actors. The csw-framework provides the Supervisor actor, the Top Level Actor and an abstract class of handlers. Component developers are expected to implement these handlers, which collectively act as the gateway from the framework to the developer’s component code.","title":"Anatomy of Component"},{"location":"/commons/create-component.html#supervisor","text":"A Supervisor actor is the actor first started for any component. The main responsibilities that the Supervisor performs is as follows:\nCreation of the TLA when the component starts up Implement and manage the component lifecycle for the TLA and for the component (see Lifecycle below). Register the component with the Location Service. Provide an administrative interface to the component to the rest of the system. For instance, the Container can perform some administrative communication with the Supervisor such as restart or shutdown of the component. Allow components outside of the Supervisor and TLA to monitor the lifecycle state of the TLA. This is particularly useful for testing, when the test needs to know that the component is ready before performing its test actions. Supports the locking functionality for the component (see Locking) Receives external commands and passes them to the correct component handlers.\nThe source code of the Supervisor actor can be found here","title":"Supervisor"},{"location":"/commons/create-component.html#top-level-actor","text":"While the Supervisor works as the external interface for the component and the manager of its lifecycle, the functional implementation of a component is implemented in a Top Level Actor (TLA), spawned by the Supervisor actor for each component. However, the developer is not expected to implement a TLA code entirely. Instead, the component-specific functionality of the TLA is added by implementing the ComponentHandlers abstract class, consisting of a set of methods, or hooks, called by the TLA during specific lifecycle and command events (see Handlers). The ComponentHandlers implementation is specified during construction using a factory (see Constructing The Component)\nThe source code of the Top Level Actor can be found here.","title":"Top Level Actor"},{"location":"/commons/create-component.html#handlers","text":"The following hooks may be overridden in your ComponentHandlers implementation class:\ninitialize: called when the component is starting up, prior to be put into the Running state. validateCommand: called when the component receives a command to determine if the command is valid and can be executed. (see Validation) onSubmit: called on Submit command if validateCommand returns Accepted. onOneway: called on Oneway command if validateCommand returns Accepted. onGoOffline: called when the component receives an external message from an administrative client to go offline. onGoOnline: called when the component receives an external message from an administrative client to go online. onDiagnosticMode: called when the component receives an external message from a client to enter a specified diagnostic behavior. onOperationsMode: called when the component receives an external message from a client to exit any diagnostic behavior and return to normal, operations behavior. onLocationTrackingEvent: called when a tracked dependency changes location state. (see Tracking Dependencies) onShutdown: called when the component is shutting down.\nThe source code of ComponentHandlers can be found here.\nMore details about handler significance and invocation can be found here\nComponent Handlers in Java If the component developer wishes to write the component handler implementation in Java, they need to implement the Java version of ComponentHandlers called JComponentHandlers. The source code of JComponentHandlers can be found here. Any further reference to ComponentHandlers should be inferred as also applying to JComponentHandlers.","title":"Handlers"},{"location":"/commons/create-component.html#","text":"As seen in the Getting Started page, if you are using the giter8 template, component handler classes for both the HCD and Assembly are written for you, with implementations stubbed out. We will walk-through filling them in below.","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#constructing-the-component","text":"After writing the component handlers, a developer needs to wire it up with the framework. In order to do this, the developer needs to first implement a ComponentBehaviorFactory. This factory should be specified in a ComponentInfo configuration file for the component (see example below). The csw-framework picks up the full class path of the ComponentBehaviorFactory from the file and the Supervisor spawns the component handlers using this factory in the process of booting the component. The factory is instantiated using Java reflection.\nAdditional sample code to implement the ComponentBehaviorFactory can be found here","title":"Constructing the Component"},{"location":"/commons/create-component.html#","text":"As seen in the Getting Started page, if using the template, this factory class will be implemented for you.","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#component-configuration-componentinfo-","text":"The component configuration, called the ComponentInfo file, contains details needed to create a component. This configuration defines a few parameters needed for a particular component. The template creates one for our sample HCD as follows:\nScala prefix = \"nfiraos.SampleHcd\"\ncomponentType = hcd\nbehaviorFactoryClassName = \"org.tmt.nfiraos.samplehcd.SampleHcdBehaviorFactory\"\nlocationServiceUsage = RegisterOnly Java prefix = \"nfiraos.JSampleHcd\"\ncomponentType = hcd\nbehaviorFactoryClassName = \"org.tmt.nfiraos.samplehcd.JSampleHcdBehaviorFactory\"\nlocationServiceUsage = RegisterOnly\nWhat is a behaviorFactoryClassName? behaviorFactoryClassName refers to class name of the a concrete implementation of ComponentBehaviorFactory, which is SampleHcdBehaviorFactory for Scala in above example, JSampleHcdBehaviorFactory for Java.\nThe prefix and componentType are used to create the ComponentId identifier, which must be unique within the control system and the Location Service. The prefix must begin with a valid TMT subsystem, which establishes a scope for the component name. The list of valid subsystems is here.\nThe locationServiceUsage is used by the Supervisor actor to decide whether to only register a component with the Location Service or to register and track other components. It is also possible to choose not register the component with the Location Service.\nThe ComponentInfo file is parsed to a ComponentInfo object and injected in the Supervisor actor. It is then injected in ComponentHandlers while spawning a component and the contents is available for the developer to access.\nUsing Prefix ComponentInfo includes the Prefix for the component. Developers should try to use this prefix value rather than defining a new one to reduce errors.\nThe ComponentInfo file can also contain a list of components and services it wishes to track as dependencies. See Tracking Dependencies.\nMore details about ComponentInfo can be found here.\nAn additional sample configuration file can be found here.","title":"Component Configuration (ComponentInfo)"},{"location":"/commons/create-component.html#lifecycle","text":"The Supervisor of a component manages its lifecycle state, which can be one of the following:\nIdle Running RunningOffline Restart Shutdown Lock\nThe state the component is in dictates the actions it can take when it receives a message or command, and whether those actions are carried out.","title":"Lifecycle"},{"location":"/commons/create-component.html#idle","text":"The component initializes in the idle state. The Top Level Actor calls the initialize hook of ComponentHandlers as the first thing on boot-up. Component developers write their initialization logic in this hook. The logic can also do things like accessing the Configuration Service to fetch information such as hardware configurations to set the hardware to default positions.\nAfter initialization, if the component would have configured RegisterAndTrack for locationServiceUsage, then the Top Level Actor will start tracking the connections configured for that component. This use case is mostly applicable for Sequencers and Assemblies. HCDs should have RegisterOnly configured for locationServiceUsage in most all cases.\nIf initialize is successful, the Supervisor will register the component with the Location Service. Registering with the Location Service will notify other components tracking this component with a LocationUpdated event containing a Location with a reference to the Supervisor of the component.\nAfter successful registration, the component will transition to the Running state.","title":"Idle"},{"location":"/commons/create-component.html#running","text":"When the Supervisor actor receives Initialized message from the Top Level Actor after successful initialization, it registers itself with the Location Service and transitions the component to the Running state. Running state signifies that the component is accessible via the Location Service, which allows other entities to communicate with it by sending commands via messages. Any commands received by the Supervisor actor will be forwarded to the Top Level Actor for processing once the component is in the Running state.\nWhat does Running mean? A component should be ready for operation after successfully leaving the initialize handler and entering Running. A component should ready to process any command after initialization and must not require specific commands to be issued by users in order to become ready.","title":"Running"},{"location":"/commons/create-component.html#runningoffline","text":"When the Supervisor actor receives a GoOffline message, it transitions the component to the RunningOffline state and forwards it to the Top Level Actor. The Top Level Actor then calls the onGoOffline hook of ComponentHandlers.\nIf a GoOnline message is received by the Supervisor actor, it transitions the component back to the Running state and forwards it to the Top Level Actor. The Top Level Actor then calls the onGoOnline hook of ComponentHandlers.\nHandling RunningOffline In the RunningOffline state, if any command is received, it is forwarded to the underlying component hook through the Top Level Actor. It is then the responsibility of the component developer to check the isOnline flag provided by csw-framework and process the command according to whether the command is appropriate for the command when offline.","title":"RunningOffline"},{"location":"/commons/create-component.html#restart","text":"When the Supervisor actor receives a Restart message, it will transition the component to the Restart state. Then, it will unregister itself from the Location Service so that other components tracking this component will be notified and no commands are received while restart is in progress.\nThen, the Top Level Actor is stopped and the postStop hook of the Top Level Actor will call the onShutdown hook of ComponentHandlers. Component developers are expected to write any cleanup of resources or other logic that should be executed for the graceful shutdown of the component.\nAfter successful shutdown of component, the Supervisor actor will re-create the Top Level Actor again from scratch. This will cause the initialize hook of ComponentHandlers to be called again. After successful initialization of the component, the Supervisor actor will register itself with the Location Service once more.","title":"Restart"},{"location":"/commons/create-component.html#shutdown","text":"When the Supervisor actor receives a Shutdown message, it transitions the component to the Shutdown state. Any commands received while a shutdown is in progress will be ignored. Then, it will stop the Top Level Actor. The postStop hook of the Top Level Actor will call the onShutdown hook of ComponentHandlers. Component developers are expected to write any cleanup of resources or other logic that should be executed for the graceful shutdown of component. The component, including the Supervisor, exits after onShutdown completes.","title":"Shutdown"},{"location":"/commons/create-component.html#lock","text":"When the Supervisor actor receives a Lock message, it transitions the component to the Lock state. When locked, the Supervisor will only accept commands received from the component that originally locked the component and ignore commands from all others.\nIn the Lock state, messages like Shutdown and Restart will also be ignored. A component must first be unlocked to accept these commands.\nLock messages are constructed with a duration value specified. When this duration expires, the component will automatically be unlocked. In order to retain the Lock on the component, sender of the orginal Lock must resend the Lock message.\nThere are two ways component can be unlocked:\nSending Unlock message (Note: This message should be sent by the same component that locked the component.) Sending Unlock message with an administrative Prefix.","title":"Lock"},{"location":"/commons/create-component.html#csw-services-injection","text":"Common Software provides a set of CSW Services provided through the TLA. They are injected into the ComponentHandlers class in the constructor in a CswContext object. This object provides the following services through their respective APIs:\nLocation Service Event Service Alarm Service (Client API) Time Service (Scheduler) Configuration Service (Client API) Logging Service (Logger Factory)\nAnd the following information and support utilities:\nComponent Configuration (ComponentInfo) Command Service Command Response Manager Current State Publisher Actor (intended for HCDs)","title":"CSW Services Injection"},{"location":"/commons/create-component.html#logging","text":"csw-framework provides a LoggerFactory in the CswContext injected in the constructor of ComponentHandlers. The LoggerFactory will have the component’s Prefix predefined so long messages have a clear source. The component developer is expected to and must use this factory to log messages that work with the centralized logging facility.\nLogging works much like other popular loggers such as log4j. However, with the development of log management tools such as logstash, the emphasis on log message formatting has been to write structured logging messages in JSON format, so that they can easily be ingested, stored, and searched. Plain text writing to stdout is also supported. More details on how to use logging can be found here.","title":"Logging"},{"location":"/commons/create-component.html#","text":"Let’s use logging to flesh out some of our command handlers. The template will instantiate a logger for you to use by constructing one from the LoggerFactory from in the CswContext passed in the constructor, instantiated as a log object.\nAdd some simple log messages in the initialize and onShutdown hooks, and to the onLocationTrackingEvent hook as well, although we won’t be using it for this HCD:\nScala var maybePublishingGenerator: Option[Cancellable] = None\noverride def initialize(): Future[Unit] = {\n  log.info(\"In HCD initialize\")\n  maybePublishingGenerator = Some(publishCounter())\n  Future.unit\n}\n\noverride def onLocationTrackingEvent(trackingEvent: TrackingEvent): Unit = {\n  log.debug(s\"TrackingEvent received: ${trackingEvent.connection.name}\")\n}\n\noverride def onShutdown(): Future[Unit] = {\n  log.info(\"HCD is shutting down\")\n  Future.unit\n} Java private Optional<Cancellable> maybePublishingGenerator = Optional.empty();\n\n@Override\npublic CompletableFuture<Void> jInitialize() {\n    return CompletableFuture.runAsync(() -> {\n        log.info(\"In HCD initialize\");\n        maybePublishingGenerator = Optional.of(publishCounter());\n    });\n}\n\n@Override\npublic void onLocationTrackingEvent(TrackingEvent trackingEvent) {\n    log.debug(() -> \"TrackingEvent received: \" + trackingEvent.connection().name());\n}\n\n@Override\npublic CompletableFuture<Void> jOnShutdown() {\n    return CompletableFuture.runAsync(() -> log.info(\"HCD is shutting down\"));\n}\nIn the example code, you’ll notice we have added some functionality to start publishing events. We will cover the Event Service later. You can leave that code out for now.\nNext we’ll discuss handling commands.","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#receiving-commands","text":"A command is something that carries some metadata and a set of parameters. A component sends commands to other components to execute actions. CSW defines three kinds of commands as follows:\nSetup : Contains goal, command, or demand information to be used to configure the target OMOA component. Observe: Contains goal or demand information to be used by a detector. system. Properties and their value types will be standardized by the ESW subsystem. Wait: Sequencer only. Instructs a sequencer to pause until told to continue.\nA Sequencer receives a Sequence, which is a list of the above commands that are executed sequentially.\nMore details about creating commands can be found here.\nWhenever a command is sent to a component, it is sent using a Command Service method. There are two general ways to send a command:\nsubmit: A command is sent using submit when a completion result is expected from the destination component. There are two options: submit and submitAndWait. oneway: A command is sent using oneway when the completion of command is not expected from the destination component.","title":"Receiving Commands"},{"location":"/commons/create-component.html#validation","text":"When any command is received by a component using submit or oneway, the Top Level Actor will first call the validateCommand hook of ComponentHandlers. Component developers are expected to perform appropriate validation of a command to determine if it is valid to execute the requested actions and then return a ValidateCommandResponse. The ValidateCommandResponse returned from this handler will be returned to the sender directly by csw-framework if the command fails validation.\nThe component developer should return either an Accepted response or an or Invalid response specifying whether the command is valid to be executed or not. CSW defines a set of CommandIssues for use in validation here that should be used within Invalid responses.\nIf the handler is being called as part of a submit or oneway call, the command will automatically be passed on to the onSubmit or onOneway handlers (see Command Response) only if the validation handler returns a ValidationCommandResponse of Accepted. Otherwise, the Invalid response is returned to the caller immediately.\nDifferent types of command responses and their significance can be found here.","title":"Validation"},{"location":"/commons/create-component.html#","text":"Let’s add some command validation to our HCD. For our sample HCD, we will only handle one command, sleep, in which we will cause the HCD to sleep for the time specified in a parameter of the command. This will simulate a long-running command.\nAdd some code to ensure the command we receive is the sleep command, and return an Invalid response if not. You could imagine much more checking could be added, such as checking the types and values of the parameters of our sleep command, but we will keep it simple for our demonstration.\nScala override def validateCommand(runId: Id, controlCommand: ControlCommand): ValidateCommandResponse = {\n  log.info(s\"Validating command: ${controlCommand.commandName.name}\")\n  controlCommand.commandName.name match {\n    case \"sleep\" => Accepted(runId)\n    case x       => Invalid(runId, CommandIssue.UnsupportedCommandIssue(s\"Command $x. not supported.\"))\n  }\n} Java @Override\npublic CommandResponse.ValidateCommandResponse validateCommand(Id runId, ControlCommand controlCommand) {\n    String commandName = controlCommand.commandName().name();\n    log.info(() -> \"Validating command: \" + commandName);\n    if (commandName.equals(\"sleep\")) {\n        return new CommandResponse.Accepted(runId);\n    }\n    return new CommandResponse.Invalid(runId, new CommandIssue.UnsupportedCommandIssue(\"Command \" + commandName + \". not supported.\"));\n}","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#command-response","text":"The response returned from validateCommand handler of ComponentHandlers will be received by the Supervisor. If the response returned was Accepted, then it either calls the onSubmit hook or the onOneway hook of ComponentHandlers depending on the whether submit, submitAndWait, or oneway was used to send the command.\nIf sent with submit or submitAndWait and the validation response is Accepted, the framework calls the onSubmit hook of ComponentHandlers. The return value of onSubmit is then returned to the sender, which can be a Completed for commands that return quickly, or Started for long running commands. If the response from validation was Invalid, this is returned to the sender of the command without calling the onSubmit or onOneway handler.\nIf onSubmit returns Started, the component’s CommandResponseManager keeps track of the status of long-running submit commands. The sender of a Started command (and any component, really) can query a Started command’s status or wait for the final response using queryFinal of the CommandService API.\nIf a command is sent using oneway, the validation handler if called first. The onOneway handler of ComponentHandlers is also called when the validation result is Accepted. The validation response is always sent back to the sender as the response for a oneway command. There is no final response from a ownway command and no way to wait for it using query or queryFinal.\nThe CommandService class provides convenient API methods for communicating with other components, and should be the primary means of sending commands to other components. This will be described in the next tutorial section, Sending Commands.\nWhen the onSubmit hook is called and Started is returned, it is the responsibility of the component developer to update the sender with the final status of the Started command when the actions complete using the CommandResponseManager API. An instance of CommandResponseManager is provided in the CswContext object in ComponentHandlers and should be injected in any worker actor or other actor/class created for the component that needs it.\nMore details on the methods available in CommandResponseManager can be found here.","title":"Command Response"},{"location":"/commons/create-component.html#","text":"We will implement command handling in the onSubmit hook. Note that this hook actually receives a ControlCommand as an argument, which can be either a Setup or an Observe. We will use pattern matching to handle the command if it is a Setup and forward to an onSetup handling method. Observe commands will be ignored and returned with as Invalid.\nScala override def onSubmit(runId: Id, controlCommand: ControlCommand): SubmitResponse = {\n  log.info(s\"Handling command: ${controlCommand.commandName}\")\n\n  controlCommand match {\n    case setupCommand: Setup => onSetup(runId, setupCommand)\n    case observeCommand: Observe => // implement (or not)\n      Error(runId, \"Observe not supported\")\n  }\n}\n\ndef onSetup(runId: Id, setup: Setup): SubmitResponse = {\n  val sleepTimeKey: Key[Long] = KeyType.LongKey.make(\"SleepTime\")\n\n  // get param from the Parameter Set in the Setup\n  val sleepTimeParam: Parameter[Long] = setup(sleepTimeKey)\n\n  // values of parameters are arrays. Get the first one (the only one in our case) using `head` method available as a convenience method on `Parameter`.\n  val sleepTimeInMillis: Long = sleepTimeParam.head\n\n  log.info(s\"command payload: ${sleepTimeParam.keyName} = $sleepTimeInMillis\")\n\n  workerActor ! Sleep(runId, sleepTimeInMillis)\n\n  Started(runId)\n} Java @Override\npublic CommandResponse.SubmitResponse onSubmit(Id runId, ControlCommand controlCommand) {\n    log.info(() -> \"Handling command: \" + controlCommand.commandName());\n\n    if (controlCommand instanceof Setup) {\n        onSetup(runId, (Setup) controlCommand);\n        return new CommandResponse.Started(runId);\n    } else if (controlCommand instanceof Observe) {\n        // implement (or not)\n    }\n    return new CommandResponse.Error(runId, \"Observe command not supported\");\n}\n\nprivate void onSetup(Id runId, Setup setup) {\n    Key<Long> sleepTimeKey = JKeyType.LongKey().make(\"SleepTime\", JUnits.millisecond);\n\n    // get param from the Parameter Set in the Setup\n    Optional<Parameter<Long>> sleepTimeParamOption = setup.jGet(sleepTimeKey);\n\n    // values of parameters are arrays.  Get the first one (the only one in our case) using `head` method available as a convenience method on `Parameter`.\n    if (sleepTimeParamOption.isPresent()) {\n        Parameter<Long> sleepTimeParam = sleepTimeParamOption.orElseThrow();\n        long sleepTimeInMillis = sleepTimeParam.head();\n\n        log.info(() -> \"command payload: \" + sleepTimeParam.keyName() + \" = \" + sleepTimeInMillis);\n\n        workerActor.tell(new Sleep(runId, sleepTimeInMillis));\n    }\n}\nIn our example, the sleep command has one parameter called SleepTime. We retrieve this parameter from the Setup by creating a Key to this parameter using the name and type, and then calling an apply method on the Setup (the setup(sleepKey) shorthand) which finds the matching Parameter in the Setup’s ParameterSet (use the Setup.jget() method in Java). By doing this, the Parameter is returned with the proper typing, and so the values retrieved from the Parameter are typed as well. Note, all values are stored as an array, so we get our single value for sleepTime by using the head method available as a convenience method on ParameterSet.\nAt this point, to prevent our HCD from blocking and simulate a long-running command, we pass the sleep function off to a worker actor, which we will specify elsewhere in this class.\nUse of an External Class The worker actor can be defined in a separate class, but writing it as an internal class allows us to use the logging facility and CommandResponseManager without having to inject them into a new actor class. Additional versions of the tutorial code show a separate, enhanced worker actor class.\nNote that our onSetup command handling logic returns a Started response. This indicates that the command is a long-running command and will be finished after some time, and that the final result will be posted to the CommandResponseManager. The submitAndWait command in the CommandService is implemented such that when it receives a Started response, it automatically issues a queryFinal to the commanded component to await the final completion response. When the command is updated using the CommandResponseManager, the Future returned by submitAndWait command is completed with this value. Note that in this code, there is a chance that there is no sleepTime parameter. Good validation code would ensure this so onSetup does not need to worry. Enhanced versions of the tutorial code show this.\nScala sealed trait WorkerCommand\ncase class Sleep(runId: Id, timeInMillis: Long) extends WorkerCommand\n\nprivate val workerActor =\n  ctx.spawn(\n    Behaviors.receiveMessage[WorkerCommand](msg => {\n      msg match {\n        case sleep: Sleep =>\n          log.trace(s\"WorkerActor received sleep command with time of ${sleep.timeInMillis} ms\")\n          // simulate long running command\n          val when: UTCTime = UTCTime.after(FiniteDuration(sleep.timeInMillis, MILLISECONDS))\n          timeServiceScheduler.scheduleOnce(when) {\n            commandResponseManager.updateCommand(CommandResponse.Completed(sleep.runId))\n          }\n        case _ => log.error(\"Unsupported message type\")\n      }\n      Behaviors.same\n    }),\n    \"WorkerActor\"\n  ) Java private interface WorkerCommand {\n}\n\nprivate static final class Sleep implements WorkerCommand {\n    private final Id runId;\n    private final long timeInMillis;\n\n    private Sleep(Id runId, long timeInMillis) {\n        this.runId = runId;\n        this.timeInMillis = timeInMillis;\n    }\n}\n\nprivate ActorRef<WorkerCommand> createWorkerActor() {\n    return actorContext.spawn(\n            Behaviors.receiveMessage(msg -> {\n                if (msg instanceof Sleep) {\n                    Sleep sleep = (Sleep) msg;\n                    log.trace(() -> \"WorkerActor received sleep command with time of \" + sleep.timeInMillis + \" ms\");\n                    UTCTime when = UTCTime.after(new FiniteDuration(sleep.timeInMillis, MILLISECONDS));\n                    Runnable task = () -> cswCtx.commandResponseManager().updateCommand(new CommandResponse.Completed(sleep.runId));\n                    // simulate long running command that updates CRM when completed\n                    cswCtx.timeServiceScheduler().scheduleOnce(when, task);\n                } else {\n                    log.error(\"Unsupported message type\");\n                }\n                return Behaviors.same();\n            }),\n            \"WorkerActor\"\n    );\n}\nThis worker actor takes the time passed in the message and sleeps that amount using the TimeService scheduling API. When the time is expired, the worker updates the CommandResponseManager that the command is Completed.","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#events","text":"CSW Events have a similar structure to commands in that along with a name and a prefix (used to represent the source of the event), they include data represented in the Event in a set of parameters. More details about events can be found here.\nAccess to the Event Service is in the CswContext object passed in to the handlers class in the constructor. The Event Service provides a factory method to create a “default” publisher and subscriber, which can be accessed in various parts of your code to reuse a single connection to the service. In most cases, reusing this connection will provide the performance needed. But if you prefer to create new connections, custom publishers and subscribers can be constructed. See the manual on the Event Service for more information.\nPublishers have an API that allows the publishing of a single event, a stream of events, or periodic events created by an EventGenerator, which is simply a function that returns an Event.","title":"Events"},{"location":"/commons/create-component.html#","text":"Let’s add a publisher to our component. We will use the default publisher that will periodically publish events generated by an EventGenerator.\nScala import scala.concurrent.duration._\nprivate def publishCounter(): Cancellable = {\n  var counter = 0\n  def incrementCounterEvent() = Option {\n    counter += 1\n    val param: Parameter[Int] = KeyType.IntKey.make(\"counter\").set(counter)\n    SystemEvent(componentInfo.prefix, EventName(\"HcdCounter\")).add(param)\n  }\n\n  log.info(\"Starting publish stream.\")\n  eventService.defaultPublisher.publish(incrementCounterEvent(), 2.second, err => log.error(err.getMessage, ex = err))\n}\n\nprivate def stopPublishingGenerator(): Unit = {\n  log.info(\"Stopping publish stream\")\n  maybePublishingGenerator.foreach(_.cancel)\n} Java private int counter = 0;\n\nprivate Optional<Event> incrementCounterEvent() {\n    counter += 1;\n    Parameter<Integer> param = JKeyType.IntKey().make(\"counter\", JUnits.NoUnits).set(counter);\n    return Optional.of(new SystemEvent(cswCtx.componentInfo().prefix(), new EventName(\"HcdCounter\")).add(param));\n}\n\nprivate Cancellable publishCounter() {\n    log.info(\"Starting publish stream.\");\n    return cswCtx.eventService().defaultPublisher().publish(this::incrementCounterEvent, java.time.Duration.ofSeconds(2));\n}\n\nprivate void stopPublishingGenerator() {\n    log.info(\"Stopping publish stream\");\n    maybePublishingGenerator.ifPresent(Cancellable::cancel);\n}\nWe encapsulate the starting of the publishing in our method publishCounter. Our EventGenerator is the incrementCounterEvent method which increments our integer variable counter and stores it in the ParameterSet of a new SystemEvent and returns it. Once our defaultPublisher is resolved, we pass in a reference to incrementCounterEvent and specify a period of 2 seconds. We log a message when publishing the event so that it can be observed when running the component.\nThe publish method returns a Cancellable type in a future. When the publishing is set up, the Cancellable can be used to stop the event generator. We demonstrate its usage in the stopPublishingGenerator method, although this method is not called in our tutorial.\nWe will start this publishing when our component initializes, so we return to our initialize method and add a call to our publishCounter method. We save a reference to the Cancellable object for future use in our stopPublishingGenerator method.\nScala var maybePublishingGenerator: Option[Cancellable] = None\noverride def initialize(): Future[Unit] = {\n  log.info(\"In HCD initialize\")\n  maybePublishingGenerator = Some(publishCounter())\n  Future.unit\n}\n\noverride def onLocationTrackingEvent(trackingEvent: TrackingEvent): Unit = {\n  log.debug(s\"TrackingEvent received: ${trackingEvent.connection.name}\")\n}\n\noverride def onShutdown(): Future[Unit] = {\n  log.info(\"HCD is shutting down\")\n  Future.unit\n} Java private Optional<Cancellable> maybePublishingGenerator = Optional.empty();\n\n@Override\npublic CompletableFuture<Void> jInitialize() {\n    return CompletableFuture.runAsync(() -> {\n        log.info(\"In HCD initialize\");\n        maybePublishingGenerator = Optional.of(publishCounter());\n    });\n}\n\n@Override\npublic void onLocationTrackingEvent(TrackingEvent trackingEvent) {\n    log.debug(() -> \"TrackingEvent received: \" + trackingEvent.connection().name());\n}\n\n@Override\npublic CompletableFuture<Void> jOnShutdown() {\n    return CompletableFuture.runAsync(() -> log.info(\"HCD is shutting down\"));\n}","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#starting-csw-services","text":"Before we run our application, we must first start the Location Service and the Event Service. A script has been provided to simplify the starting and stopping of CSW services, and is included in the application bundle that comes with each release. The application bundle is called\ncsw-apps-2.0.0-RC2.zip\nand the script is named csw-services.sh. The version must match the CSW release in use!\nThe csw-services.sh script has two basic commands: start and stop. The start command can start specific services using passed in flags, or all services without any. Services are started on default ports but those ports can be overridden using command line arguments. It is important to pass in a network interface name that is appropriate for your system. These can be obtained using ifconfig on Linux and Mac computers. en0 typically, works for most machines. This can alternatively be set using the environment variable INTERFACE_NAME. Setting the interface name at the command with this script also sets this environment variable.\nEnvironment Variables used by CSW There are several environment variables that are used by the CSW framework and services. The environment variables used by CSW services are specified here.\nTo get information on the arguments for the tool, use csw-services.sh --help or csw-services.sh start --help.","title":"Starting CSW Services"},{"location":"/commons/create-component.html#","text":"Let’s go ahead and start our CSW Services using the script. Go to the release page and download and unpack the CSW application bundle. Then go into the bin directory and enter the command\n./csw-services.sh start -a -i <iname>\nwhere <iname> is your interface name (e.g. en0).","title":"Tutorial: Developing an HCD"},{"location":"/commons/create-component.html#building-and-running-component-in-standalone-mode","text":"Once the component is ready, it is started using the ContainerCmd object in a standalone mode. The details for starting the ContainerCmd in a standalone mode can be found here.\nThere are various ways to build and run the project. A simple way during development is to to use sbt to run it. The sbt command runMain can be used to specify an application with a main method and run it with arguments specified at the command line. When this command is executed, sbt will take care of any downloading of dependencies, compiling, or building necessary to run your application.\nOur template includes a wrapper application around ContainerCmd that we can use in the deployment module. To run our HCD in a standalone mode, go to the project root directory and type sbt \"<deploy-module>/runMain <mainClass> --local --standalone <path-to-config-file>\", where\n<deploy-module> is the name of the deployment module created by the template (sample-deploy if using defaults) <mainClass> is the full class name of our ContainerCmd application, which the template names <package>.<name>deploy.<Name>ContainerCmdApp. If you accept the defaults for the template, it will be org.tmt.esw.sampledeploy.SampleContainerCmdApp. If you are having problems determining the class name, use sbt <deploy-module>/run and it will prompt you the possibilities. <path-to-config-file> is the filename, which can be an absolute path or relative to the directory of the deployment module. If using defaults, this would be src/main/resources/SampleHcdStandalone.conf for Scala, and src/main/resources/JSampleHcdStandalone.conf for Java.\nSo if using the template defaults, the full command would be\nScala sbt \"sample-deploy/runMain org.tmt.esw.sampledeploy.SampleContainerCmdApp --local --standalone src/main/resources/SampleHcdStandalone.conf\"\n Java sbt \"sample-deploy/runMain org.tmt.esw.sampledeploy.SampleContainerCmdApp --local --standalone src/main/resources/JSampleHcdStandalone.conf\"\nTo run the component using the deployment package, perform the following steps:\nRun sbt <project>/universal:packageBin, where <project> is your deployment module (e.g. sample-deploy). This will create self-contained zip in <project>/target/universal directory Unzip generated zip file and enter into bin directory Run the ./<project>-cmd-app --local --standalone <path-to-local-config-file-to-start-the-component>\nAlternatively, you can run sbt stage, which installs the application under target/universal/stage/bin.","title":"Building and Running component in standalone mode"},{"location":"/commons/create-component.html#enhanced-tutorial-versions","text":"There are three supplemental versions of the tutorial Assembly and HCD called: basic, moderate, and full. The basic version is similar to the tutorial example here with possible best practices. Two other versions are included that introduce ways of programming components along with increasing functionality. The following table shows the features of each version. Moderate adds functionality to Basic, and Full adds functionality to Moderate.\nThe code for the enhanced tutorials is in the CSW distribution at the following locations:\nScala versions are here Scala test code is here Java version of basic is here Java test code is here\nAt this time there is no Java versions of moderate and full.","title":"Enhanced Tutorial Versions"},{"location":"/commons/create-component.html#basic","text":"","title":"Basic"},{"location":"/commons/create-component.html#basic-hcd","text":"Implements a simple sleep worker in HCD using Time Service scheduler. Provides basic command validation. Shows one way to write onSetup handler. Shows how to publish events.","title":"Basic HCD"},{"location":"/commons/create-component.html#basic-assembly","text":"Provides simple validation in HCD and Assembly. Assembly shows how to use onTrackEvent to manage CommandService creation and loss of HCD. Code shows how to send a command while noticing if HCD is available. Simulates different commands that use the sleep functionality of HCD. Shows how to use CommandResponseManager to update a long-running command. Shows a “complex” command that uses CommandResponseManager queryFinalAll call. Shows how to subscribe to events and process events.\nIncludes standalone HCD tests and Assembly+HCD integration tests that start a container with both components.","title":"Basic Assembly"},{"location":"/commons/create-component.html#moderate","text":"","title":"Moderate"},{"location":"/commons/create-component.html#moderate-hcd","text":"Sleep worker is interruptable allowing sleep command to be cancelled. Supports command that will cancel the “long command”. Uses validation code shared with Assembly. Uses “info” file that is shared between HCD and Assembly.","title":"Moderate HCD"},{"location":"/commons/create-component.html#moderate-assembly","text":"Adds command to cancel “long command”. Keeps track of long command runId. Uses validation code shared with Assembly. Uses “info” file that is shared between HCD and Assembly.\nIncludes standalone HCD tests and Assembly+HCD integration tests that start a container with both components. Adds test to start a long command and cancel it.","title":"Moderate Assembly"},{"location":"/commons/create-component.html#full","text":"","title":"Full"},{"location":"/commons/create-component.html#full-hcd","text":"Adds a worker monitor that tracks data allowing any sleep command to be cancelled. Sleep worker enhanced to work with worker monitor. Implements cancel “long command” using new functionality.","title":"Full HCD"},{"location":"/commons/create-component.html#full-assembly","text":"Uses worker monitor to associate runIds with sub-commandIds. Imlements cancel “long command” using worker monitor.\nIncludes standalone HCD tests and Assembly+HCD integration tests that start a container with both components. Integration test to start a long command and cancel it.","title":"Full Assembly"},{"location":"/commons/multiple-components.html","text":"","title":"Multiple Components"},{"location":"/commons/multiple-components.html#multiple-components","text":"In this part of the tutorial, we will demonstrate functionality involving multiple components. We will do this by creating an Assembly, demonstrating how to deploy the Assembly and an HCD in a container, and having them communicate with each other.","title":"Multiple Components"},{"location":"/commons/multiple-components.html#creating-an-assembly","text":"Similar to the HCD in the previous page, to create an Assembly, the component developer needs to implement the ComponentHandlers for the Assembly. More details about implementing ComponentHandlers can be found here.","title":"Creating an Assembly"},{"location":"/commons/multiple-components.html#","text":"If using the giter8 template with the default parameters, our ComponentHandlers class will be the SampleAssemblyHandlers class (JSampleAssemblyHandlers in Java), and the factory will be SampleAssemblyBehaviorFactory (JSampleAssemblyBehaviorFactory in Java).\nLike we did for the HCD, let’s add some log messages for the initialize and onShutdown hooks, but not the onTrackingLocationEvent hook. We’ll cover that in more detail later.\nScala private var maybeEventSubscription: Option[EventSubscription] = None\noverride def initialize(): Future[Unit] = {\n  log.info(\"In Assembly initialize\")\n  maybeEventSubscription = Some(subscribeToHcd())\n  Future.unit\n}\n\noverride def onShutdown(): Future[Unit] = {\n  log.info(\"Assembly is shutting down.\")\n  Future.unit\n} Java private Optional<IEventSubscription> maybeEventSubscription = Optional.empty();\n\n@Override\npublic CompletableFuture<Void> jInitialize() {\n    return CompletableFuture.runAsync(() -> {\n        log.info(\"In Assembly initialize\");\n        maybeEventSubscription = Optional.of(subscribeToHcd());\n    });\n}\n\n@Override\npublic CompletableFuture<Void> jOnShutdown() {\n    return CompletableFuture.runAsync(() -> log.info(\"Assembly is shutting down.\"));\n}\nOnce again, ignore the code about setting up the event subscription. This will be covered later when we discuss subscribing to events.","title":"Tutorial: Developing an Assembly"},{"location":"/commons/multiple-components.html#component-configuration-componentinfo-","text":"Also as with the HCD, we need to create a ComponentInfo file for the Assembly. The following shows an example of ComponentInfo file for an Assembly:\nScala componentType = assembly\nbehaviorFactoryClassName = \"org.tmt.nfiraos.sampleassembly.SampleAssemblyBehaviorFactory\"\nprefix = \"nfiraos.SampleAssembly\"\nlocationServiceUsage = RegisterAndTrackServices\nconnections = [\n  {\n    prefix: \"nfiraos.SampleHcd\"\n    componentType: hcd\n    connectionType: akka\n  }\n]\n Java componentType = assembly\nbehaviorFactoryClassName = \"org.tmt.nfiraos.sampleassembly.JSampleAssemblyBehaviorFactory\"\nprefix = \"nfiraos.JSampleAssembly\"\nlocationServiceUsage = RegisterAndTrackServices\nconnections = [\n  {\n    prefix: \"nfiraos.JSampleHcd\"\n    componentType: hcd\n    connectionType: akka\n  }\n]\nConnections in ComponentInfo There is a section for listing connections. These are the connections that the component will automatically track and can be other components or services. For instance, an Assembly may command one or more HCDs. Entries here can be used to track those HCDs and the Assembly will be notified if one of the tracked HCDs starts up, crashes, or shuts down. In some cases, it may make sense to track services such as the Event Service. These connections can also be specified for HCDs, but HCDs should not have any component dependencies.\nThe above shows a configuration file for running in standalone mode. If we want to run both the assembly and HCD in a container, the ComponentInfo file combined entries for both components and looks like this:\nScala name = \"SampleContainer\"\ncomponents: [\n  {\n    prefix = \"nfiraos.SampleAssembly\"\n    componentType = assembly\n    behaviorFactoryClassName = \"org.tmt.nfiraos.sampleassembly.SampleAssemblyBehaviorFactory\"\n    locationServiceUsage = RegisterAndTrackServices\n    connections = [{\n      prefix: \"nfiraos.SampleHcd\"\n      componentType: hcd\n      connectionType: akka\n    }]\n  },\n  {\n    prefix = \"nfiraos.SampleHcd\"\n    componentType = hcd\n    behaviorFactoryClassName = \"org.tmt.nfiraos.samplehcd.SampleHcdBehaviorFactory\"\n    locationServiceUsage = RegisterOnly\n  }\n] Java name = \"JSampleContainer\"\ncomponents: [\n  {\n    prefix = \"NFIRAOS.JSampleAssembly\"\n    componentType = assembly\n    behaviorFactoryClassName = \"org.tmt.nfiraos.sampleassembly.JSampleAssemblyBehaviorFactory\"\n    locationServiceUsage = RegisterAndTrackServices\n    connections = [\n      {\n        prefix: \"NFIRAOS.JSampleHcd\"\n        componentType: hcd\n        connectionType: akka\n      }\n    ]\n  },\n  {\n    prefix = \"NFIRAOS.JSampleHcd\"\n    componentType = hcd\n    behaviorFactoryClassName = \"org.tmt.nfiraos.samplehcd.JSampleHcdBehaviorFactory\"\n    locationServiceUsage = RegisterOnly\n  }\n]\nMore details about each configuration and its significance can be found here.\nAnother sample container configuration file can be found here.","title":"Component Configuration (ComponentInfo)"},{"location":"/commons/multiple-components.html#tracking-dependencies","text":"The connections that are defined in the ComponentInfo file for an Assembly will be tracked by the csw-framework. For each connection the following details are configured:\nScala {\nprefix: \"nfiraos.SampleHcd\"\ncomponentType: hcd\nconnectionType: akka\n}\n Java {\nprefix: \"nfiraos.JSampleHcd\"\ncomponentType: hcd\nconnectionType: akka\n}\nThe configuration includes the prefix of the component consisting of a valid subsystem and the component’s name. The prefix, component type (hcd, service, etc), and the connection type (akka, http, tcp) will be used to create a Connection object. The Connection object will be then used to track the location of a component using Location Service.\nThe Location object has one of the following types:\nAkkaLocation: Contains the remote address of the actorRef. The actorRef will be the Supervisor actor of a component. HttpLocation: Holds the HTTP URI of the web server, e.g. Configuration Service TcpLocation: Represents a TCP URI of the server or service, e.g. Event Service\nMore details about tracking a component using the Location Service can be found here.","title":"Tracking Dependencies"},{"location":"/commons/multiple-components.html#onlocationtrackingevent-handler","text":"For all the tracked connections, whenever a Location is changed, added, or removed, one of the following events is generated:\nLocationUpdated: a location was added or changed LocationRemoved: a location is no longer available on the network\nWhenever such an event is generated, the Top level actor will call the onLocationTrackingEvent hook of ComponentHandlers with the event (LocationUpdated or LocationRemoved) as parameter of the handler.\nMore details about tracking connections can be found here.","title":"onLocationTrackingEvent Handler"},{"location":"/commons/multiple-components.html#","text":"For our sample component, we will set it up so that when the HCD is found by the Location Service, we will immediately send a command to it. We will do this by using the Location obtained to create a CommandService reference (see below) to the HCD and store at as a variable in the TLA. This can be a good way to easily track the connection and keep a reference up to date. The code then sends a shortCommand command through the onSetup handler. Sending commands from the location event tracking handler is not a best practice; it is shown here for demonstration purposes.\nSimilarly, if the HCD is removed, a message is logged and the Command Service variable is set to none.\nScala override def onLocationTrackingEvent(trackingEvent: TrackingEvent): Unit = {\n  log.debug(s\"onLocationTrackingEvent called: $trackingEvent\")\n  trackingEvent match {\n    case LocationUpdated(location) =>\n      val hcd = CommandServiceFactory.make(location.asInstanceOf[AkkaLocation])(ctx.system)\n      commandSender ! SendCommand(hcd)\n    case LocationRemoved(_) => log.info(\"HCD no longer available\")\n  }\n} Java @Override\npublic void onLocationTrackingEvent(TrackingEvent trackingEvent) {\n    log.debug(() -> \"onLocationTrackingEvent called: \" + trackingEvent.toString());\n    if (trackingEvent instanceof LocationUpdated) {\n        LocationUpdated updated = (LocationUpdated) trackingEvent;\n        Location location = updated.location();\n        ICommandService hcd = CommandServiceFactory.jMake(location, actorContext.getSystem());\n        commandSender.tell(new SendCommand(hcd));\n    } else if (trackingEvent instanceof LocationRemoved) {\n        log.info(\"HCD no longer available\");\n    }\n}","title":"Tutorial: Developing an Assembly"},{"location":"/commons/multiple-components.html#trackconnection","text":"If the component developer wants to track a connection that is not configured in its configuration file then it can use the trackConnection method provided by csw-framework in ComponentHandlers. The trackConnection method will take the Connection instance. Information on how to create a connection instance can be found here.\nNote Connections tracked by csw-framework (from the configuration file) or by a component developer using the trackConnection method both will be received in the onLocationTrackingEvent hook of ComponentHandlers.","title":"trackConnection"},{"location":"/commons/multiple-components.html#sending-commands","text":"From the location information obtained either by tracking dependencies or manually resolving a location, a CommandService instance can be created to provide a command interface to the component. The following snippet, not from our tutorial, shows how to obtain a CommandService reference using by resolving a location using the Location Service.\nScala implicit val system: ActorSystem[Nothing] = ctx.system\n\nval eventualCommandService: Future[CommandService] =\n  cswCtx.locationService.resolve(hcdConnection.of[AkkaLocation], 5.seconds).map {\n    case Some(hcdLocation: AkkaLocation) => CommandServiceFactory.make(hcdLocation)\n    case _                               => throw HcdNotFoundException()\n  }\n\neventualCommandService.foreach { commandService =>\n  hcd = commandService\n} Java CompletableFuture<Optional<AkkaLocation>> resolvedHcdLocation = locationService.resolve(hcdConnection, Duration.ofSeconds(5));\n\nCompletableFuture<ICommandService> eventualCommandService = resolvedHcdLocation.thenApply((Optional<AkkaLocation> hcdLocation) -> {\n    if (hcdLocation.isPresent())\n        return CommandServiceFactory.jMake(hcdLocation.orElseThrow(), ctx.getSystem());\n    else\n        throw new HcdNotFoundException();\n});\n\neventualCommandService.thenAccept((jcommandService) -> hcd = jcommandService);\nIf a component wants to send a command to another component, it uses a CommandService instance. The creation of aCommandService instance and its usage can be found here.","title":"Sending Commands"},{"location":"/commons/multiple-components.html#handling-short-and-long-running-commands","text":"The Command Service provides a two different ways of performing Submit commands. In particular, for sending single commands, there are two flavors: submit and submitAndWait. Both commands take a Setup or Observe and return a Future response encapsulating a SubmitResponse type.\nTo understand the difference between submit and submitAndWait and how to choose between them it is necessary to understand how commands are processed in the destination component, and what you as the developer need to accomplish. Every submitted command is first validated by calling the validateCommand handler. If validateCommand returns Invalid, the submit returns immediately. If validateCommand returns Accepted, the onSubmit handler is called.\nIf a command is short, less than 1 second, it completes quickly with the response of Completed or an Error response. In this case submit and submitAndWait behave the same working like a function call or RPC command. The Completed response can also return a result if needed.\nIf the command is long-running, longer than 1 second, an onSubmit handler should start the actions and return a Started response. The sender of the command obtains the final response of the command by polling using the Command Service query call or by waiting using queryFinal. When an onSubmit handler returns Started it is expected to update the command with the final response when the actions complete using the updateCommand call of the CommandResponseManager that is provided to the TLA in the CswContext.\nOn the sender side, submitAndWait doesn’t complete until the long-running command completes and returns the final SubmitResponse. That is to say, if the Future completes, no matter whether the command has an immediate response or is long-running, is invalid, or encounters an error at any point, the Future completes with the final response. If there is an exception, which may occur if the command times out, that must be handled explicitly using Future exception handling (this can be done many ways; an example is given below).","title":"Handling Short and Long-Running Commands"},{"location":"/commons/multiple-components.html#","text":"TWe use our worker actor to submit the command to the HCD, and then subscribe to the HCD’s CommandResponseManager for command completion, using the shortcut submitAndWait.\nScala : @@snip SampleAssemblyHandlers.scala { #worker-actor }\nJava : @@snip SampleAssemblyHandlers.java { #worker-actor }\nWaiting is not Waiting Even through submitAndWait waits, it is not blocking. The Assembly and HCD are still available for commands. The support for commands is all asynchronous. For example, complexCommand sends two commands to the HCD, which is happy to execute two sleeps concurrently. The Assembly is also available for other commands. For instance, other versions of the SampleAssembly show how to cancel a long-running command. Note also that if your Assembly or HCD can only execute one command at a time, your code must test for this during validation and stop multiple commands from executing.","title":"Tutorial: Developing an Assembly"},{"location":"/commons/multiple-components.html#matchers","text":"When a component sends a command as Oneway to another component, it may be interested in knowing the receiver component’s CurrentState and match it against a desired state. In order to do that, the component developer can use the onewayAndMatch method of CommandService or use oneway and then use a Matcher explicitly to match a desired state with current state.\nMore details on how to use Matcher can be found here.","title":"Matchers"},{"location":"/commons/multiple-components.html#pubsub-connection","text":"A component might need to subscribe to the current state of any other component provided it knows the location of that component. In order to subscribe to current state, it may use the subscribeCurrentState method of the CommandService. More details about the usage of subscribeCurrentState can be found here.\nIf a component wants to publish its current state then it can use the currentStatePublisher provided by csw-framework in the CswContext object passed into ComponentHandlers. More details about the usage of currentStatePublisher can be found here.","title":"PubSub Connection"},{"location":"/commons/multiple-components.html#subscribing-to-events","text":"To subscribe to events, a subscriber is accessed in a similar way to publishing. Typically a defaultSubscriber is obtained, but additional subscribers with their own connection can be created.\nThe subscribe API specifies a set of Events to subscribe to and then specifies how the events should be handled. This can be a callback, an Actor reference to receive the Event as a message, or as a stream to allow flow operations to be applied.","title":"Subscribing to Events"},{"location":"/commons/multiple-components.html#","text":"We will setup our subscription to the counter events generated by our HCD in the subscribeToHCD method.\nScala private val counterEventKey = EventKey(Prefix(\"nfiraos.samplehcd\"), EventName(\"HcdCounter\"))\nprivate val hcdCounterKey   = KeyType.IntKey.make(\"counter\")\n\nprivate def processEvent(event: Event): Unit = {\n  log.info(s\"Event received: ${event.eventKey}\")\n  event match {\n    case e: SystemEvent =>\n      e.eventKey match {\n        case `counterEventKey` =>\n          val counter = e(hcdCounterKey).head\n          log.info(s\"Counter = $counter\")\n        case _ => log.warn(\"Unexpected event received.\")\n      }\n    case e: ObserveEvent => log.warn(\"Unexpected ObserveEvent received.\") // not expected\n  }\n}\n\nprivate def subscribeToHcd(): EventSubscription = {\n  log.info(\"Starting subscription.\")\n  eventService.defaultSubscriber.subscribeCallback(Set(counterEventKey), processEvent)\n}\n\nprivate def unsubscribeHcd(): Unit = {\n  log.info(\"Stopping subscription.\")\n  maybeEventSubscription.foreach(_.unsubscribe())\n} Java private EventKey counterEventKey = new EventKey(Prefix.apply(JSubsystem.NFIRAOS, \"samplehcd\"), new EventName(\"HcdCounter\"));\nprivate Key<Integer> hcdCounterKey = JKeyType.IntKey().make(\"counter\", JUnits.NoUnits);\n\nprivate void processEvent(Event event) {\n    log.info(\"Event received: \" + event.eventKey());\n    if (event instanceof SystemEvent) {\n        SystemEvent sysEvent = (SystemEvent) event;\n        if (event.eventKey().equals(counterEventKey)) {\n            int counter = sysEvent.parameter(hcdCounterKey).head();\n            log.info(\"Counter = \" + counter);\n        } else {\n            log.warn(\"Unexpected event received.\");\n        }\n    } else {\n        // ObserveEvent, not expected\n        log.warn(\"Unexpected ObserveEvent received.\");\n    }\n}\n\nprivate IEventSubscription subscribeToHcd() {\n    log.info(\"Starting subscription.\");\n    return cswCtx.eventService().defaultSubscriber().subscribeCallback(Set.of(counterEventKey), this::processEvent);\n}\n\nprivate void unsubscribeHcd() {\n    log.info(\"Stopping subscription.\");\n    maybeEventSubscription.ifPresent(IEventSubscription::unsubscribe);\n}\nWe use the subscribeCallback method from the API and specify the method processEvent as our callback, in which we unpack the event and log the counter value. The subscribe methods in the API return a EventSubscription object, which can be used to stop the subscription, as demonstrated in the unsubscribeHCD method (which again, is not called in our tutorial).\nAgain, we return to our initialize method to show how subscription is started, and the reference to the subscription is stored for later use.\nScala private var maybeEventSubscription: Option[EventSubscription] = None\noverride def initialize(): Future[Unit] = {\n  log.info(\"In Assembly initialize\")\n  maybeEventSubscription = Some(subscribeToHcd())\n  Future.unit\n}\n\noverride def onShutdown(): Future[Unit] = {\n  log.info(\"Assembly is shutting down.\")\n  Future.unit\n} Java private Optional<IEventSubscription> maybeEventSubscription = Optional.empty();\n\n@Override\npublic CompletableFuture<Void> jInitialize() {\n    return CompletableFuture.runAsync(() -> {\n        log.info(\"In Assembly initialize\");\n        maybeEventSubscription = Optional.of(subscribeToHcd());\n    });\n}\n\n@Override\npublic CompletableFuture<Void> jOnShutdown() {\n    return CompletableFuture.runAsync(() -> log.info(\"Assembly is shutting down.\"));\n}","title":"Tutorial: Developing an Assembly"},{"location":"/commons/multiple-components.html#deploying-and-running-components","text":"","title":"Deploying and Running Components"},{"location":"/commons/multiple-components.html#pre-requisite","text":"A project, for example with the name sample-deploy, contains applications (ContainerCmd and HostConfig coming from csw-framework) to run components. Make sure that the necessary dependencies are added in the sample-deploy.","title":"Pre-requisite"},{"location":"/commons/multiple-components.html#run","text":"To start the Assembly and HCD, sbt runMain can be used as with the HCD, but with slightly different options. Now, we do not want to run in standalone mode, and we need to make sure to pass the container configuration file.\nGo to the project root directory and type sbt \"<deploy-module>/runMain <mainClass> --local <path-to-config-file>\", where\n<deploy-module> is the name of the deployment module created by the template (sample-deploy if using defaults) <mainClass> is the full class name of our ContainerCmd application, which the template names <package>.<name>deploy.<Name>ContainerCmdApp. If you accept the defaults for the template, it will be org.tmt.esw.sampledeploy.SampleContainerCmdApp. If you are having problems determining the class name, use sbt <deploy-module>/run and it will prompt you the possibilities. <path-to-config-file> is the filename, which can be an absolute path or relative to the directory of the deployment module. If using defaults, this would be src/main/resources/SampleContainer.conf for Scala, and src/main/resources/JSampleContainer.conf for Java.\nSo if using the template defaults, the full command would be:\nScala sbt \"sample-deploy/runMain org.tmt.esw.sampledeploy.SampleContainerCmdApp --local src/main/resources/SampleContainer.conf\"\n Java sbt \"sample-deploy/runMain org.tmt.esw.sampledeploy.SampleContainerCmdApp --local src/main/resources/JSampleContainer.conf\"\nLike with the HCD, the sbt stage command can also be used to create binaries in the target/universal/stage/bin directories of the root project.\nTo run using the deployment packaging, follow the steps below:\nRun sbt sample-deploy/universal:packageBin, this will create self contained zip in sample-deploy/target/universal directory. Unzip the generated zip file and enter into bin directory. You will see four scripts in the bin directory (two bash scripts and two windows scripts). If you want to start multiple containers on a host machine, follow this guide here. If you want to start multiple components in container mode or single component in standalone mode, follow this guide here. Example to run container: ./sample-container-cmd-app --local ../../../../sample-deploy/src/main/resources/SampleContainer.conf Example to run host config: ./sample-host-config-app --local ../../../../sample-deploy/src/main/resources/SampleHostConfig.conf -s ./sample-container-cmd-app\nNote This assumes you still have the CSW Services running using the csw-services.sh script as described in the Create a Component tutorial page.","title":"Run"},{"location":"/commons/using-alarms.html","text":"","title":"Using Alarms"},{"location":"/commons/using-alarms.html#using-alarms","text":"Alarms are used to indicate when some condition has arisen that warrants operator intervention. In order to ensure the integrity of alarms, components are required to broadcast the state of each alarm periodically. When situations occur in which a component is unable to determine and/or broadcast the alarm state, the alarm severity is automatically set to Disconnected after some period of time. This is brought to the operator’s attention and the operator can then take appropriate action. This means even when situations are normal, the component must continue to publish each alarm with an Okay severity; otherwise, the alarm severity is automatically marked as Disconnected, prompting the operator to investigate.\nThe CSW Alarm Service provides two APIs: a “Client API” used by a component, and an “Admin API” used to manage the Alarm Store and for operator use (typically via HCMS user interfaces). The Client API consists of a single method call, setSeverity. The Admin API includes methods to set up the Alarm Store, get alarm severities and health of components or subsystems, acknowledge alarms, reset latched alarms, and other operator tasks.\nThe Admin API can be exercised using the command line tool provided with CSW: csw-alarm-cli. See the reference manual for more info.\nMore details about the Alarm Service are found in the Alarm Service manual.","title":"Using Alarms"},{"location":"/commons/using-alarms.html#","text":"We will use our sample Assembly to monitor the counter event it is subscribed to, published by our sample HCD. Our alarm is based on the value of the counter, with the normal (Okay) range of 0 to 10, Warning range of 11 to 15, Major range of 16 to 20, and any other value generating a Critical alarm severity.\nFirst, the Alarm Store must be initialized with our alarm using the CLI tool csw-alarm-cli. A configuration file must be written that describes every alarm that will be used in the system. For TMT operations, this configuration will be generated from the ICD-DB models. For our tutorial, we will use a configuration with only the alarm we will be using.\nalarms.conf alarms: [\n  {\n    prefix = esw.SampleAssembly\n    name = counterTooHighAlarm\n    description = \"Warns when counter value is too high\"\n    location = \"enclosure\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major, Critical]\n    probableCause = \"Sample HCD has run for too long\"\n    operatorResponse = \"Restart HCD\"\n    isAutoAcknowledgeable = false\n    isLatchable = false\n    activationStatus = Active\n  }\n]\nFor our tutorial, let’s save this file to disk in our resources folder in the sample-deploy module (sample-deploy/src/main/resources/alarms.conf).\nNow, we will use the CLI tool. Find it in the bin directory of the CSW application package available with the release as csw-alarm-cli.\nUse the init command to initialize the Alarm Store (this assumes csw-services is running, which sets up the Redis store for alarms).\ncsw-alarm-cli init $PROJECTDIR/sample-deploy/src/main/resources/alarms.conf --local\nwhere $PROJECTDIR is the root directory of your sample project. The --local flag indicates the configuration file is obtains from disk; omitting it would attempt to find the file in the Configuration Service, as would be done during operations.\nNow we will add code to our assembly to publish an alarm severity on every counter event. Let’s create some logic to take the counter as an argument and generate an alarm:\nScala private val safeRange  = 0 to 10\nprivate val warnRange  = 11 to 15\nprivate val majorRange = 16 to 20\nprivate def getCounterSeverity(counter: Int) = counter match {\n  case x if safeRange contains x  => AlarmSeverity.Okay\n  case x if warnRange contains x  => AlarmSeverity.Warning\n  case x if majorRange contains x => AlarmSeverity.Major\n  case _                          => AlarmSeverity.Critical\n}\n\nprivate val counterAlarmKey = AlarmKey(componentInfo.prefix, \"CounterTooHighAlarm\")\nprivate def setCounterAlarm(counter: Int): Unit = {\n  // fire alarm according to counter value\n  val severity = getCounterSeverity(counter)\n  alarmService.setSeverity(counterAlarmKey, severity).onComplete {\n    case Success(_)  => log.info(s\"Severity for alarm ${counterAlarmKey.name} set to \" + severity.toString)\n    case Failure(ex) => log.error(s\"Error setting severity for alarm ${counterAlarmKey.name}: ${ex.getMessage}\")\n  }\n} Java private AlarmSeverity getCounterSeverity(int counter) {\n    if (counter >= 0 && counter <= 10) {\n        return JAlarmSeverity.Okay;\n    } else if (counter >= 11 && counter <= 15) {\n        return JAlarmSeverity.Warning;\n    } else if (counter >= 16 && counter <= 20) {\n        return JAlarmSeverity.Major;\n    }\n    return JAlarmSeverity.Critical;\n}\n\nprivate void setCounterAlarm(int counter) {\n    AlarmKey counterAlarmKey = new AlarmKey(cswCtx.componentInfo().prefix(), \"CounterTooHighAlarm\");\n    AlarmSeverity severity = getCounterSeverity(counter);\n    cswCtx.alarmService().setSeverity(counterAlarmKey, severity)\n            .whenComplete((d, ex) -> {\n                if (ex != null) {\n                    log.error(\"Error setting severity for alarm \" + counterAlarmKey.name() + \": \" + ex.getMessage());\n                } else {\n                    log.info(\"Severity for alarm \" + counterAlarmKey.name() + \" set to \" + severity.toString());\n                }\n            });\n}\nThis code determines the severity of the alarm based on the rules we established above:\nOkay: 0-10 Warning: 11-15 Major: 16-20 Critical: any other value\nNow, all we have to do is call this whenever we receive a counter event. We add a call to the setCounterAlarm method in the processEvent method:\nScala private def processEvent(event: Event): Unit = {\n  log.info(s\"Event received: ${event.eventKey}\")\n  event match {\n    case e: SystemEvent =>\n      e.eventKey match {\n        case `counterEventKey` =>\n          val counter = e(hcdCounterKey).head\n          log.info(s\"Counter = $counter\")\n          setCounterAlarm(counter)\n        case _ => log.warn(\"Unexpected event received.\")\n      }\n    case _: ObserveEvent => log.warn(\"Unexpected ObserveEvent received.\") // not expected\n  }\n} Java private void processEvent(Event event) {\n    log.info(\"Event received: \" + event.eventKey());\n    if (event instanceof SystemEvent) {\n        SystemEvent sysEvent = (SystemEvent) event;\n        if (event.eventKey().equals(counterEventKey)) {\n            int counter = sysEvent.parameter(hcdCounterKey).head();\n            log.info(\"Counter = \" + counter);\n            setCounterAlarm(counter);\n        } else {\n            log.warn(\"Unexpected event received.\");\n        }\n    } else {\n        // ObserveEvent, not expected\n        log.warn(\"Unexpected ObserveEvent received.\");\n    }\n}\nTo see the effect, let’s use the CLI to set up a subscription to the alarm. Note the alarm key is the component’s prefix (esw.sampleassembly for Scala, esw.jsampleassembly for Java), and the alarm name (counterTooHighAlarm).\nScala csw-alarm-cli severity subscribe --subsystem esw --component sampleassembly --name counterTooHighAlarm\n Java csw-alarm-cli severity subscribe --subsystem esw --component jsampleassembly --name counterTooHighAlarm\nNote that the alarm severity is currently Disconnected. This is the appropriate state, since we are not running the components. Now, run the Assembly and HCD, and you will see the severity of our alarm updated in the CLI as the severity changes.","title":"Tutorial: Using alarms in a component"},{"location":"/commons/unit-tests.html","text":"","title":"Adding Unit Tests"},{"location":"/commons/unit-tests.html#adding-unit-tests","text":"Unit testing is a fundamental part of programming, and essential component of the TMT quality assurance policy. TMT CSW extensively uses unit testing for both Scala and Java code, using ScalaTest for the former, and primarily JUnit for the latter (TestNG is used in one instance for the Event Service). While this guide will not attempt to educate the reader on how to use these popular packages, it will serve to show some examples of how tests can be created for component code and demonstrate some tools provided by CSW to simplify and enable integration of TMT components and other software with CSW and its services.","title":"Adding Unit Tests"},{"location":"/commons/unit-tests.html#csw-test-kit","text":"CSW provides a set of tools for use by developers called the CSW Test Kit. This allows the developer to start CSW services within the testing environment, so that they can be accessed by the components and/or applications being tested, as well as the testing fixtures themselves. It also provides simple methods to start components or a set of components within a container, as well as an ActorContext to be used if other Actors are needed to be created in the tests.\nMore information about testing with CSW and the CSW Test Kit can be found here.","title":"CSW Test Kit"},{"location":"/commons/unit-tests.html#","text":"In this part of the tutorial, we will write a few unit tests for our components. These tests are in no way meant to be comprehensive, but hopefully, they show enough to get you started.\nThe giter8 template provides the required directory structure, and skeletons for tests of the HCD and Assembly in both Java and Scala. It also provides some Component Configuration (ComponentInfo) files for running each of the HCD and Assembly in standalone mode for both languages. They are there for convenience, but may not be required depending your deployment and testing strategy. We will be using them in our tutorial.\nWe will first look at the tests for the Assembly. As described on the Testing Manual page, the Scala version overrides the CSW-provided superclass csw.testkit.scaladsl.ScalaTestFrameworkTestKit to get access to the services it needs. By passing in the needed services in the constructor, those services are started in the superclass’s beforeAll method. In the Java version, we must create an instance of csw.testkit.javadsl.FrameworkTestKitJunitResource to get access to and start our services, with the services we want to start passed into the constructor of this object.\nScala class BasicSampleAssemblyTest extends ScalaTestFrameworkTestKit(AlarmServer, EventServer) with AnyFunSuiteLike {\n  import frameworkTestKit.frameworkWiring._ Java public class JSampleAssemblyTest extends JUnitSuite {\n    @ClassRule\n    public static final FrameworkTestKitJunitResource testKit =\n            new FrameworkTestKitJunitResource(Arrays.asList(JCSWService.AlarmServer, JCSWService.EventServer));\nFor our tests, we will want to run the Assembly first. We will do this in the beforeAll method in Scala, and in a method with a @BeforeClass annotation in Java, so that it is run only once, before any of the tests are run. The Component Configuration files use are the one provided by the giter8 template. Note that for Scala, we must call the superclass’s beforeAll method to ensure the services are run.\nNote This code has been provided as part of the giter8 template, so the relevant line only needs to be uncommented.\nScala override def beforeAll(): Unit = {\n  super.beforeAll()\n  spawnStandalone(com.typesafe.config.ConfigFactory.load(\"BasicSampleAssemblyStandalone.conf\"))\n} Java @BeforeClass\npublic static void setup() {\n    testKit.spawnStandalone(com.typesafe.config.ConfigFactory.load(\"JBasicSampleAssemblyStandalone.conf\"));\n}\nNext, let’s add a test. We will add a simple test that uses the Location Service to make sure the Assembly is running and resolve the registration information for it.\nNote This test has been provided as part of the giter8 template as an example.\nScala import scala.concurrent.duration._\ntest(\"Assembly should be locatable using Location Service\") {\n  val connection   = AkkaConnection(ComponentId(Prefix(Subsystem.ESW, \"SampleAssembly\"), ComponentType.Assembly))\n  val akkaLocation = Await.result(locationService.resolve(connection, 10.seconds), 10.seconds).get\n\n  akkaLocation.connection shouldBe connection\n} Java @Test\npublic void testAssemblyShouldBeLocatableUsingLocationService() throws ExecutionException, InterruptedException {\n    AkkaConnection connection = new AkkaConnection(new ComponentId(Prefix.apply(JSubsystem.ESW, \"JSampleAssembly\"), JComponentType.Assembly));\n    ILocationService locationService = testKit.jLocationService();\n    AkkaLocation location = locationService.resolve(connection, Duration.ofSeconds(10)).get().orElseThrow();\n\n    Assert.assertEquals(location.connection(), connection);\n}\nYou can try running the test either using sbt (sbt test from the project root directory) or directly in the IDE. If you are using IntelliJ, you can run the test by right-clicking on the file in the project explorer and clicking on Run 'SampleAssemblyTest' or Run 'JSampleAssemblyTest'. You can also right-click in the class body or the specific test body, if you want to run a single test.\nThe Assembly we have written does not have much of a public API, so we’ll turn to the HCD now, which has a few additional things we can test, including the publishing of Events and the handling of commands.\nFirst, we will set up the test fixtures similarly as we did for the Assembly, and add a similar test to show the component registers itself with the Location Service on startup.\nNote This also has been provided in the giter8 template. Uncomment the line that launches the HCD before tests are run.\nScala class BasicSampleHcdTest\n    extends ScalaTestFrameworkTestKit(AlarmServer, EventServer)\n    with AnyFunSuiteLike\n    with BeforeAndAfterEach {\n  import frameworkTestKit.frameworkWiring._\n\n  override def beforeAll(): Unit = {\n    super.beforeAll()\n    spawnStandalone(com.typesafe.config.ConfigFactory.load(\"BasicSampleHcdStandalone.conf\"))\n  }\n\n  import scala.concurrent.duration._\n  test(\"HCD should be locatable using Location Service\") {\n    val connection   = AkkaConnection(ComponentId(Prefix(Subsystem.ESW, \"SampleHcd\"), ComponentType.HCD))\n    val akkaLocation = Await.result(locationService.resolve(connection, 10.seconds), 10.seconds).get\n\n    akkaLocation.connection shouldBe connection\n  } Java public class JSampleHcdTest extends JUnitSuite {\n\n    @ClassRule\n    public static final FrameworkTestKitJunitResource testKit =\n            new FrameworkTestKitJunitResource(Arrays.asList(JCSWService.AlarmServer, JCSWService.EventServer));\n\n\n    @BeforeClass\n    public static void setup() {\n        testKit.spawnStandalone(com.typesafe.config.ConfigFactory.load(\"JBasicSampleHcdStandalone.conf\"));\n    }\n\n    // DEOPSCSW-39: examples of Location Service\n    @Test\n    public void testHCDShouldBeLocatableUsingLocationService() throws ExecutionException, InterruptedException {\n        AkkaConnection connection = new AkkaConnection(new ComponentId(Prefix.apply(JSubsystem.ESW, \"JSampleHcd\"), JComponentType.HCD));\n        ILocationService locationService = testKit.jLocationService();\n        AkkaLocation location = locationService.resolve(connection, Duration.ofSeconds(10)).get().orElseThrow();\n\n        Assert.assertEquals(connection, location.connection());\n    }\nNow let’s add a test to verify our component is publishing. We will set up a test subscriber to the counterEvent Events published by the HCD. Since we cannot guarantee the order in which the tests are run, we cannot be certain how long the component has been running when this specific test is run. Therefore, checking the contents of the Events received is tricky. We will wait a bit at the start of the test to ensure we don’t get a InvalidEvent, which would be returned if we start our subscription before the HCD publishes any Events. Then, after setting up the subscription, we wait 5 seconds to allow the HCD to publish two additional Events plus the one we receive when the subscription starts. We will look at the counter value of the first counterEvent to determine what the set of counter values we expect to get in our subscription.\nScala test(\"should be able to subscribe to HCD events\") {\n  val counterEventKey = EventKey(Prefix(\"ESW.SampleHcd\"), EventName(\"HcdCounter\"))\n  val hcdCounterKey   = KeyType.IntKey.make(\"counter\")\n\n  val eventService = eventServiceFactory.make(locationService)(actorSystem)\n  val subscriber   = eventService.defaultSubscriber\n\n  // wait for a bit to ensure HCD has started and published an event\n  Thread.sleep(2000)\n\n  val subscriptionEventList = mutable.ListBuffer[Event]()\n  subscriber.subscribeCallback(Set(counterEventKey), { ev =>\n    subscriptionEventList.append(ev)\n  })\n\n  // Sleep for 5 seconds, to allow HCD to publish events\n  Thread.sleep(5000)\n\n  // Event publishing period is 2 seconds.\n  // Expecting 3 events: first event on subscription\n  // and two more events 2 and 4 seconds later.\n  subscriptionEventList.toList.size shouldBe 3\n\n  // extract counter values to a List for comparison\n  val counterList = subscriptionEventList.toList.map {\n    case sysEv: SystemEvent if sysEv.contains(hcdCounterKey) => sysEv(hcdCounterKey).head\n    case _                                                   => -1\n  }\n\n  // we don't know exactly how long HCD has been running when this test runs,\n  // so we don't know what the first value will be,\n  // but we know we should get three consecutive numbers\n  val expectedCounterList = (0 to 2).toList.map(_ + counterList.head)\n\n  counterList shouldBe expectedCounterList\n} Java @Test\npublic void testShouldBeAbleToSubscribeToHCDEvents() throws InterruptedException {\n    EventKey counterEventKey = new EventKey(Prefix.apply(JSubsystem.ESW, \"JSampleHcd\"), new EventName(\"HcdCounter\"));\n    Key<Integer> hcdCounterKey = JKeyType.IntKey().make(\"counter\");\n\n    IEventService eventService = testKit.jEventService();\n    IEventSubscriber subscriber = eventService.defaultSubscriber();\n\n    // wait for a bit to ensure HCD has started and published an event\n    Thread.sleep(5000);\n\n\n    ArrayList<Event> subscriptionEventList = new ArrayList<>();\n    subscriber.subscribeCallback(Set.of(counterEventKey), subscriptionEventList::add);\n\n    // Sleep for 4 seconds, to allow HCD to publish events\n    Thread.sleep(4000);\n\n    // Event publishing period is 2 seconds.\n    // Expecting 3 events: first event on subscription\n    // and two more events 2 and 4 seconds later.\n    Assert.assertEquals(3, subscriptionEventList.size());\n\n    // extract counter values to a List for comparison\n    List<Integer> counterList = subscriptionEventList.stream()\n            .map(ev -> {\n                SystemEvent sysEv = ((SystemEvent) ev);\n                if (sysEv.contains(hcdCounterKey)) {\n                    return sysEv.parameter(hcdCounterKey).head();\n                } else {\n                    return -1;\n                }\n            })\n            .collect(Collectors.toList());\n\n    // we don't know exactly how long HCD has been running when this test runs,\n    // so we don't know what the first value will be,\n    // but we know we should get three consecutive numbers\n    int counter0 = counterList.get(0);\n    List<Integer> expectedCounterList = Arrays.asList(counter0, counter0 + 1, counter0 + 2);\n\n    Assert.assertEquals(expectedCounterList, counterList);\n}\nNext, we’ll add a test for command handling in the HCD. The HCD supports a “sleep” command, which sleeps some amount of seconds as specified in the command payload, and then returns a CommandResponse.Completed. We will specify a sleep of 5 seconds, and then check that we get the expected response. Note that the obtaining a CommandService reference requires an Akka Typed Actor System, so our code will create one using the Actor System provided by the Test Kit.\nScala implicit val typedActorSystem: ActorSystem[_] = actorSystem\ntest(\"basic: should be able to send sleep command to HCD\") {\n  import scala.concurrent.duration._\n  implicit val sleepCommandTimeout: Timeout = Timeout(10000.millis)\n\n  // Construct Setup command\n  val testPrefix: Prefix = Prefix(\"ESW.test\")\n\n  // Helper to get units set\n  def setSleepTime(setup: Setup, milli: Long): Setup = setup.add(sleepTimeKey.set(milli).withUnits(Units.millisecond))\n\n  val setupCommand = setSleepTime(Setup(testPrefix, hcdSleep, Some(ObsId(\"2018A-001\"))), 5000)\n\n  val connection = AkkaConnection(ComponentId(Prefix(Subsystem.ESW, \"SampleHcd\"), ComponentType.HCD))\n\n  val akkaLocation = Await.result(locationService.resolve(connection, 10.seconds), 10.seconds).get\n\n  val hcd = CommandServiceFactory.make(akkaLocation)\n  // submit command and handle response\n  val responseF = hcd.submitAndWait(setupCommand)\n\n  Await.result(responseF, 10000.millis) shouldBe a[CommandResponse.Completed]\n} Java private ActorSystem<SpawnProtocol.Command> typedActorSystem = testKit.actorSystem();\n\n// DEOPSCSW-39: examples of Location Service\n@Test\npublic void testShouldBeAbleToSendSleepCommandToHCD() throws ExecutionException, InterruptedException, TimeoutException {\n\n    // Construct Setup command\n    Parameter<Long> sleepTimeParam = sleepTimeKey.set(5000L).withUnits(JUnits.millisecond);\n\n    Setup setupCommand = new Setup(Prefix.apply(JSubsystem.CSW, \"move\"), hcdSleep, Optional.of(new ObsId(\"2018A-001\"))).add(sleepTimeParam);\n\n    Timeout commandResponseTimeout = new Timeout(10, TimeUnit.SECONDS);\n\n    AkkaConnection connection = new AkkaConnection(new ComponentId(Prefix.apply(JSubsystem.ESW, \"JSampleHcd\"), JComponentType.HCD));\n    ILocationService locationService = testKit.jLocationService();\n    AkkaLocation location = locationService.resolve(connection, Duration.ofSeconds(10)).get().orElseThrow();\n\n    ICommandService hcd = CommandServiceFactory.jMake(location, typedActorSystem);\n\n    CommandResponse.SubmitResponse result = hcd.submitAndWait(setupCommand, commandResponseTimeout).get(10, TimeUnit.SECONDS);\n    Assert.assertTrue(result instanceof CommandResponse.Completed);\n}\nFinally, we will show an example of tests that check that exceptions are thrown when expected. We will do this by using the “sleep” command, but failing to wait long enough for the sleep to complete. This causes a TimeoutException in Scala, and an ExecutionException in Java, and our tests check to see that these types are in fact thrown.\nScala test(\"should get timeout exception if submit timeout is too small\") {\n  import scala.concurrent.duration._\n  implicit val sleepCommandTimeout: Timeout = Timeout(1000.millis)\n\n  // Construct Setup command\n  val testPrefix: Prefix    = Prefix(\"ESW.test\")\n  val hcdSleep: CommandName = CommandName(\"hcdSleep\")\n  // Helper to get units set\n  def setSleepTime(milli: Long): Parameter[Long] = sleepTimeKey.set(milli).withUnits(Units.millisecond)\n\n  val setupCommand = Setup(testPrefix, hcdSleep, Some(ObsId(\"2018A-001\"))).add(setSleepTime(5000))\n\n  val connection = AkkaConnection(ComponentId(Prefix(Subsystem.ESW, \"SampleHcd\"), ComponentType.HCD))\n\n  val akkaLocation = Await.result(locationService.resolve(connection, 10.seconds), 10.seconds).get\n\n  val hcd = CommandServiceFactory.make(akkaLocation)\n\n  // submit command and handle response\n  intercept[java.util.concurrent.TimeoutException] {\n    val responseF = hcd.submitAndWait(setupCommand)\n    Await.result(responseF, 10000.millis) shouldBe a[CommandResponse.Completed]\n  }\n} Java @Rule\npublic ExpectedException thrown = ExpectedException.none();\n\n@Test\npublic void testShouldGetExecutionExceptionIfSubmitTimeoutIsTooSmall() throws ExecutionException, InterruptedException {\n\n    // Construct Setup command\n    Key<Long> sleepTimeKey = JKeyType.LongKey().make(\"SleepTime\");\n    Parameter<Long> sleepTimeParam = sleepTimeKey.set(5000L).withUnits(JUnits.millisecond);\n\n    Setup setupCommand = new Setup(Prefix.apply(JSubsystem.CSW, \"move\"), hcdSleep, Optional.of(new ObsId(\"2018A-001\"))).add(sleepTimeParam);\n\n    Timeout commandResponseTimeout = new Timeout(1, TimeUnit.SECONDS);\n\n    AkkaConnection connection = new AkkaConnection(new ComponentId(Prefix.apply(JSubsystem.ESW, \"JSampleHcd\"), JComponentType.HCD));\n    ILocationService locationService = testKit.jLocationService();\n    AkkaLocation location = locationService.resolve(connection, Duration.ofSeconds(10)).get().orElseThrow();\n\n    ICommandService hcd = CommandServiceFactory.jMake(location, typedActorSystem);\n\n    thrown.expect(ExecutionException.class);\n    hcd.submitAndWait(setupCommand, commandResponseTimeout).get();\n}","title":"Tutorial: Writing unit tests for our components"},{"location":"/commons/params.html","text":"","title":"Params"},{"location":"/commons/params.html#params","text":"In the distributed environment of TMT observatory, Components communicate with each other by sending asynchronous Messages. These messages have a Command payload, which flows down through the Sequencer components to the Assemblies, HCDs and finally to the hardware. At each hop Commands are validated, interpreted and further propagated making their journey to its destination. Commands provide flexible placeholders to store values to convey precise intent of the sender component.\ncsw-params is available for Scala, Java, and scala.js.","title":"Params"},{"location":"/commons/params.html#dependencies","text":"sbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-params\" % \"2.0.0-RC2\"\ncsw-params library provides out of the box support to cater to the diverse communication requirements. Consumer of this library will be able to create Commands, Events, States to store ParameterSets.\nKeys and Parameters How to Create a Parameter Primitive Datatypes Arrays Matrices Domain Specific Types Coordinate Types Source Code for Examples Units Default Units for Keys SI Units CGS Units Astrophysical Units Imperial Units Others - Engineering Usage Examples Source Code for Examples Subsystem List of Subsystems Usage Examples Commands ObsId Prefix CommandName Setup Command Observe Command Wait Command JSON serialization Unique Key constraint Cloning a Command Source Code for Examples Events EventTime System Event Observe Event JSON Serialization Unique Key Constraint State Variables DemandState CurrentState JSON Serialization Unique Key Constraint Source Code for Examples Result JSON serialization Unique Key Constraint Source Code for Examples","title":"Dependencies"},{"location":"/params/keys-parameters.html","text":"","title":"Keys and Parameters"},{"location":"/params/keys-parameters.html#keys-and-parameters","text":"The library offers a flexible and typesafe means to create Parameters to store values like primitive types, collection types or domain specific types.\nA Parameter is a Key and Value where the Value must be from a set of defined primitive types including binary data. The Value of a Parameter is always considered to be an Array of the type (i.e. if a single value is stored it is at array location 0). A Parameter is immutable; a modification to an existing Parameter will return a new instance.\nA Value can also have Units, which must be of the defined types. See Units for more information. At this time Units are informational only–no calculation or conversion support is provided. Some systems may provide a key value with different units, and receiver can inspect the Units to make a decision on how to handle the value.\nA ParameterSet is a Set of Parameter. Various other message types include a ParameterSet (e.g. Setup, Event). A key is unique in a ParameterSet since it is a Set.","title":"Keys and Parameters"},{"location":"/params/keys-parameters.html#how-to-create-a-parameter","text":"choose an appropriate KeyType from the tables below for your language(Scala/Java). calling the make method on KeyType and supplying a String keyName will return a suitably typed Key instance. exploit the overloaded set and -> methods, which will allow you to store values of the based on chosen KeyType. e.g. JKeyType.BooleanKey will allow storing only java.lang.Boolean values.","title":"How to Create a Parameter"},{"location":"/params/keys-parameters.html#primitive-datatypes","text":"Primitive Scala KeyType Java KeyType Boolean KeyType.BooleanKey JKeyType.BooleanKey Character KeyType.CharKey JKeyType.JCharKey Byte KeyType.ByteKey JKeyType.ByteKey Short KeyType.ShortKey JKeyType.ShortKey Long KeyType.LongKey JKeyType.LongKey Int KeyType.IntKey JKeyType.IntKey Float KeyType.FloatKey JKeyType.FloatKey Double KeyType.DoubleKey JKeyType.DoubleKey String KeyType.StringKey JKeyType.StringKey UtcTime KeyType.UTCTimeKey JKeyType.UTCTimeKey TaiTime KeyType.TAITimeKey JKeyType.TAITimeKey\nScala //declare keyName\nval s1: String = \"encoder\"\n\n//making 3 keys\nval k1: Key[Boolean] = KeyType.BooleanKey.make(s1)\nval k2: Key[Short]   = KeyType.ShortKey.make(\"RandomKeyName\")\nval k3: Key[String]  = KeyType.StringKey.make(s1)\n\n//storing a single value\nval booleanParam: Parameter[Boolean] = k1.set(true)\n\n//storing multiple values\nval paramWithShorts1: Parameter[Short] = k2.set(1, 2, 3, 4)\nval paramWithShorts2: Parameter[Short] = k2 -> (1, 2, 3, 4)\nval paramWithShorts3: Parameter[Short] = k2 -> Array[Short](1, 2, 3, 4)\n\n//associating units\nval weekDays: Array[String]            = Array(\"Sunday\", \"Monday\", \"Tuesday\")\nval paramWithUnits1: Parameter[String] = k3.setAll(weekDays)\nval paramWithUnits2: Parameter[String] = k3 -> weekDays withUnits Units.day\n\n//default unit is NoUnits\nassert(booleanParam.units === Units.NoUnits)\n\n//set units explicitly on an existing Parameter\nval paramWithUnits3: Parameter[Short] = paramWithShorts1.withUnits(Units.meter)\n\n//retrieve values from Parameter\nval allValues: Array[Short] = paramWithShorts1.values\n\n//retrieve just top value\nval head: Short = paramWithShorts1.head Java //making 3 keys\nString keyName = \"encoder\";\nKey<Boolean> k1 = JKeyType.BooleanKey().make(keyName);\nKey<Short> k2 = JKeyType.ShortKey().make(keyName, JUnits.NoUnits);\nKey<String> k3 = JKeyType.StringKey().make(keyName, JUnits.day);\n\n//storing a single value\nParameter<Boolean> booleanParam = k1.set(true);\n\n//storing multiple values\nShort[] shortArray = {1, 2, 3, 4};\nParameter<Short> paramWithManyShorts1 = k2.setAll(shortArray);\nParameter<Short> paramWithManyShorts2 = k2.set((short) 1, (short) 2, (short) 3, (short) 4);\n\n//associating units\nString[] weekDays = {\"Sunday\", \"Monday\", \"Tuesday\"};\nParameter<String> paramWithUnits1 = k3.setAll(weekDays);\nParameter<String> paramWithUnits2 = k3.setAll(weekDays).withUnits(JUnits.day);\n\n//deault unit is NoUnits()\nboolean hasDefaultUnit = booleanParam.units() == JUnits.NoUnits; //true\n\n//set units explicitly on an existing Parameter\nParameter<Short> paramWithUnits3 = paramWithManyShorts1.withUnits(JUnits.meter);\n\n//retrieve values from Parameter\nShort[] allValues = (Short[]) paramWithManyShorts1.values();\n\n//retrieve just top value\nShort head = paramWithManyShorts1.head();","title":"Primitive Datatypes"},{"location":"/params/keys-parameters.html#arrays","text":"Primitive Scala KeyType Java KeyType ByteArray KeyType.ByteArrayKey JKeyType.ByteArrayKey ShortArray KeyType.ShortArrayKey JKeyType.ShortArrayKey LongArray KeyType.LongArrayKey JKeyType.LongArrayKey IntArray KeyType.IntArrayKey JKeyType.IntArrayKey FloatArray KeyType.FloatArrayKey JKeyType.FloatArrayKey DoubleArray KeyType.DoubleArrayKey JKeyType.DoubleArrayKey\nScala //make some arrays\nval arr1: Array[Double] = Array(1.0, 2.0, 3.0, 4.0, 5.0)\nval arr2: Array[Double] = Array(10.0, 20.0, 30.0, 40.0, 50.0)\n\n//keys\nval filterKey: Key[ArrayData[Double]] = KeyType.DoubleArrayKey.make(\"filter\")\n\n//Store some values using helper class ArrayData\nval p1: Parameter[ArrayData[Double]] = filterKey.set(ArrayData(arr1), ArrayData(arr2))\nval p2: Parameter[ArrayData[Double]] = filterKey -> ArrayData(arr1 ++ arr2) withUnits Units.liter\n\n//add units to existing parameters\nval p1AsCount = p1.withUnits(Units.count)\n\n//default unit is NoUnits\nassert(p1.units === Units.NoUnits)\n\n//retrieving values\nval head: Array[Double]                 = p1.head.data.toArray\nval allValues: Array[ArrayData[Double]] = p1.values Java //make some arrays\nDouble[] arr1 = {1.0, 2.0, 3.0, 4.0, 5.0};\nDouble[] arr2 = {10.0, 20.0, 30.0, 40.0, 50.0};\n\n//keys\nKey<ArrayData<Double>> filterKey = JKeyType.DoubleArrayKey().make(\"filter\", JUnits.NoUnits);\n\n//Store some values using helper method in ArrayData\nParameter<ArrayData<Double>> p1 = filterKey.set(ArrayData.fromArray(arr1), ArrayData.fromArray(arr2));\nParameter<ArrayData<Double>> p2 = filterKey.set(ArrayData.fromArray(arr2)).withUnits(JUnits.liter);\n\n//add units to existing parameters\nParameter<ArrayData<Double>> p1AsCount = p1.withUnits(JUnits.count);\n\n//default unit is NoUnits()\nboolean bDefaultUnit = JUnits.NoUnits == p1.units();\n\n//retrieving values\nList<Double> head = p1.head().jValues();\nList<ArrayData<Double>> listOfArrayData = p1.jValues();\nDouble[] arrayOfDoubles = (Double[]) p2.jValues().get(0).values();","title":"Arrays"},{"location":"/params/keys-parameters.html#matrices","text":"Primitive Scala KeyType Java KeyType ByteMatrix KeyType.ByteMatrixKey JKeyType.ByteMatrixKey ShortMatrix KeyType.ShortMatrixKey JKeyType.ShortMatrixKey LongMatrix KeyType.LongMatrixKey JKeyType.LongMatrixKey IntMatrix KeyType.IntMatrixKey JKeyType.IntMatrixKey FloatMatrix KeyType.FloatMatrixKey JKeyType.FloatMatrixKey DoubleMatrix KeyType.DoubleMatrixKey JKeyType.DoubleMatrixKey\nScala //make some arrays\nval m1: Array[Array[Byte]] = Array(Array[Byte](1, 2, 3), Array[Byte](4, 5, 6), Array[Byte](7, 8, 9))\nval m2: Array[Array[Byte]] = Array(Array[Byte](1, 2, 3, 4, 5), Array[Byte](10, 20, 30, 40, 50))\n\n//keys\nval encoderKey: Key[MatrixData[Byte]] = KeyType.ByteMatrixKey.make(\"encoder\")\n\n//Store some values using helper class MatrixData\nval p1: Parameter[MatrixData[Byte]] = encoderKey.set(MatrixData.fromArrays(m1))\nval p2: Parameter[MatrixData[Byte]] = encoderKey.set(m1, m2) withUnits Units.liter\n\n//add units to existing parameters\nval p1AsLiter = p1.withUnits(Units.liter)\n\n//default unit is NoUnits\nassert(p1.units === Units.NoUnits)\n\n//retrieving values\nval head: Array[Array[Byte]]           = p1.head.data.map(_.toArray).toArray\nval allValues: Array[MatrixData[Byte]] = p1.values Java //make some arrays\nByte[][] m1 = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};\nByte[][] m2 = {{1, 2, 3, 4, 5}, {10, 20, 30, 40, 50}};\n\n//keys\nKey<MatrixData<Byte>> encoderKey = JKeyType.ByteMatrixKey().make(\"encoder\", JUnits.NoUnits);\n\n//Store some values using helper method in ArrayData\nParameter<MatrixData<Byte>> p1 = encoderKey.set(\n        MatrixData.fromArrays(m1),\n        MatrixData.fromArrays(m2));\nParameter<MatrixData<Byte>> p2 = encoderKey.set(\n        MatrixData.fromArrays(m2)\n).withUnits(JUnits.liter);\n\n//add units to existing parameters\nParameter<MatrixData<Byte>> p1AsLiter = p1.withUnits(JUnits.liter);\n\n//default unit is NoUnits()\nboolean bDefaultUnit = JUnits.NoUnits == p1.units();\n\n//retrieving values\nMatrixData<Byte> head = p1.head();\nList<MatrixData<Byte>> matrixData1 = p1.jValues();\nList<MatrixData<Byte>> matrixData2 = p2.jValues();","title":"Matrices"},{"location":"/params/keys-parameters.html#domain-specific-types","text":"Primitive Scala KeyType Java KeyType Choice KeyType.ChoiceKey JKeyType.ChoiceKey Struct KeyType.StructKey JKeyType.StructKey","title":"Domain Specific Types"},{"location":"/params/keys-parameters.html#coordinate-types","text":"Primitive Scala KeyType Java KeyType RaDec KeyType.RaDecKey JKeyType.RaDecKey EqCoord KeyType.EqCoordKey JKeyType.EqCoordKey SolarSystemCoord KeyType.SolarSystemCoordKey JKeyType.SolarSystemCoordKey MinorPlanetCoord KeyType.MinorPlanetCoordKey JKeyType.MinorPlanetCoordKey CometCoord KeyType.CometCoordKey JKeyType.CometCoordKey AltAzCoord KeyType.AltAzCoordKey JKeyType.AltAzCoordKey Coord (*) KeyType.CoordKey JKeyType.CoordKey\nNote Note that since Coord is the base trait of the other coordinate types, you can use it as the key for any of the coordinate types.","title":"Coordinate Types"},{"location":"/params/keys-parameters.html#coordinate-types-example","text":"The following example demonstrates the basic usage of the coordinate parameter types:\nScala import Angle._\nimport Coords._\n\n// Coordinate types\nval pm               = ProperMotion(0.5, 2.33)\nval eqCoord          = EqCoord(ra = \"12:13:14.15\", dec = \"-30:31:32.3\", frame = FK5, pmx = pm.pmx, pmy = pm.pmy)\nval solarSystemCoord = SolarSystemCoord(Tag(\"BASE\"), Venus)\nval minorPlanetCoord = MinorPlanetCoord(Tag(\"GUIDER1\"), 2000, 90.degree, 2.degree, 100.degree, 1.4, 0.234, 220.degree)\nval cometCoord       = CometCoord(Tag(\"BASE\"), 2000.0, 90.degree, 2.degree, 100.degree, 1.4, 0.234)\nval altAzCoord       = AltAzCoord(Tag(\"BASE\"), 301.degree, 42.5.degree)\n\n// Can use base trait CoordKey to store values for all types\nval basePosKey = CoordKey.make(\"BasePosition\")\nval posParam   = basePosKey.set(eqCoord, solarSystemCoord, minorPlanetCoord, cometCoord, altAzCoord)\n\n//retrieving values\nassert(posParam.values.length == 5)\nassert(posParam.values(0) == eqCoord)\nassert(posParam.values(1) == solarSystemCoord)\nassert(posParam.values(2) == minorPlanetCoord)\nassert(posParam.values(3) == cometCoord)\nassert(posParam.values(4) == altAzCoord) Java //import csw.params.core.models.Coords.*;\n//import static csw.params.core.models.JCoords.*;\n//import static csw.params.core.models.Coords.*;\n\n// Coordinate types\nProperMotion pm = new ProperMotion(0.5, 2.33);\n\nEqCoord eqCoord = new EqCoord(\"12:13:14.15\", \"-30:31:32.3\", FK5(), BASE(),\n        DEFAULT_CATNAME(), pm.pmx(), pm.pmy());\n\nSolarSystemCoord solarSystemCoord = new SolarSystemCoord(BASE(), Venus());\n\nMinorPlanetCoord minorPlanetCoord = new MinorPlanetCoord(GUIDER1(), 2000, JAngle.degree(90),\n        JAngle.degree(2), JAngle.degree(100), 1.4, 0.234, JAngle.degree(220));\n\nCometCoord cometCoord = new CometCoord(BASE(), 2000.0, JAngle.degree(90),\n        JAngle.degree(2), JAngle.degree(100), 1.4, 0.234);\n\nAltAzCoord altAzCoord = new AltAzCoord(BASE(), JAngle.degree(301), JAngle.degree(42.5));\n\n// Can use base trait CoordKey to store values for all types\nKey<Coord> basePosKey = JKeyType.CoordKey().make(\"BasePosition\", JUnits.NoUnits);\n\nParameter<Coord> posParam = basePosKey.set(eqCoord, solarSystemCoord, minorPlanetCoord, cometCoord, altAzCoord);\n\n//retrieving values\nassert (posParam.jValues().size() == 5);\nassert (posParam.jValues().get(0).equals(eqCoord));\nassert (posParam.jValues().get(1).equals(solarSystemCoord));\nassert (posParam.jValues().get(2).equals(minorPlanetCoord));\nassert (posParam.jValues().get(3).equals(cometCoord));\nassert (posParam.jValues().get(4).equals(altAzCoord));","title":"Coordinate Types Example"},{"location":"/params/keys-parameters.html#choice","text":"A key for a choice item similar to an enumeration.\nScala //Choice\nval choices = Choices.from(\"A\", \"B\", \"C\")\n\n//keys\nval choice1Key: GChoiceKey = ChoiceKey.make(\"mode\", NoUnits, choices)\nval choice2Key: GChoiceKey = ChoiceKey.make(\n  \"mode-reset\",\n  NoUnits,\n  Choices.fromChoices(Choice(\"c\"), Choice(\"b\"), Choice(\"a\"))\n)\n\n//store values\nval p1: Parameter[Choice] = choice1Key\n  .setAll(Array(Choice(\"A\")))\n  .withUnits(Units.foot)\nval p2: Parameter[Choice] = choice2Key.setAll(Array(Choice(\"c\")))\n\n//add units\nval paramWithFoot = p1.withUnits(Units.foot)\n\n//default unit is NoUnits\nassert(p2.units === Units.NoUnits)\n\n//retrieving values\nval head: Choice          = p1.head\nval values: Array[Choice] = p2.values\n Java //Choice\nfinal Choices choices = Choices.from(\"A\", \"B\", \"C\");\n\n//keys\nGChoiceKey choice1Key = JKeyType.ChoiceKey().make(\"mode\", JUnits.NoUnits, choices);\nGChoiceKey choice2Key = JKeyType.ChoiceKey().make(\n        \"mode-reset\", JUnits.NoUnits,\n        Choices.fromChoices(\n                new Choice(\"c\"),\n                new Choice(\"b\"),\n                new Choice(\"a\")));\n\n//store values\nParameter<Choice> p1 = choice1Key.set(new Choice(\"A\")).withUnits(JUnits.foot);\nParameter<Choice> p2 = choice2Key.set(new Choice(\"c\"));\n\n//add units\nParameter<Choice> paramWithFoot = p1.withUnits(JUnits.foot);\n\n//default unit is NoUnits()\nboolean bDefaultUnit = JUnits.NoUnits == p2.units();\n\n//retrieving values\nChoice head = p1.head();\nList<Choice> values = p2.jValues();","title":"Choice"},{"location":"/params/keys-parameters.html#struct","text":"Structs can be used to create a hierarchy of parameters. The following snippet also demonstrates some of the utility functions available for manipulating parameter sets:\nScala //keys\nval skey: Key[Struct] = StructKey.make(\"myStruct\")\n\nval ra    = KeyType.StringKey.make(\"ra\")\nval dec   = KeyType.StringKey.make(\"dec\")\nval epoch = KeyType.DoubleKey.make(\"epoch\")\n\n//initialize struct\nval struct1: Struct = Struct().madd(ra.set(\"12:13:14.1\"), dec.set(\"32:33:34.4\"), epoch.set(1950.0))\nval struct2: Struct = Struct().madd(dec.set(\"32:33:34.4\"), ra.set(\"12:13:14.1\"), epoch.set(1970.0))\n\n//make parameters\nval p1: Parameter[Struct] = skey.set(struct1)\nval p2: Parameter[Struct] = skey.set(struct1, struct2)\n\n//add units\nval paramWithLightYear = p1.withUnits(Units.lightyear)\n\n//default unit is NoUnits\nassert(p2.units === Units.NoUnits)\n\n//retrieving values\nval head: Struct          = p1.head\nval values: Array[Struct] = p2.values\n\n//get individual keys\nval firstKey: Option[Parameter[String]]  = struct1.get(KeyType.StringKey.make(\"ra\"))\nval secondKey: Option[Parameter[String]] = struct1.get(\"dec\", KeyType.StringKey)\nval thirdKey: Option[Parameter[Double]]  = struct1.get(\"epoch\", KeyType.DoubleKey)\n\n//access parameter using 'parameter' or 'apply' method\nassert(struct1.parameter(ra) === struct1(ra))\n\n//remove a parameter and verify it doesn't exist\nval mutated1: Struct = struct1.remove(ra) //using key\nval mutated2         = struct1.remove(firstKey.get)\nassert(mutated1.exists(ra) === false)\nassert(mutated2.exists(ra) === false)\n\n//find out missing keys\nval missingKeySet: Set[String] = mutated1.missingKeys(ra, dec, epoch, KeyType.FloatKey.make(\"missingKey\"))\nassert(missingKeySet === Set(\"ra\", \"missingKey\"))\n Java //keys\nKey<Struct> skey = JKeyType.StructKey().make(\"myStruct\", JUnits.NoUnits);\n\nKey<String> ra = JKeyType.StringKey().make(\"ra\", JUnits.NoUnits);\nKey<String> dec = JKeyType.StringKey().make(\"dec\", JUnits.NoUnits);\nKey<Double> epoch = JKeyType.DoubleKey().make(\"epoch\", JUnits.year);\n\n//initialize struct\nStruct struct1 = new Struct().madd(\n        ra.set(\"12:13:14.1\"),\n        dec.set(\"32:33:34.4\"),\n        epoch.set(1950.0));\nStruct struct2 = new Struct().madd(\n        dec.set(\"32:33:34.4\"),\n        ra.set(\"12:13:14.1\"),\n        epoch.set(1970.0));\n\n//make parameters\nParameter<Struct> p1 = skey.set(struct1);\nParameter<Struct> p2 = skey.set(struct1, struct2);\n\n//add units\nParameter<Struct> paramWithLightYear = p1.withUnits(JUnits.lightyear);\n\n//default unit is NoUnits()\nboolean bDefaultUnit = JUnits.NoUnits == p1.units();\n\n//retrieving values\nStruct head = p1.head();\nList<Struct> structs = p2.jValues();\n\n//get individual keys\nOptional<Parameter<String>> firstKey = struct1.jGet(JKeyType.StringKey().make(\"ra\", JUnits.NoUnits));\nOptional<Parameter<String>> secondKey = struct1.jGet(\"dec\", JKeyType.StringKey());\nOptional<Parameter<Double>> thirdKey = struct1.jGet(\"epoch\", JKeyType.DoubleKey());\n\n//access parameter using 'parameter' and 'apply' method\nboolean bSuccess = struct1.parameter(ra) == struct1.apply(ra);\n\n//remove a parameter and verify it doesn't exist\nStruct mutated1 = struct1.remove(ra); //using key\nStruct mutated2 = struct1.remove(firstKey.orElseThrow());\n\n//find out missing keys\nSet<String> missingKeySet = mutated1.jMissingKeys(ra, dec, epoch, JKeyType.StringKey().make(\"someRandomKey\", JUnits.NoUnits));\nSet<String> expectedMissingKeys = Set.of(\"ra\", \"someRandomKey\");","title":"Struct"},{"location":"/params/keys-parameters.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source Code for Examples"},{"location":"/params/units.html","text":"","title":"Units"},{"location":"/params/units.html#units","text":"Listed below are Units of Measurement, supported by TMT observatory framework and are available as Enumerated values. Units are optionally attached to Parameter Keys.\nNote Units are made available via separate files, for consumption in Scala and Java code. Import csw.messages.params.models.Units for Scala Import csw.params.javadsl.JUnits for Java.\nNote The set of supported Units will be modified as more required Units are discovered.","title":"Units"},{"location":"/params/units.html#default-units-for-keys","text":"The default unit for UTCTimeKey and TAITimeKey (in Scala and Java both) is second. For all the remaining keys, default unit is NoUnits.","title":"Default Units for Keys"},{"location":"/params/units.html#si-units","text":"Name Abbreviation Description angstrom Angstrom 10 -1 nm arcmin arcmin arc minute; angular measurement arcsec arcsec arc second: angular measurement day d day - 24 hours degree deg degree: agular measurement 1/360 of full rotation elvolt eV electron volt 1.6022x10-19 J gram g gram 10-3 kg hour h hour 3.6x10+3 s hertz Hz frequency joule J Joule: energy N m kelvin K Kelvin: temperature with a null point at absolute zero kilogram kg kilogram, base unit of mass in SI kilometer km kilometers - 10+3 m liter l liter, metric unit of volume 10+3 cm+3 meter m meter: base unit of length in SI marcsec mas milli arc second: angular measurement 10-3 arcsec millimeter mm millimeters - 10-3 m millisecond ms milliseconds - 10-3 s micron µm micron: alias for micrometer micrometer µm micron: 10-6 m minute min minute 6x10+1 s newton N Newton: force pascal Pa Pascal: pressure radian rad radian: angular measurement of the ratio between the length of an arc and its radius second s second: base unit of time in SI sday sday sidereal day is the time of one rotation of the Earth: 8.6164x10+4 s steradian sr steradian: unit of solid angle in SI - rad+2 microarcsec µas micro arcsec: angular measurement volt V Volt: electric potential or electromotive force watt W Watt: power week wk week - 7 d year yr year - 3.6525x10+2 d","title":"SI Units"},{"location":"/params/units.html#cgs-units","text":"Name Abbreviation Description coulomb C coulomb: electric charge centimeter cm centimeter erg erg erg: CGS unit of energy","title":"CGS Units"},{"location":"/params/units.html#astrophysical-units","text":"Name Abbreviation Description au AU astronomical unit: approximately the mean Earth-Sun distance jansky Jy Jansky: spectral flux density - 10-26 W/Hz m+2 lightyear lyr light year - 9.4607x10+15 m mag mag stellar magnitude","title":"Astrophysical Units"},{"location":"/params/units.html#imperial-units","text":"Name Abbreviation Description cal cal thermochemical calorie: pre-SI metric unit of energy foot ft international foot - 1.2x10+1 inch inch inch international inch - 2.54 cm pound lb international avoirdupois pound - 1.6x10+1 oz mile mi international mile - 5.28x10+3 ft ounce oz international avoirdupois ounce yard yd international yard - 3 ft","title":"Imperial Units"},{"location":"/params/units.html#others-engineering","text":"Name Abbreviation Description NoUnits none scalar - no units specified encoder enc encoder counts count ct counts as for an encoder or detector pix pix pixel","title":"Others - Engineering"},{"location":"/params/units.html#usage-examples","text":"Scala //declare keyname\nval s1: String = \"encoder\"\n\n//making 3 keys\nval k1: Key[Boolean] = KeyType.BooleanKey.make(s1)\nval k2: Key[Short]   = KeyType.ShortKey.make(\"RandomKeyName\")\nval k3: Key[String]  = KeyType.StringKey.make(s1)\n\n//storing a single value, default unit is NoUnits\nval bParam: Parameter[Boolean] = k1.set(true)\nval bDefaultUnitSet: Boolean   = bParam.units === Units.NoUnits //true\n\n//default unit for UTCTimeKey\nval tParam: Parameter[UTCTime] = KeyType.UTCTimeKey.make(\"now\").set(UTCTime.now())\nval defaultTimeUnit: Units     = tParam.units //is second\n\n//storing multiple values\nval paramOfShorts: Parameter[Short] = k2.set(1, 2, 3, 4)\n\n//values to store\nval weekDays: Array[String] = Array(\"Sunday\", \"Monday\", \"Tuesday\")\n\n//default units via set\nval paramWithUnits1: Parameter[String] = k3.setAll(weekDays)\n//associating units via withUnits\nval paramWithUnits2: Parameter[String] = k3 -> weekDays withUnits Units.day\n//change existing unit\nval paramWithUnits3: Parameter[Short] = paramOfShorts.withUnits(Units.meter) Java //declare keyname\nString s1 = \"encoder\";\n\n//making 3 keys\nKey<Boolean> k1 = JKeyType.BooleanKey().make(s1);\nKey<Short> k2 = JKeyType.ShortKey().make(\"RandomKeyName\", JUnits.NoUnits);\nKey<String> k3 = JKeyType.StringKey().make(s1, JUnits.NoUnits);\n\n//storing a single value, default unit is NoUnits()\nParameter<Boolean> bParam = k1.set(true);\nboolean bDefaultUnitSet = bParam.units() == JUnits.NoUnits; //true\n\n//default unit for UTCTimeKey/TAITimeKey\nParameter<UTCTime> tParam = JKeyType\n        .UTCTimeKey()\n        .make(\"now\")\n        .set(UTCTime.now());\nUnits defaultTimeUnit = tParam.units(); //is second\n\n//storing multiple values\nParameter<Short> paramOfShorts = k2.set(\n        (short) 1,\n        (short) 2,\n        (short) 3,\n        (short) 4\n);\n\n//values to store\nString[] weekDays = {\"Sunday\", \"Monday\", \"Tuesday\"};\n\n//defaults units via set\nParameter<String> paramWithUnits1 = k3.setAll(weekDays);\n//associating units via withUnits\nParameter<String> paramWithUnits2 = k3.setAll(weekDays).withUnits(JUnits.day);\n//change existing unit\nParameter<Short> paramWithUnits3 = paramOfShorts.withUnits(JUnits.meter);","title":"Usage Examples"},{"location":"/params/units.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source Code for Examples"},{"location":"/params/subsystem.html","text":"","title":"Subsystem"},{"location":"/params/subsystem.html#subsystem","text":"TMT Observatory system is composed of many subsystems. The subsystems that are known participants in the TMT Software System are predefined and the list is covered under the Subsystem enumeration. They are identified using an abbreviation typically of 3 or 4 letters.\nSusbsystem values are used to construct Prefix and are used in communication vehicles such as Commands, Events and States.\nNote Subsystems are made available via separate files, for consumption in Scala and Java code. Import csw.prefix.models.Subsystem for Scala Import csw.prefix.javadsl.JSubsystem for Java.","title":"Subsystem"},{"location":"/params/subsystem.html#list-of-subsystems","text":"Abbreviation Susbsystem name AOESW AO Executive Software APS Alignment and Phasing System CIS Communications and Information Systems CLN Mirror Cleaning System CRYO Cryogenic Cooling System CSW Common Software DMS Data Management System DPS Data Processing System ESEN Engineering Sensor System ESW Executive Software System FMCS Facility Management Control System GMS Global Metrology System Controls IRIS InfraRed Imaging Spectrometer LGSF Lasert Guide Star Facility M1CS M1 Control System MODHIS Multi-Object Diffraction-limited High-resolution Infrared Spectrograph NFIRAOS Narrow Field Infrared AO System NSCU NFIRAOS Science Calibration Unit OSS Observatory Safety System PFCS Prime Focus Camera Controls PSFR NFIRAOS AO PSF Reconstructor REFR Refrigeration Control System RTC NFIRAOS Real-time Controller RPG NFIRAOS AO Reconstructor Parameter Generator SCMS Site Conditions Monitoring System SOSS Science Operations Support System TCS Telescope Control System WFOS Wide Field Optical Spectrometer Container Container subsystem","title":"List of Subsystems"},{"location":"/params/subsystem.html#usage-examples","text":"The usage examples can be found in Events, Commands, States","title":"Usage Examples"},{"location":"/params/commands.html","text":"","title":"Commands"},{"location":"/params/commands.html#commands","text":"Commands are parameter sets called Setup, Observe, and Wait. A command is created with the source of the command, given by a prefix, the name of the command, and an optional ObsId. Parameters are added to the command as needed. As the ESW design is developed, these command structures may evolve.","title":"Commands"},{"location":"/params/commands.html#obsid","text":"An ObsID, or observation ID, indicates the observation the command is associated with. It can be constructed by creating an instance of ObsId.\nScala val obsId: ObsId = ObsId(\"Obs001\") Java ObsId obsId = new ObsId(\"Obs001\");","title":"ObsId"},{"location":"/params/commands.html#prefix","text":"The source of the command is given by the prefix, which should be the full name of the component sending the command. A prefix can be constructed with a string, but must start with a valid subsystem as in Subsystem. A component developer should supply a valid prefix string and the subsystem will be automatically parsed from it. An example of a valid string prefix is “nfiraos.ncc.trombone”.\nSee below examples:\nScala //using constructor, supplying subsystem and prefix both\nval prefix1: Prefix = Prefix(\"nfiraos.ncc.trombone\")\n\n//just by supplying prefix\nval prefix2: Prefix = Prefix(\"tcs.mobie.blue.filter\")\n\n//invalid prefix string which does not contain valid subsystem in the beginning will throw an exception,\n//val badPrefix: Prefix = Prefix(\"abcdefgh\")\n\n//use implicit conversion to convert from String to Prefix\nval prefix3: Prefix = Prefix(\"wfos.prog.cloudcover\") Java //using constructor, supplying subsystem and prefix both\nPrefix prefix1 = Prefix.apply(JSubsystem.NFIRAOS, \"ncc.trombone\");\n\n//just by supplying prefix\nPrefix prefix2 = Prefix.apply(JSubsystem.TCS, \"mobie.blue.filter\");\n\n//invalid prefix string which does not contain valid subsystem in the beginning will throw an exception,\n// Prefix badPrefix = Prefix.apply(\"abcdefgh\");","title":"Prefix"},{"location":"/params/commands.html#commandname","text":"Each command has a name given as a string. The CommandName object wraps the string name. The string should be continuous with no spaces.","title":"CommandName"},{"location":"/params/commands.html#setup-command","text":"This command is used to describe a goal that a system should match. The component developer is required to supply following arguments to create a Setup command.\nPrefix: the source of the command as described above CommandName: a simple string name for the command (no spaces) ObsId: an optional observation Id. paramSet: Optional Set of Parameters. Default is empty.\nScala //keys\nval k1: Key[Int]    = KeyType.IntKey.make(\"encoder\")\nval k2: Key[String] = KeyType.StringKey.make(\"stringThing\")\nval k2bad: Key[Int] = KeyType.IntKey.make(\"missingKey\")\nval k3: Key[Int]    = KeyType.IntKey.make(\"filter\")\nval k4: Key[Float]  = KeyType.FloatKey.make(\"correction\")\n\n//prefix\nval prefix: Prefix = Prefix(\"wfos.red.detector\")\n\n//parameters\nval i1: Parameter[Int]    = k1.set(22)\nval i2: Parameter[String] = k2.set(\"A\")\n\n//create Setup, add sequentially using add\nval sc1: Setup = Setup(prefix, CommandName(\"move\"), Some(obsId)).add(i1).add(i2)\n\n//access keys\nval k1Exists: Boolean = sc1.exists(k1) //true\n\n//access parameters\nval tryParam1: Try[Parameter[Int]] = Try(sc1(k1))    //success\nval tryk2Bad: Try[Parameter[Int]]  = Try(sc1(k2bad)) //failure\n\n//add more than one parameters, using madd\nval sc2: Setup     = sc1.madd(k3.set(1, 2, 3, 4).withUnits(Units.day), k4.set(1.0f, 2.0f))\nval paramSize: Int = sc2.size\n\n//add binary payload\nval byteKey1: Key[Byte] = ByteKey.make(\"byteKey1\")\nval byteKey2: Key[Byte] = ByteKey.make(\"byteKey2\")\nval bytes1: Array[Byte] = Array[Byte](10, 20)\nval bytes2: Array[Byte] = Array[Byte](30, 40)\n\nval b1: Parameter[Byte] = byteKey1.setAll(bytes1)\nval b2: Parameter[Byte] = byteKey2.setAll(bytes2)\n\nval sc3: Setup = Setup(prefix, CommandName(\"move\"), Some(obsId), Set(b1, b2))\n\n//remove a key\nval sc4: Setup = sc3.remove(b1)\n\n//list all keys\nval allKeys: Set[String] = sc4.paramSet.map(_.keyName)\n Java //keys\nKey<Integer> k1 = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<String> k2 = JKeyType.StringKey().make(\"stringThing\");\nKey<Integer> k2bad = JKeyType.IntKey().make(\"missingKey\");\nKey<Integer> k3 = JKeyType.IntKey().make(\"filter\");\nKey<Float> k4 = JKeyType.FloatKey().make(\"correction\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"red.detector\");\n\n//parameters\nParameter<Integer> i1 = k1.set(22);\nParameter<String> i2 = k2.set(\"A\");\n\n//create setup, add sequentially using add\nSetup sc1 = new Setup(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(i1).add(i2);\n\n//access keys\nboolean k1Exists = sc1.exists(k1); //true\n\n//access parameters\nOptional<Parameter<Integer>> optParam1 = sc1.jGet(k1); //present\nOptional<Parameter<Integer>> optK2Bad = sc1.jGet(k2bad); //absent\n\n//add more than one parameters, using madd\nSetup sc2 = sc1.madd(k3.set(1, 2, 3, 4).withUnits(JUnits.day), k4.set(1.0f, 2.0f));\nint paramSize = sc2.size();\n\n//add binary payload\nKey<Byte> byteKey1 = JKeyType.ByteKey().make(\"byteKey1\");\nKey<Byte> byteKey2 = JKeyType.ByteKey().make(\"byteKey2\");\nByte[] bytes1 = {10, 20};\nByte[] bytes2 = {30, 40};\n\nParameter<Byte> b1 = byteKey1.setAll(bytes1);\nParameter<Byte> b2 = byteKey2.setAll(bytes2);\n\nSetup sc3 = new Setup(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(b1).add(b2);\n\n//remove a key\nSetup sc4 = sc3.remove(b1);\n\n//list all keys\njava.util.List<String> allKeys = sc4.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toList());","title":"Setup Command"},{"location":"/params/commands.html#observe-command","text":"This command describes a science observation. It is intended to only be sent to Science Detector Assemblies and Sequencers.\nScala //keys\nval k1: Key[Boolean] = KeyType.BooleanKey.make(\"repeat\")\nval k2: Key[Int]     = KeyType.IntKey.make(\"expTime\")\nval k2bad: Key[Int]  = KeyType.IntKey.make(\"missingKey\")\nval k3: Key[Int]     = KeyType.IntKey.make(\"filter\")\nval k4: Key[UTCTime] = KeyType.UTCTimeKey.make(\"creation-time\")\n\n//prefix\nval prefix: Prefix = Prefix(\"wfos.red.detector\")\n\n//parameters\nval i1: Parameter[Boolean] = k1.set(true, false, true, false)\nval i2: Parameter[Int]     = k2.set(1, 2, 3, 4)\n\n//create Observe, add sequentially using add\nval oc1: Observe = Observe(prefix, CommandName(\"move\"), Some(obsId)).add(i1).add(i2)\n\n//access parameters using apply method\nval k1Param: Parameter[Boolean] = oc1.get(k1).get //true\nval values: Array[Boolean]      = k1Param.values\n\n//access parameters\nval tryParam1: Try[Parameter[Boolean]] = Try(oc1(k1))    //success\nval tryk2Bad: Try[Parameter[Int]]      = Try(oc1(k2bad)) //failure\n\n//add more than one parameters, using madd\nval oc2: Observe   = oc1.madd(k3.set(1, 2, 3, 4).withUnits(Units.day), k4.set(UTCTime.now()))\nval paramSize: Int = oc2.size\n\n//update existing key with set\nval oc3: Observe = oc1.add(k2.set(5, 6, 7, 8))\n\n//remove a key\nval oc4: Observe = oc2.remove(k4)\n Java //keys\nKey<Boolean> k1 = JKeyType.BooleanKey().make(\"repeat\");\nKey<Integer> k2 = JKeyType.IntKey().make(\"expTime\", JUnits.second);\nKey<Integer> k2bad = JKeyType.IntKey().make(\"missingKey\");\nKey<Integer> k3 = JKeyType.IntKey().make(\"filter\");\nKey<UTCTime> k4 = JKeyType.UTCTimeKey().make(\"creation-time\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"red.detector\");\n\n//parameters\nBoolean[] boolArray = {true, false, true, false};\nParameter<Boolean> i1 = k1.setAll(boolArray);\nParameter<Integer> i2 = k2.set(1, 2, 3, 4);\n\n//create Observe, add sequentially using add\nObserve oc1 = new Observe(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(i1).add(i2);\n\n//access parameters\nOptional<Parameter<Boolean>> k1Param = oc1.jGet(k1); //present\njava.util.List<Boolean> values = k1Param.orElseThrow().jValues();\n\n//access parameters\nOptional<Parameter<ArrayData<Float>>> k2BadParam = oc1.jGet(k2bad.keyName(), JKeyType.FloatArrayKey());\n\n//add more than one parameters, using madd\nObserve oc2 = oc1.madd(k3.set(1, 2, 3, 4).withUnits(JUnits.day), k4.set(UTCTime.now()));\nint paramSize = oc2.size();\n\n//update existing key with set\nInteger[] intArray = {5, 6, 7, 8};\nObserve oc3 = oc1.add(k2.setAll(intArray));\n\n//remove a key\nObserve oc4 = oc2.remove(k4);\n\n//list all keys\njava.util.List<String> allKeys = oc4.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toList());","title":"Observe Command"},{"location":"/params/commands.html#wait-command","text":"This command causes a Sequencer to wait until notified. It can only be sent to Sequencers.\nScala //keys\nval k1: Key[Boolean] = KeyType.BooleanKey.make(\"repeat\")\nval k2: Key[Int]     = KeyType.IntKey.make(\"expTime\")\nval k2bad: Key[Int]  = KeyType.IntKey.make(\"missingKey\")\nval k3: Key[Int]     = KeyType.IntKey.make(\"filter\")\nval k4: Key[UTCTime] = KeyType.UTCTimeKey.make(\"creation-time\")\n\n//parameters\nval i1: Parameter[Boolean] = k1.set(true, false, true, false)\nval i2: Parameter[Int]     = k2.set(1, 2, 3, 4)\n\n//prefix\nval prefix: Prefix = Prefix(\"wfos.red.detector\")\n\n//create wait, add sequentially using add\nval wc1: Wait = Wait(prefix, CommandName(\"move\"), Some(obsId)).add(i1).add(i2)\n\n//access params using get method\nval k1Param: Option[Parameter[Boolean]] = wc1.get(k1)\nval values: Array[Boolean]              = k1Param.map(_.values).getOrElse(Array.empty[Boolean])\n\n//access parameters\nval tryParam1: Try[Parameter[Boolean]] = Try(wc1(k1))    //success\nval tryk2Bad: Try[Parameter[Int]]      = Try(wc1(k2bad)) //failure\n\n//add more than one parameters, using madd\nval wc2: Wait      = wc1.madd(k3.set(1, 2, 3, 4).withUnits(Units.day), k4.set(UTCTime.now()))\nval paramSize: Int = wc2.size\n\n//update existing key with set\nval wc3: Wait = wc1.add(k2.set(5, 6, 7, 8))\n\n//remove a key\nval wc4: Wait = wc2.remove(k4)\n Java //keys\nKey<Boolean> k1 = JKeyType.BooleanKey().make(\"repeat\");\nKey<Integer> k2 = JKeyType.IntKey().make(\"expTime\", JUnits.second);\nKey<Integer> k2bad = JKeyType.IntKey().make(\"missingKey\");\nKey<Integer> k3 = JKeyType.IntKey().make(\"filter\");\nKey<UTCTime> k4 = JKeyType.UTCTimeKey().make(\"creation-time\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"red.detector\");\n\n//parameters\nBoolean[] boolArray = {true, false, true, false};\nParameter<Boolean> i1 = k1.setAll(boolArray);\nParameter<Integer> i2 = k2.set(1, 2, 3, 4);\n\n//create Wait, add sequentially using add\nWait wc1 = new Wait(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(i1).add(i2);\n\n//access parameters using jGet\nOptional<Parameter<Boolean>> k1Param = wc1.jGet(k1); //present\njava.util.List<Boolean> values = k1Param.orElseThrow().jValues();\n\n//access parameters\nOptional<Parameter<ArrayData<Float>>> k2BadParam = wc1.jGet(\"absentKeyHere\", JKeyType.FloatArrayKey());\n\n//add more than one parameters, using madd\nWait wc2 = wc1.madd(k3.set(1, 2, 3, 4).withUnits(JUnits.day), k4.set(UTCTime.now()));\nint paramSize = wc2.size();\n\n//update existing key with set\nInteger[] intArray = {5, 6, 7, 8};\nWait wc3 = wc1.add(k2.setAll(intArray));\n\n//remove a key\nWait wc4 = wc2.remove(k4);\n\n//list all keys\njava.util.List<String> allKeys = wc4.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toList());","title":"Wait Command"},{"location":"/params/commands.html#json-serialization","text":"Commands can be serialized to JSON. The library has provided JsonSupport helper class and methods to serialize Setup, Observe and Wait commands.\nScala import play.api.libs.json.{JsValue, Json}\n\n//key\nval k1: Key[MatrixData[Double]] = DoubleMatrixKey.make(\"myMatrix\")\n//values\nval m1: MatrixData[Double] = MatrixData.fromArrays(\n  Array(1.0, 2.0, 3.0),\n  Array(4.1, 5.1, 6.1),\n  Array(7.2, 8.2, 9.2)\n)\n\n//prefix\nval prefix: Prefix = Prefix(\"wfos.red.detector\")\n\n//parameter\nval i1: Parameter[MatrixData[Double]] = k1.set(m1)\n\n//commands\nval sc: Setup   = Setup(prefix, CommandName(\"move\"), Some(obsId)).add(i1)\nval oc: Observe = Observe(prefix, CommandName(\"move\"), Some(obsId)).add(i1)\nval wc: Wait    = Wait(prefix, CommandName(\"move\"), Some(obsId)).add(i1)\n\n//json support - write\nval scJson: JsValue = JsonSupport.writeSequenceCommand(sc)\nval ocJson: JsValue = JsonSupport.writeSequenceCommand(oc)\nval wcJson: JsValue = JsonSupport.writeSequenceCommand(wc)\n\n//optionally prettify\nval str: String = Json.prettyPrint(scJson)\n\n//construct command from string\nval scFromPrettyStr = JsonSupport.readSequenceCommand[Setup](Json.parse(str))\n\n//json support - read\nval sc1: Setup   = JsonSupport.readSequenceCommand[Setup](scJson)\nval oc1: Observe = JsonSupport.readSequenceCommand[Observe](ocJson)\nval wc1: Wait    = JsonSupport.readSequenceCommand[Wait](wcJson) Java //key\nKey<MatrixData<Double>> k1 = JKeyType.DoubleMatrixKey().make(\"myMatrix\");\n\n//values\nDouble[][] doubles = {{1.0, 2.0, 3.0}, {4.1, 5.1, 6.1}, {7.2, 8.2, 9.2}};\nMatrixData<Double> m1 = MatrixData.fromArrays(doubles);\n\n//parameter\nParameter<MatrixData<Double>> i1 = k1.set(m1);\n\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"blue.filter\");\n\n//commands\nSetup sc = new Setup(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(i1);\nObserve oc = new Observe(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(i1);\nWait wc = new Wait(prefix, new CommandName(\"move\"), Optional.of(obsId)).add(i1);\n\n//json support - write\nJsValue scJson = JavaJsonSupport.writeSequenceCommand(sc);\nJsValue ocJson = JavaJsonSupport.writeSequenceCommand(oc);\nJsValue wcJson = JavaJsonSupport.writeSequenceCommand(wc);\n\n//optionally prettify\nString str = Json.prettyPrint(scJson);\n\n//construct command from string\nSetup sc1 = JavaJsonSupport.readSequenceCommand(Json.parse(str));\nObserve oc1 = JavaJsonSupport.readSequenceCommand(ocJson);\nWait wc1 = JavaJsonSupport.readSequenceCommand(wcJson);","title":"JSON serialization"},{"location":"/params/commands.html#unique-key-constraint","text":"By design, a ParameterSet in a Setup, Observe, or Wait command is optimized to store only unique keys. When using add or madd methods on commands to add new parameters, if the parameter being added has a key which is already present in the paramSet, the already stored parameter will be replaced by the given parameter.\nNote If the Set is created by component developers and given directly while creating a command, then it will be the responsibility of component developers to maintain uniqueness with parameters based on key.\nHere are some examples that illustrate this point:\nScala //keys\nval encoderKey: Key[Int] = KeyType.IntKey.make(\"encoder\")\nval filterKey: Key[Int]  = KeyType.IntKey.make(\"filter\")\nval miscKey: Key[Int]    = KeyType.IntKey.make(\"misc.\")\n\n//prefix\nval prefix: Prefix = Prefix(\"wfos.red.detector\")\n\n//params\nval encParam1: Parameter[Int] = encoderKey.set(1)\nval encParam2: Parameter[Int] = encoderKey.set(2)\nval encParam3: Parameter[Int] = encoderKey.set(3)\n\nval filterParam1: Parameter[Int] = filterKey.set(1)\nval filterParam2: Parameter[Int] = filterKey.set(2)\nval filterParam3: Parameter[Int] = filterKey.set(3)\n\nval miscParam1 = miscKey.set(100)\n\n//Setup command with duplicate key via constructor\nval setup: Setup =\n  Setup(\n    prefix,\n    CommandName(\"move\"),\n    Some(obsId),\n    Set(encParam1, encParam2, encParam3, filterParam1, filterParam2, filterParam3)\n  )\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nval uniqueKeys1 = setup.paramSet.toList.map(_.keyName)\n\n//try adding duplicate keys via add + madd\nval changedSetup: Setup = setup\n  .add(encParam3)\n  .madd(\n    filterParam1,\n    filterParam2,\n    filterParam3\n  )\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nval uniqueKeys2: List[String] = changedSetup.paramSet.toList.map(_.keyName)\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nval finalSetUp: Setup = setup.madd(Set(miscParam1, encParam1))\n//now contains encoderKey, filterKey, miscKey\nval uniqueKeys3: List[String] = finalSetUp.paramSet.toList.map(_.keyName) Java //keys\nKey<Integer> encoderKey = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<Integer> filterKey = JKeyType.IntKey().make(\"filter\");\nKey<Integer> miscKey = JKeyType.IntKey().make(\"misc.\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"blue.filter\");\n\n//params\nParameter<Integer> encParam1 = encoderKey.set(1);\nParameter<Integer> encParam2 = encoderKey.set(2);\nParameter<Integer> encParam3 = encoderKey.set(3);\n\nParameter<Integer> filterParam1 = filterKey.set(1);\nParameter<Integer> filterParam2 = filterKey.set(2);\nParameter<Integer> filterParam3 = filterKey.set(3);\n\nParameter<Integer> miscParam1 = miscKey.set(100);\n\n//Setup command with duplicate key via madd\nSetup setup = new Setup(prefix, new CommandName(\"move\"), Optional.of(obsId)).madd(\n        encParam1,\n        encParam2,\n        encParam3,\n        filterParam1,\n        filterParam2,\n        filterParam3);\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nSet<String> uniqueKeys1 = setup.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//try adding duplicate keys via add + madd\nSetup changedSetup = setup.add(encParam3).madd(filterParam1, filterParam2, filterParam3);\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nSet<String> uniqueKeys2 = changedSetup.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nSetup finalSetUp = setup.madd(miscParam1, encParam1);\n//now contains encoderKey, filterKey, miscKey\nSet<String> uniqueKeys3 = finalSetUp.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());","title":"Unique Key constraint"},{"location":"/params/commands.html#cloning-a-command","text":"In order to track the completion of a command, every command that is sent must have a unique RunId. If you wish to resubmit a previously sent Setup, the cloneCommand method must be used prior to submission to create a new command from existing parameters, but with a new RunId.\nScala val setup  = Setup(prefix, commandName, Some(obsId)).madd(i1)\nval setup2 = setup.cloneCommand\n\nval observe  = Observe(prefix, commandName, Some(obsId)).madd(i1)\nval observe2 = observe.cloneCommand\n\nval wait  = Wait(prefix, commandName, Some(obsId)).madd(i1)\nval wait2 = wait.cloneCommand Java Setup setup = new Setup(prefix, commandName, Optional.of(obsId)).add(encoderParam);\nSetup setup2 = setup.cloneCommand();\n\nObserve observe = new Observe(prefix, commandName, Optional.empty()).add(encoderParam);\nObserve observe2 = observe.cloneCommand();\n\nWait wait = new Wait(prefix, commandName, Optional.of(obsId)).add(encoderParam);\nWait wait2 = wait.cloneCommand();","title":"Cloning a Command"},{"location":"/params/commands.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source Code for Examples"},{"location":"/params/events.html","text":"","title":"Events"},{"location":"/params/events.html#events","text":"Events are the most basic type of asynchronous notification in TMT when an activity occurs somewhere in the TMT system and other components need to be notified. Each type of event has a unique purpose and unique information, but they all share same structural features. All events have EventInfo and a ParameterSet.\nNote The csw-params library offers out of the box support to serialize Events using Cbor, so that events can be produced and consumed by JVM (Java virtual machine) as well as Non-JVM applications. For more on Cbor, refer to the technical doc.","title":"Events"},{"location":"/params/events.html#eventtime","text":"Each event includes its time of creation in UTC format. You can access that eventTime as follows:\nScala val source    = Prefix(\"iris.filter.wheel\")\nval eventName = EventName(\"temperatures\")\nval event     = SystemEvent(source, eventName)\n\n// accessing eventTime\nval eventTime = event.eventTime Java //apply returns current time in UTC\nUTCTime now = UTCTime.now();\n\n//using constructor\nUTCTime anHourAgo = new UTCTime(Instant.now().minusSeconds(3600));\n\n//return current time in UTC\nUTCTime currentTime = UTCTime.now();\n\n//some past time using utility function\nUTCTime aDayAgo = new UTCTime(Instant.now().minusSeconds(86400));","title":"EventTime"},{"location":"/params/events.html#system-event","text":"SystemEvent is the type used to describe the majority of events in the system. An example is a demand that is the output of an algorithm in one component that is used as an input to another. SystemEvent is also used to publish internal state or status values of a component that may be of interest to other components in the system.\nScala //keys\nval k1: Key[Int]    = KeyType.IntKey.make(\"encoder\")\nval k2: Key[Int]    = KeyType.IntKey.make(\"speed\")\nval k3: Key[String] = KeyType.StringKey.make(\"filter\")\nval k4: Key[Int]    = KeyType.IntKey.make(\"notUsed\")\n\n//prefixes\nval ck1   = Prefix(\"wfos.red.filter\")\nval name1 = EventName(\"filterWheel\")\nval ck3   = Prefix(\"iris.imager.filter\")\nval name3 = EventName(\"status\")\n\n//parameters\nval p1: Parameter[Int]    = k1.set(22)\nval p2: Parameter[Int]    = k2.set(44)\nval p3: Parameter[String] = k3.set(\"A\", \"B\", \"C\", \"D\")\n\n//Create SystemEvent using madd\nval se1: SystemEvent = SystemEvent(ck1, name1).madd(p1, p2)\n//Create SystemEvent using apply\nval se2: SystemEvent = SystemEvent(ck3, name3, Set(p1, p2))\n//Create SystemEvent and use add\nval se3: SystemEvent = SystemEvent(ck3, name3).add(p1).add(p2).add(p3)\n\n//access keys\nval k1Exists: Boolean = se1.exists(k1) //true\n\n//access Parameters\nval p4: Option[Parameter[Int]] = se1.get(k1)\n\n//access values\nval v1: Array[Int] = se1(k1).values\nval v2: Array[Int] = se2.parameter(k2).values\n//k4 is missing\nval missingKeys: Set[String] = se3.missingKeys(k1, k2, k3, k4)\n\n//remove keys\nval se4: SystemEvent = se3.remove(k3)\n\n//add more than one parameters, using madd\nval se5: SystemEvent = se4.madd(k3.set(\"X\", \"Y\", \"Z\").withUnits(Units.day), k4.set(99, 100))\nval paramSize: Int   = se5.size\n\n//update existing key with set\nval se6: SystemEvent = se5.add(k2.set(5, 6, 7, 8))\n Java //keys\nKey<Integer> k1 = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<Integer> k2 = JKeyType.IntKey().make(\"speed\");\nKey<String> k3 = JKeyType.StringKey().make(\"filter\");\nKey<Integer> k4 = JKeyType.IntKey().make(\"notUsed\");\n\n//prefixes\nPrefix prefix1 = Prefix.apply(JSubsystem.WFOS, \"red.filter\");\nEventName name1 = new EventName(\"filterWheel\");\nPrefix prefix2 = Prefix.apply(JSubsystem.IRIS, \"imager.filter\");\nEventName name2 = new EventName(\"status\");\n\n//parameters\nParameter<Integer> p1 = k1.set(22);\nParameter<Integer> p2 = k2.set(44);\nParameter<String> p3 = k3.set(\"A\", \"B\", \"C\", \"D\");\n\n//Create SystemEvent using madd\nSystemEvent se1 = new SystemEvent(prefix1, name1).madd(p1, p2);\n//Create SystemEvent using add\nSystemEvent se2 = new SystemEvent(prefix2, name2).add(p1).add(p2);\n//Create SystemEvent and use add\nSystemEvent se3 = new SystemEvent(prefix2, name2).add(p1).add(p2).add(p3);\n\n//access keys\nboolean k1Exists = se1.exists(k1); //true\n\n//access Parameters\nOptional<Parameter<Integer>> p4 = se1.jGet(k1);\n\n//access values\nList<Integer> v1 = se1.jGet(k1).orElseThrow().jValues();\nList<Integer> v2 = se2.parameter(k2).jValues();\n//k4 is missing\nSet<String> missingKeys = se3.jMissingKeys(k1, k2, k3, k4);\n\n//remove keys\nSystemEvent se4 = se3.remove(k3);","title":"System Event"},{"location":"/params/events.html#observe-event","text":"ObserveEvent are standardized events used to describe an activities within the data acquisition process. These events are typically published by Science Detector Assemblies, which emit ObserveEvents during their exposures to signal the occurrence of specific activities/actions during the acquisition of data.\nNote The example ObserveEvents do not match the descriptions of the ESW Phase 1 review. The model files and documents can be used to create standard ObserveEvents.\nScala //keys\nval k1: Key[Int]    = KeyType.IntKey.make(\"readoutsCompleted\")\nval k2: Key[Int]    = KeyType.IntKey.make(\"coaddsCompleted\")\nval k3: Key[String] = KeyType.StringKey.make(\"fileID\")\nval k4: Key[Int]    = KeyType.IntKey.make(\"notUsed\")\n\n//prefixes\nval ck1   = Prefix(\"iris.ifu.detectorAssembly\")\nval name1 = EventName(\"readoutEnd\")\nval ck3   = Prefix(\"wfos.red.detector\")\nval name3 = EventName(\"exposureStarted\")\n\n//parameters\nval p1: Parameter[Int]    = k1.set(4)\nval p2: Parameter[Int]    = k2.set(2)\nval p3: Parameter[String] = k3.set(\"WFOS-RED-0001\")\n\n//Create ObserveEvent using madd\nval se1: ObserveEvent = ObserveEvent(ck1, name1).madd(p1, p2)\n//Create ObserveEvent using apply\nval se2: ObserveEvent = ObserveEvent(ck3, name3, Set(p1, p2))\n//Create ObserveEvent and use add\nval se3: ObserveEvent = ObserveEvent(ck3, name3).add(p1).add(p2).add(p3)\n\n//access keys\nval k1Exists: Boolean = se1.exists(k1) //true\n\n//access Parameters\nval p4: Option[Parameter[Int]] = se1.get(k1)\n\n//access values\nval v1: Array[Int] = se1(k1).values\nval v2: Array[Int] = se2.parameter(k2).values\n//k4 is missing\nval missingKeys: Set[String] = se3.missingKeys(k1, k2, k3, k4)\n\n//remove keys\nval se4: ObserveEvent = se3.remove(k3)\n Java //keys\nKey<Integer> k1 = JKeyType.IntKey().make(\"readoutsCompleted\", JUnits.NoUnits);\nKey<Integer> k2 = JKeyType.IntKey().make(\"coaddsCompleted\", JUnits.NoUnits);\nKey<String> k3 = JKeyType.StringKey().make(\"fileID\");\nKey<Integer> k4 = JKeyType.IntKey().make(\"notUsed\");\n\n//prefixes\nPrefix prefix1 = Prefix.apply(JSubsystem.IRIS, \"ifu.detectorAssembly\");\nEventName name1 = new EventName(\"readoutEnd\");\nPrefix prefix2 = Prefix.apply(JSubsystem.WFOS, \"red.detector\");\nEventName name2 = new EventName(\"exposureStarted\");\n\n//parameters\nParameter<Integer> p1 = k1.set(4);\nParameter<Integer> p2 = k2.set(2);\nParameter<String> p3 = k3.set(\"WFOS()-RED-0001\");\n\n//Create ObserveEvent using madd\nObserveEvent oc1 = new ObserveEvent(prefix1, name1).madd(p1, p2);\n//Create ObserveEvent using add\nObserveEvent oc2 = new ObserveEvent(prefix2, name2).add(p1).add(p2);\n//Create ObserveEvent and use add\nObserveEvent oc3 = new ObserveEvent(prefix2, name2).add(p1).add(p2).add(p3);\n\n//access keys\nboolean k1Exists = oc1.exists(k1); //true\n\n//access Parameters\nOptional<Parameter<Integer>> p4 = oc1.jGet(k1);\n\n//access values\nList<Integer> v1 = oc1.jGet(k1).orElseThrow().jValues();\nList<Integer> v2 = oc2.parameter(k2).jValues();\n//k4 is missing\nSet<String> missingKeys = oc3.jMissingKeys(k1, k2, k3, k4);\n\n//remove keys\nObserveEvent oc4 = oc3.remove(k3);","title":"Observe Event"},{"location":"/params/events.html#json-serialization","text":"Events can be serialized to JSON. The library has provided JsonSupport helper class and methods to serialize Status, Observe and System events.\nScala import play.api.libs.json.{JsValue, Json}\n\n//key\nval k1: Key[MatrixData[Double]] = DoubleMatrixKey.make(\"myMatrix\")\n\nval name1  = EventName(\"correctionInfo\")\nval prefix = Prefix(\"aoesw.rpg\")\n\n//values\nval m1: MatrixData[Double] = MatrixData.fromArrays(\n  Array(1.0, 2.0, 3.0),\n  Array(4.1, 5.1, 6.1),\n  Array(7.2, 8.2, 9.2)\n)\n//parameter\nval i1: Parameter[MatrixData[Double]] = k1.set(m1)\n//events\nval observeEvent: ObserveEvent = ObserveEvent(prefix, name1).add(i1)\nval systemEvent: SystemEvent   = SystemEvent(prefix, name1).add(i1)\n\n//json support - write\nval observeJson: JsValue = JsonSupport.writeEvent(observeEvent)\nval systemJson: JsValue  = JsonSupport.writeEvent(systemEvent)\n\n//optionally prettify\nval str: String = Json.prettyPrint(systemJson)\n\n//construct command from string\nval systemEventFromPrettyStr: SystemEvent = JsonSupport.readEvent[SystemEvent](Json.parse(str))\n\n//json support - read\nval observeEvent1: ObserveEvent = JsonSupport.readEvent[ObserveEvent](observeJson)\nval systemEvent1: SystemEvent   = JsonSupport.readEvent[SystemEvent](systemJson) Java //key\nKey<MatrixData<Double>> k1 = JKeyType.DoubleMatrixKey().make(\"myMatrix\");\n\n//prefixes\nPrefix prefix1 = Prefix.apply(JSubsystem.AOESW, \"rpg\");\nEventName name1 = new EventName(\"correctionInfo\");\n\n//values\nDouble[][] doubles = {{1.0, 2.0, 3.0}, {4.1, 5.1, 6.1}, {7.2, 8.2, 9.2}};\nMatrixData<Double> m1 = MatrixData.fromArrays(doubles);\n\n//parameter\nParameter<MatrixData<Double>> i1 = k1.set(m1);\n\n\n//events\nObserveEvent observeEvent = new ObserveEvent(prefix1, name1).add(i1);\nSystemEvent systemEvent = new SystemEvent(prefix1, name1).add(i1);\n\n//json support - write\nJsValue observeJson = JavaJsonSupport.writeEvent(observeEvent);\nJsValue systemJson = JavaJsonSupport.writeEvent(systemEvent);\n\n//optionally prettify\nString str = Json.prettyPrint(systemJson);\n\n//construct DemandState from string\nSystemEvent statusFromPrettyStr = JavaJsonSupport.readEvent(Json.parse(str));\n\n//json support - read\nObserveEvent observeEvent1 = JavaJsonSupport.readEvent(observeJson);\nSystemEvent systemEvent1 = JavaJsonSupport.readEvent(systemJson);","title":"JSON Serialization"},{"location":"/params/events.html#unique-key-constraint","text":"By choice, a ParameterSet in either ObserveEvent or SystemEvent event will be optimized to store only unique keys. When using add or madd methods on events to add new parameters, if the parameter being added has a key which is already present in the paramSet, the already stored parameter will be replaced by the given parameter.\nNote If the Set is created by component developers and given directly while creating an event, then it will be the responsibility of component developers to maintain uniqueness with parameters based on key.\nHere are some examples that illustrate this point:\nScala //keys\nval encoderKey: Key[Int] = KeyType.IntKey.make(\"encoder\")\nval filterKey: Key[Int]  = KeyType.IntKey.make(\"filter\")\nval miscKey: Key[Int]    = KeyType.IntKey.make(\"misc\")\n\n//prefix\nval prefix = Prefix(\"wfos.blue.filter\")\n\nval name1 = EventName(\"filterWheel\")\n\n//params\nval encParam1 = encoderKey.set(1)\nval encParam2 = encoderKey.set(2)\n\nval encParam3    = encoderKey.set(3)\nval filterParam1 = filterKey.set(1)\nval filterParam2 = filterKey.set(2)\n\nval filterParam3 = filterKey.set(3)\n\nval miscParam1 = miscKey.set(100)\n//StatusEvent with duplicate key via constructor\nval systemEvent =\n  SystemEvent(prefix, name1, Set(encParam1, encParam2, encParam3, filterParam1, filterParam2, filterParam3))\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nval uniqueKeys1 = systemEvent.paramSet.toList.map(_.keyName)\n\n//try adding duplicate keys via add + madd\nval changedStatusEvent = systemEvent\n  .add(encParam3)\n  .madd(\n    filterParam1,\n    filterParam2,\n    filterParam3\n  )\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nval uniqueKeys2 = changedStatusEvent.paramSet.toList.map(_.keyName)\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nval finalStatusEvent = systemEvent.madd(Set(miscParam1, encParam1))\n//now contains encoderKey, filterKey, miscKey\nval uniqueKeys3 = finalStatusEvent.paramSet.toList.map(_.keyName) Java //keys\nKey<Integer> encoderKey = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<Integer> filterKey = JKeyType.IntKey().make(\"filter\");\nKey<Integer> miscKey = JKeyType.IntKey().make(\"misc\");\n\n//prefix\nPrefix prefix1 = Prefix.apply(JSubsystem.WFOS, \"blue.filter\");\nEventName name1 = new EventName(\"filterWheel\");\n\n//params\nParameter<Integer> encParam1 = encoderKey.set(1);\nParameter<Integer> encParam2 = encoderKey.set(2);\nParameter<Integer> encParam3 = encoderKey.set(3);\n\nParameter<Integer> filterParam1 = filterKey.set(1);\nParameter<Integer> filterParam2 = filterKey.set(2);\nParameter<Integer> filterParam3 = filterKey.set(3);\n\nParameter<Integer> miscParam1 = miscKey.set(100);\n\n//StatusEvent with duplicate key via madd\nSystemEvent event = new SystemEvent(prefix1, name1).madd(\n        encParam1,\n        encParam2,\n        encParam3,\n        filterParam1,\n        filterParam2,\n        filterParam3);\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nSet<String> uniqueKeys1 = event.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//try adding duplicate keys via add + madd\nSystemEvent changedEvent = event.add(encParam3).madd(filterParam1, filterParam2, filterParam3);\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nSet<String> uniqueKeys2 = changedEvent.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nSystemEvent finalEvent = changedEvent.madd(miscParam1, encParam1);\n//now contains encoderKey, filterKey, miscKey\nSet<String> uniqueKeys3 = finalEvent.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());","title":"Unique Key Constraint"},{"location":"/params/states.html","text":"","title":"State Variables"},{"location":"/params/states.html#state-variables","text":"State variables are used when an Assembly wants to track the status of a command sent to an HCD using a matcher. For more information, see Publishing State.\nA state represents some aspect of a component’s internal state. There are two types called CurrentState and DemandState. They both share the same structural features. All state variables have Prefix and ParameterSet.\nThe PubSub feature of the HCD provides CurrentState values to the PubSub subscriber.","title":"State Variables"},{"location":"/params/states.html#demandstate","text":"A state variable that indicates the demand or requested state.\nScala //prefix\nval prefix = Prefix(\"wfos.prog.cloudcover\")\n\n//key\nval charKey: Key[Char]       = KeyType.CharKey.make(\"charKey\")\nval intKey: Key[Int]         = KeyType.IntKey.make(\"intKey\")\nval booleanKey: Key[Boolean] = KeyType.BooleanKey.make(\"booleanKey\")\nval utcTimeKey: Key[UTCTime] = KeyType.UTCTimeKey.make(\"utcTimeKey\")\nval notUsedKey: Key[String]  = KeyType.StringKey.make(\"notUsed\")\n\n//parameters\nval charParam: Parameter[Char]       = charKey.set('A', 'B', 'C').withUnits(NoUnits)\nval intParam: Parameter[Int]         = intKey.set(1, 2, 3).withUnits(meter)\nval booleanParam: Parameter[Boolean] = booleanKey.set(true, false)\nval utcTime: Parameter[UTCTime]      = utcTimeKey.set(UTCTime.now())\n\n//create DemandState and use sequential add\nval ds1: DemandState = DemandState(prefix, StateName(\"testStateName\")).add(charParam).add(intParam)\n//create DemandState and add more than one Parameters using madd\nval ds2: DemandState = DemandState(prefix, StateName(\"testStateName\")).madd(intParam, booleanParam)\n//create DemandState using apply\nval ds3: DemandState = DemandState(prefix, StateName(\"testStateName\"), Set(utcTime))\n\n//access keys\nval charKeyExists: Boolean = ds1.exists(charKey) //true\n\n//access Parameters\nval p1: Option[Parameter[Int]] = ds1.get(intKey)\n\n//access values\nval v1: Array[Char]    = ds1(charKey).values\nval v2: Array[Boolean] = ds2.parameter(booleanKey).values\nval missingKeys: Set[String] = ds3.missingKeys(\n  charKey,\n  intKey,\n  booleanKey,\n  utcTimeKey,\n  notUsedKey\n)\n\n//remove keys\nval ds4: DemandState = ds3.remove(utcTimeKey)\n\n//update existing keys - set it back by an hour\nval ds5: DemandState = ds3.add(utcTimeKey.set(UTCTime(UTCTime.now().value.minusSeconds(3600))))\n Java //prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"prog.cloudcover\");\n\n//keys\nKey<Character> charKey = JKeyType.CharKey().make(\"charKey\");\nKey<Integer> intKey = JKeyType.IntKey().make(\"intKey\");\nKey<Boolean> booleanKey = JKeyType.BooleanKey().make(\"booleanKey\");\nKey<UTCTime> utcTimeKey = JKeyType.UTCTimeKey().make(\"utcTimeKey\");\nKey<String> notUsedKey = JKeyType.StringKey().make(\"notUsed\");\n\n\nObsId obsId = new ObsId(\"Obs001\");\n\n//parameters\nParameter<Character> charParam = charKey.set('A', 'B', 'C').withUnits(NoUnits);\nParameter<Integer> intParam = intKey.set(1, 2, 3).withUnits(meter);\nParameter<Boolean> booleanParam = booleanKey.set(true, false);\nParameter<UTCTime> timestamp = utcTimeKey.set(UTCTime.now());\n\n//create DemandState and use sequential add\nDemandState ds1 = new DemandState(prefix, new StateName(\"testStateName\")).add(charParam).add(intParam);\n//create DemandState and add more than one Parameters using madd\nDemandState ds2 = new DemandState(prefix, new StateName(\"testStateName\")).madd(intParam, booleanParam);\n//create DemandState using apply\nDemandState ds3 = new DemandState(prefix, new StateName(\"testStateName\")).add(timestamp);\n\n//access keys\nboolean charKeyExists = ds1.exists(charKey); //true\n\n//access Parameters\nOptional<Parameter<Integer>> p1 = ds1.jGet(intKey);\n\n//access values\nList<Character> v1 = ds1.jGet(charKey).orElseThrow().jValues();\nList<Boolean> v2 = ds2.parameter(booleanKey).jValues();\nSet<String> missingKeys = ds3.jMissingKeys(charKey,\n        intKey,\n        booleanKey,\n        utcTimeKey,\n        notUsedKey);\n\n//remove keys\nDemandState ds4 = ds3.remove(utcTimeKey);\n\n//update existing keys - set it back by an hour\nDemandState ds5 = ds3.add(utcTimeKey.set(new UTCTime(UTCTime.now().value().minusSeconds(3600))));","title":"DemandState"},{"location":"/params/states.html#currentstate","text":"A state variable that is published by a component that describes its internal state. Used by Assemblies to determine command completion in Command Service.\nScala //prefix\nval prefix = Prefix(\"wfos.prog.cloudcover\")\n\n//key\nval charKey    = KeyType.CharKey.make(\"charKey\")\nval intKey     = KeyType.IntKey.make(\"intKey\")\nval booleanKey = KeyType.BooleanKey.make(\"booleanKey\")\nval utcTimeKey = KeyType.UTCTimeKey.make(\"utcTimeKey\")\nval notUsedKey = KeyType.StringKey.make(\"notUsed\")\n\n//parameters\nval charParam    = charKey.set('A', 'B', 'C').withUnits(NoUnits)\nval intParam     = intKey.set(1, 2, 3).withUnits(meter)\nval booleanParam = booleanKey.set(true, false)\nval utcTime      = utcTimeKey.set(UTCTime.now)\n\n//create CurrentState and use sequential add\nval cs1 = CurrentState(prefix, StateName(\"testStateName\")).add(charParam).add(intParam)\n//create CurrentState and add more than one Parameters using madd\nval cs2 = CurrentState(prefix, StateName(\"testStateName\")).madd(intParam, booleanParam)\n//create CurrentState using apply\nval cs3 = CurrentState(prefix, StateName(\"testStateName\"), Set(utcTime))\n\n//access keys\nval charKeyExists = cs1.exists(charKey) //true\n\n//access Parameters\nval p1: Option[Parameter[Int]] = cs1.get(intKey)\n\n//access values\nval v1: Array[Char]    = cs1(charKey).values\nval v2: Array[Boolean] = cs2.parameter(booleanKey).values\nval missingKeys: Set[String] = cs3.missingKeys(\n  charKey,\n  intKey,\n  booleanKey,\n  utcTimeKey,\n  notUsedKey\n)\n\n//remove keys\nval cs4 = cs3.remove(utcTimeKey)\n\n//update existing keys - set it back by an hour\nval cs5 = cs3.add(utcTimeKey.set(UTCTime(UTCTime.now().value.minusSeconds(3600))))\n Java //prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"prog.cloudcover\");\n\n//keys\nKey<Character> charKey = JKeyType.CharKey().make(\"charKey\");\nKey<Integer> intKey = JKeyType.IntKey().make(\"intKey\");\nKey<Boolean> booleanKey = JKeyType.BooleanKey().make(\"booleanKey\");\nKey<UTCTime> timestampKey = JKeyType.UTCTimeKey().make(\"timestampKey\");\nKey<String> notUsedKey = JKeyType.StringKey().make(\"notUsed\");\n\n\nObsId obsId = new ObsId(\"Obs001\");\n\n//parameters\nParameter<Character> charParam = charKey.set('A', 'B', 'C').withUnits(NoUnits);\nParameter<Integer> intParam = intKey.set(1, 2, 3).withUnits(meter);\nParameter<Boolean> booleanParam = booleanKey.set(true, false);\nParameter<UTCTime> timestamp = timestampKey.set(UTCTime.now());\n\n//create CurrentState and use sequential add\nCurrentState cs1 = new CurrentState(prefix, new StateName(\"testStateName\")).add(charParam).add(intParam);\n//create CurrentState and add more than one Parameters using madd\nCurrentState cs2 = new CurrentState(prefix, new StateName(\"testStateName\")).madd(intParam, booleanParam);\n//create CurrentState using apply\nCurrentState cs3 = new CurrentState(prefix, new StateName(\"testStateName\")).add(timestamp);\n\n//access keys\nboolean charKeyExists = cs1.exists(charKey); //true\n\n//access Parameters\nOptional<Parameter<Integer>> p1 = cs1.jGet(intKey);\n\n//access values\nList<Character> v1 = cs1.jGet(charKey).orElseThrow().jValues();\nList<Boolean> v2 = cs2.parameter(booleanKey).jValues();\nSet<String> missingKeys = cs3.jMissingKeys(charKey,\n        intKey,\n        booleanKey,\n        timestampKey,\n        notUsedKey);\n\n//remove keys\nCurrentState cs4 = cs3.remove(timestampKey);\n\n//update existing keys - set it back by an hour\nCurrentState cs5 = cs3.add(timestampKey.set(new UTCTime(UTCTime.now().value().minusSeconds(3600))));","title":"CurrentState"},{"location":"/params/states.html#json-serialization","text":"State variables can be serialized to JSON. The library has provided JsonSupport helper class and methods to serialize DemandState and CurrentState.\nScala import play.api.libs.json.{JsValue, Json}\n\n//key\nval k1: Key[MatrixData[Double]] = DoubleMatrixKey.make(\"myMatrix\")\n//values\nval m1: MatrixData[Double] = MatrixData.fromArrays(\n  Array(1.0, 2.0, 3.0),\n  Array(4.1, 5.1, 6.1),\n  Array(7.2, 8.2, 9.2)\n)\n\n//parameter\nval p1: Parameter[MatrixData[Double]] = k1.set(m1)\n\n//state variables\nval ds: DemandState  = DemandState(Prefix(\"wfos.blue.filter\"), StateName(\"testStateName\")).add(p1)\nval cs: CurrentState = CurrentState(Prefix(\"wfos.blue.filter\"), StateName(\"testStateName\")).add(p1)\n\n//json support - write\nval dsJson: JsValue = JsonSupport.writeStateVariable(ds)\nval csJson: JsValue = JsonSupport.writeStateVariable(cs)\n\n//optionally prettify\nval str: String = Json.prettyPrint(dsJson)\n\n//construct command from string\nval scFromPrettyStr = JsonSupport.readStateVariable[DemandState](Json.parse(str))\n\n//json support - read\nval ds1: DemandState  = JsonSupport.readStateVariable[DemandState](dsJson)\nval cs1: CurrentState = JsonSupport.readStateVariable[CurrentState](csJson) Java //key\nKey<MatrixData<Double>> k1 = JKeyType.DoubleMatrixKey().make(\"myMatrix\");\n\n//values\nDouble[][] doubles = {{1.0, 2.0, 3.0}, {4.1, 5.1, 6.1}, {7.2, 8.2, 9.2}};\nMatrixData<Double> m1 = MatrixData.fromArrays(doubles);\n\n//parameter\nParameter<MatrixData<Double>> i1 = k1.set(m1);\n\n//state variables\nDemandState ds = new DemandState(Prefix.apply(JSubsystem.WFOS, \"blue.filter\"), new StateName(\"testStateName\")).add(i1);\nCurrentState cs = new CurrentState(Prefix.apply(JSubsystem.WFOS, \"blue.filter\"), new StateName(\"testStateName\")).add(i1);\n\n//json support - write\nJsValue dsJson = JavaJsonSupport.writeStateVariable(ds);\nJsValue csJson = JavaJsonSupport.writeStateVariable(cs);\n\n//optionally prettify\nString str = Json.prettyPrint(dsJson);\n\n//construct DemandState from string\nDemandState dsFromPrettyStr = JavaJsonSupport.readStateVariable(Json.parse(str));\n\n//json support - read\nDemandState ds1 = JavaJsonSupport.readStateVariable(dsJson);\nCurrentState cs1 = JavaJsonSupport.readStateVariable(csJson);","title":"JSON Serialization"},{"location":"/params/states.html#unique-key-constraint","text":"By design, a ParameterSet in either DemandState or CurrentState will be optimized to store only unique keys. When using add or madd methods on events to add new parameters, if the parameter being added has a key which is already present in the paramSet, the already stored parameter will be replaced by the given parameter.\nNote If the Set is created by component developers and given directly while creating an event, then it will be the responsibility of component developers to maintain uniqueness with parameters based on key.\nHere are some examples that illustrate this point:\nScala //keys\nval encoderKey: Key[Int] = KeyType.IntKey.make(\"encoder\")\nval filterKey: Key[Int]  = KeyType.IntKey.make(\"filter\")\nval miscKey: Key[Int]    = KeyType.IntKey.make(\"misc.\")\n\n//prefix\nval prefix = Prefix(\"wfos.blue.filter\")\n\n//params\nval encParam1 = encoderKey.set(1)\nval encParam2 = encoderKey.set(2)\nval encParam3 = encoderKey.set(3)\n\nval filterParam1 = filterKey.set(1)\nval filterParam2 = filterKey.set(2)\nval filterParam3 = filterKey.set(3)\n\nval miscParam1 = miscKey.set(100)\n\n//DemandState with duplicate key via constructor\nval statusEvent = DemandState(\n  prefix,\n  StateName(\"testStateName\"),\n  Set(encParam1, encParam2, encParam3, filterParam1, filterParam2, filterParam3)\n)\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nval uniqueKeys1 = statusEvent.paramSet.toList.map(_.keyName)\n\n//try adding duplicate keys via add + madd\nval changedStatusEvent = statusEvent\n  .add(encParam3)\n  .madd(\n    filterParam1,\n    filterParam2,\n    filterParam3\n  )\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nval uniqueKeys2 = changedStatusEvent.paramSet.toList.map(_.keyName)\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nval finalStatusEvent = statusEvent.madd(Set(miscParam1, encParam1))\n//now contains encoderKey, filterKey, miscKey\nval uniqueKeys3 = finalStatusEvent.paramSet.toList.map(_.keyName) Java //keys\nKey<Integer> encoderKey = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<Integer> filterKey = JKeyType.IntKey().make(\"filter\");\nKey<Integer> miscKey = JKeyType.IntKey().make(\"misc.\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"blue.filter\");\n\n//params\nParameter<Integer> encParam1 = encoderKey.set(1);\nParameter<Integer> encParam2 = encoderKey.set(2);\nParameter<Integer> encParam3 = encoderKey.set(3);\n\nParameter<Integer> filterParam1 = filterKey.set(1);\nParameter<Integer> filterParam2 = filterKey.set(2);\nParameter<Integer> filterParam3 = filterKey.set(3);\n\nParameter<Integer> miscParam1 = miscKey.set(100);\n\n//Demand state with duplicate key via madd\nDemandState state = new DemandState(prefix, new StateName(\"testStateName\")).madd(\n        encParam1,\n        encParam2,\n        encParam3,\n        filterParam1,\n        filterParam2,\n        filterParam3);\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nSet<String> uniqueKeys1 = state.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//try adding duplicate keys via add + madd\nDemandState changedState = state.add(encParam3).madd(filterParam1, filterParam2, filterParam3);\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nSet<String> uniqueKeys2 = changedState.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nDemandState finalState = changedState.madd(miscParam1, encParam1);\n//now contains encoderKey, filterKey, miscKey\nSet<String> uniqueKeys3 = finalState.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());","title":"Unique Key Constraint"},{"location":"/params/states.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source Code for Examples"},{"location":"/params/result.html","text":"","title":"Result"},{"location":"/params/result.html#result","text":"Components use Results to return results of a command in the form of a ParameterSet. Result is the value that is returned as an argument to the Completed SubmitResponse.\nCreating a Result Requires:\nPrefix Set[Parameter]\nScala //keys\nval k1: Key[Int]    = KeyType.IntKey.make(\"encoder\")\nval k2: Key[Int]    = KeyType.IntKey.make(\"windspeed\")\nval k3: Key[String] = KeyType.StringKey.make(\"filter\")\nval k4: Key[Int]    = KeyType.IntKey.make(\"notUsed\")\n\n//prefixes\nval prefix = Prefix(\"wfos.prog.cloudcover\")\n\n//parameters\nval p1: Parameter[Int]    = k1.set(22)\nval p2: Parameter[Int]    = k2.set(44)\nval p3: Parameter[String] = k3.set(\"A\", \"B\", \"C\", \"D\")\n\n//Create Result using madd\nval r1: Result = Result().madd(p1, p2)\n//Create Result using apply\nval r2: Result = Result(p1, p2)\n//Create Result and use add\nval r3: Result = Result().add(p1).add(p2).add(p3)\n\n//access keys\nval k1Exists: Boolean = r1.exists(k1) //true\n\n//access Parameters\nval p4: Option[Parameter[Int]] = r1.get(k1)\n\n//access values\nval v1: Array[Int] = r1(k1).values\nval v2: Array[Int] = r2.parameter(k2).values\n//k4 is missing\nval missingKeys: Set[String] = r3.missingKeys(k1, k2, k3, k4)\n\n//remove keys\nval r4: Result = r3.remove(k3) Java //keys\nKey<Integer> k1 = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<Integer> k2 = JKeyType.IntKey().make(\"windspeed\", JUnits.NoUnits);\nKey<String> k3 = JKeyType.StringKey().make(\"filter\", JUnits.NoUnits);\nKey<Integer> k4 = JKeyType.IntKey().make(\"notUsed\", JUnits.NoUnits);\n\n//parameters\nParameter<Integer> p1 = k1.set(22);\nParameter<Integer> p2 = k2.set(44);\nParameter<String> p3 = k3.set(\"A\", \"B\", \"C\", \"D\");\n\n//Create Result using madd\nResult r1 = new Result().madd(p1, p2);\n//Create Result in line\nResult r2 = new Result().madd(p1, p2);\n//Create Result and use madd, add\nResult r3 = new Result().madd(p1, p2).add(p3);\n\n//access keys\nboolean k1Exists = r1.exists(k1); //true\n\n//access Parameters\nOptional<Parameter<Integer>> p4 = r1.jGet(k1);\n\n//access values\nList<Integer> v1 = r1.jGet(k1).orElseThrow().jValues();\nList<Integer> v2 = r2.parameter(k2).jValues();\n\n//k4 is missing\nSet<String> missingKeys = r3.jMissingKeys(k1, k2, k3, k4);\n\n//remove keys\nResult r4 = r3.remove(k3);","title":"Result"},{"location":"/params/result.html#json-serialization","text":"State variables can be serialized to JSON. The library has provided JsonSupport helper class and methods to serialize DemandState and CurrentState.\nScala import play.api.libs.json.{JsValue, Json}\n\n//key\nval k1: Key[MatrixData[Double]] = DoubleMatrixKey.make(\"myMatrix\")\n//values\nval m1: MatrixData[Double] = MatrixData.fromArrays(\n  Array(1.0, 2.0, 3.0),\n  Array(4.1, 5.1, 6.1),\n  Array(7.2, 8.2, 9.2)\n)\n\n//prefixes\nval prefix = Prefix(\"wfos.prog.cloudcover\")\n\n//parameter\nval i1: Parameter[MatrixData[Double]] = k1.set(m1)\n\n//result\nval result: Result = Result().add(i1)\n\n//json support - write\nval resultJson: JsValue = JsonSupport.writeResult(result)\n\n//optionally prettify\nval str: String = Json.prettyPrint(resultJson)\n\n//construct result from string\nval scFromPrettyStr: Result = JsonSupport.readResult(Json.parse(str))\n\n//json support - read\nval result1: Result = JsonSupport.readResult(resultJson) Java //key\nKey<MatrixData<Double>> k1 = JKeyType.DoubleMatrixKey().make(\"myMatrix\", JUnits.NoUnits);\n\n//values\nDouble[][] doubles = {{1.0, 2.0, 3.0}, {4.1, 5.1, 6.1}, {7.2, 8.2, 9.2}};\nMatrixData<Double> m1 = MatrixData.fromArrays(doubles);\n\n//parameter\nParameter<MatrixData<Double>> i1 = k1.set(m1);\n\n//ObsId\nObsId obsId = new ObsId(\"Obs001\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"prog.cloudcover\");\n\n//result\nResult result = new Result().add(i1);\n\n//json support - write\nJsValue resultJson = JavaJsonSupport.writeResult(result);\n\n//optionally prettify\nString str = Json.prettyPrint(resultJson);\n\n//construct result from string\nResult result1 = JavaJsonSupport.readResult(Json.parse(str));","title":"JSON serialization"},{"location":"/params/result.html#unique-key-constraint","text":"By choice, a ParameterSet in Result will be optimized to store only unique keys. In other words, trying to store multiple keys with same name, will be automatically optimized by removing duplicates.\nNote Parameters are stored in a Set, which is an unordered collection of items. Hence, it’s not predictable whether first or last duplicate copy will be retained. Hence, cautiously avoid adding duplicate keys.\nHere are some examples that illustrate this point:\nScala //keys\nval encoderKey: Key[Int] = KeyType.IntKey.make(\"encoder\")\nval filterKey: Key[Int]  = KeyType.IntKey.make(\"filter\")\nval miscKey: Key[Int]    = KeyType.IntKey.make(\"misc.\")\n\n//prefix\nval prefix = Prefix(\"wfos.blue.filter\")\n\n//params\nval encParam1 = encoderKey.set(1)\nval encParam2 = encoderKey.set(2)\nval encParam3 = encoderKey.set(3)\n\nval filterParam1 = filterKey.set(1)\nval filterParam2 = filterKey.set(2)\nval filterParam3 = filterKey.set(3)\n\nval miscParam1 = miscKey.set(100)\n\n//Setup command with duplicate key via constructor\nval result = Result(encParam1, encParam2, encParam3, filterParam1, filterParam2, filterParam3)\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nval uniqueKeys1 = result.paramSet.toList.map(_.keyName)\n\n//try adding duplicate keys via add + madd\nval changedResult = result\n  .add(encParam3)\n  .madd(\n    filterParam1,\n    filterParam2,\n    filterParam3\n  )\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nval uniqueKeys2 = changedResult.paramSet.toList.map(_.keyName)\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nval finalResult = result.madd(Set(miscParam1, encParam1))\n//now contains encoderKey, filterKey, miscKey\nval uniqueKeys3 = finalResult.paramSet.toList.map(_.keyName) Java //keys\nKey<Integer> encoderKey = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nKey<Integer> filterKey = JKeyType.IntKey().make(\"filter\", JUnits.NoUnits);\nKey<Integer> miscKey = JKeyType.IntKey().make(\"misc.\", JUnits.NoUnits);\n\n//ObsId\nObsId obsId = new ObsId(\"Obs001\");\n\n//prefix\nPrefix prefix = Prefix.apply(JSubsystem.WFOS, \"blue.filter\");\n\n//params\nParameter<Integer> encParam1 = encoderKey.set(1);\nParameter<Integer> encParam2 = encoderKey.set(2);\nParameter<Integer> encParam3 = encoderKey.set(3);\n\nParameter<Integer> filterParam1 = filterKey.set(1);\nParameter<Integer> filterParam2 = filterKey.set(2);\nParameter<Integer> filterParam3 = filterKey.set(3);\n\nParameter<Integer> miscParam1 = miscKey.set(100);\n\n//Setup command with duplicate key via madd\nResult result = new Result().madd(encParam1, encParam2, encParam3, filterParam1, filterParam2, filterParam3);\n//four duplicate keys are removed; now contains one Encoder and one Filter key\nSet<String> uniqueKeys1 = result.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//try adding duplicate keys via add + madd\nResult changedResult = result.add(encParam3).madd(filterParam1, filterParam2, filterParam3);\n//duplicate keys will not be added. Should contain one Encoder and one Filter key\nSet<String> uniqueKeys2 = changedResult.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());\n\n//miscKey(unique) will be added; encoderKey(duplicate) will not be added\nResult finalResult = result.madd(miscParam1, encParam1);\n//now contains encoderKey, filterKey, miscKey\nSet<String> uniqueKeys3 = finalResult.jParamSet().stream().map(Parameter::keyName).collect(Collectors.toUnmodifiableSet());","title":"Unique Key Constraint"},{"location":"/params/result.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source Code for Examples"},{"location":"/commons/framework.html","text":"","title":"Framework for creating components (HCD, Assembly, Container)"},{"location":"/commons/framework.html#framework-for-creating-components-hcd-assembly-container-","text":"csw-framework library provides support for creating a component as defined by TMT.","title":"Framework for creating components (HCD, Assembly, Container)"},{"location":"/commons/framework.html#dependencies","text":"sbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-framework\" % \"2.0.0-RC2\"\nTechnical Description Source code for examples ComponentInfo Creating an Assembly or Hcd Component Component Handlers Component Lifecycle Shutting Down Component Online and Offline Handling commands Handling Diagnostic Data Managing Command State Updating a Long-running Command Using the CRM with Subcommands Tracking Connections onLocationTrackingEvent Publishing State Handling Exceptions Deploying Components ContainerCmd Container for deployment Standalone components","title":"Dependencies"},{"location":"/commons/framework.html#technical-description","text":"See CSW Framework Technical Description.","title":"Technical Description"},{"location":"/commons/framework.html#source-code-for-examples","text":"Assembly Scala Example HCD Scala Example Assembly Java Example HCD Java Example","title":"Source code for examples"},{"location":"/framework/describing-components.html","text":"","title":"ComponentInfo"},{"location":"/framework/describing-components.html#componentinfo","text":"The ComponentInfo model describes a component by specifying several details. It is usually described as a configuration file called the “Component Info File” but can also be created programmatically.\nAssemblyInfo componentType = assembly\nbehaviorFactoryClassName = package.component.SampleAssembly\nprefix = ESW.Sample_Assembly\nlocationServiceUsage = RegisterAndTrackServices\nconnections = [\n    {\n      prefix: \"ESW.Sample_HCD\"\n      componentType: assembly\n      connectionType: akka\n    }\n  ]\n HcdInfo componentType = hcd\nbehaviorFactoryClassName = package.component.SampleHcd\nprefix = ESW.Sample_Hcd\nlocationServiceUsage = RegisterOnly\nFollowing is the summary of properties in the ComponentInfo config/model:\nprefix : A prefix is a valid subsystem to which this component belongs combined with the component’s name. componentType : The type of the component which could be Container, Assembly, Hcd or Service behaviorFactoryClassName : The fully qualified name of the class which extends the factory class ComponentBehaviorFactory (or JComponentBehaviorFactory, if written in Java) connections : A collection of connections of components or services which will be used by this component. This information can be used in accordance with the locationServiceUsage property to automatically track these components or services by the framework. locationServiceUsage : Indicates how the Location Service should be leveraged for this component by the framework. The following values are supported: DoNotRegister : Do not register this component with the Location Service RegisterOnly : Register this component with the Location Service RegisterAndTrackServices : Register this component with the Location Service as well as track the components/services listed in the connections property initializeTimeout (Optional) : An optional parameter that specifies the timeout time for the initialization phase of the component lifecycle (see Component Handlers). This is specified using a whole number and then units (e.g. “5 seconds”), where the units can be seconds or millis for milliseconds.","title":"ComponentInfo"},{"location":"/framework/creating-components.html","text":"","title":"Creating an Assembly or Hcd Component"},{"location":"/framework/creating-components.html#creating-an-assembly-or-hcd-component","text":"An Assembly or HCD is implemented by extending the ComponentHandlers abstract class. These handlers are executed by an Akka Actor (Top Level Actor or TLA) defined in the framework which handles the lifecycle and supervision of each component.\nAssembly/Scala class AssemblyComponentHandlers(ctx: ActorContext[TopLevelActorMessage], cswCtx: CswContext)\n    extends ComponentHandlers(ctx, cswCtx) Assembly/Java public class JAssemblyComponentHandlers extends JComponentHandlers {\n\n    private final ActorContext<TopLevelActorMessage> ctx;\n    private final ComponentInfo componentInfo;\n    private final CurrentStatePublisher currentStatePublisher;\n    private final CommandResponseManager commandResponseManager;\n    private final ILocationService locationService;\n    private final IEventService eventService;\n    private ILogger log;\n    private IConfigClientService configClient;\n    private Map<Connection, Optional<ICommandService>> runningHcds;\n    private ActorRef<DiagnosticPublisherMessages> diagnosticPublisher;\n    private ActorRef<CommandResponse.SubmitResponse> commandResponseAdapter;\n\n    public JAssemblyComponentHandlers(akka.actor.typed.javadsl.ActorContext<TopLevelActorMessage> ctx, JCswContext cswCtx) {\n        super(ctx, cswCtx);\n        this.ctx = ctx;\n        this.componentInfo = cswCtx.componentInfo();\n        this.commandResponseManager = cswCtx.commandResponseManager();\n        this.currentStatePublisher = cswCtx.currentStatePublisher();\n        this.locationService = cswCtx.locationService();\n        this.eventService = cswCtx.eventService();\n        log = cswCtx.loggerFactory().getLogger(this.getClass());\n        configClient = JConfigClientFactory.clientApi(ctx.getSystem(), locationService);\n\n        runningHcds = new HashMap<>();\n        commandResponseAdapter = TestProbe.<CommandResponse.SubmitResponse>create(ctx.getSystem()).ref();\n        commandResponseAdapter = TestProbe.<CommandResponse.SubmitResponse>create(ctx.getSystem()).ref();\n    } Hcd/Scala class HcdComponentHandlers(ctx: ActorContext[TopLevelActorMessage], cswCtx: CswContext)\n    extends ComponentHandlers(ctx, cswCtx) Hcd/Java public class JHcdComponentHandlers extends JComponentHandlers {\n\n    private final ActorContext<TopLevelActorMessage> ctx;\n    private final ComponentInfo componentInfo;\n    private final CommandResponseManager commandResponseManager;\n    private final CurrentStatePublisher currentStatePublisher;\n    private final ILocationService locationService;\n    private final IEventService eventService;\n    private ILogger log;\n    private IConfigClientService configClient;\n    private ConfigData hcdConfig;\n    private ActorRef<WorkerActorMsg> worker;\n    private int current;\n    private int stats;\n\n    public JHcdComponentHandlers(\n            akka.actor.typed.javadsl.ActorContext<TopLevelActorMessage> ctx,\n            JCswContext cswCtx\n    ) {\n        super(ctx, cswCtx);\n        this.ctx = ctx;\n        this.componentInfo = cswCtx.componentInfo();\n        this.commandResponseManager = cswCtx.commandResponseManager();\n        this.currentStatePublisher = cswCtx.currentStatePublisher();\n        this.locationService = cswCtx.locationService();\n        this.eventService = cswCtx.eventService();\n        log = cswCtx.loggerFactory().getLogger(this.getClass());\n    }\nNote Converting a typed actor system to an untyped actor system The ctx available to the component is of type akka.actor.typed.scaladsl.ActorContext in Scala or akka.actor.typed.javadsl.ActorContext in Java. This context can be used to get resources such as an actor system which is also typed. In order to get the untyped version of an actor system or actor references, Akka has provided some implicit extension methods in Scala and static methods in Java which can be used by adding the following import: Scala: import akka.actor.typed.scaladsl.adapter._ Java: import akka.actor.typed.javadsl.Adapter.*\nA component can be created by a factory which extends the ComponentBehaviorFactory base class and provides a definition of the handlers method to return the appropriate implementation of ComponentHandlers.\nAssembly/Scala class AssemblyComponentBehaviorFactory extends ComponentBehaviorFactory {\n  protected override def handlers(ctx: ActorContext[TopLevelActorMessage], cswCtx: CswContext): ComponentHandlers =\n    new AssemblyComponentHandlers(ctx, cswCtx)\n} Assembly/Java public class JAssemblyComponentBehaviorFactory extends JComponentBehaviorFactory {\n    @Override\n    public JComponentHandlers jHandlers(ActorContext<TopLevelActorMessage> ctx, JCswContext cswCtx) {\n        return new JAssemblyComponentHandlers(ctx, cswCtx);\n    }\n} Hcd/Scala class HcdComponentBehaviorFactory extends ComponentBehaviorFactory {\n  protected override def handlers(ctx: ActorContext[TopLevelActorMessage], cswCtx: CswContext): ComponentHandlers =\n    new HcdComponentHandlers(ctx, cswCtx)\n} Hcd/Java public class JHcdComponentBehaviorFactory extends JComponentBehaviorFactory {\n    @Override\n    public JComponentHandlers jHandlers(ActorContext<TopLevelActorMessage> ctx, JCswContext cswCtx) {\n        return new JHcdComponentHandlers(ctx, cswCtx);\n    }\n}","title":"Creating an Assembly or Hcd Component"},{"location":"/framework/handling-lifecycle.html","text":"","title":"Component Handlers"},{"location":"/framework/handling-lifecycle.html#component-handlers","text":"A component developer creates a Top Level Actor (TLA) by inheriting from an abstract class ComponentHandlers or JComponentHandlers for Scala or Java, respectively. Each of these abstract classes provides several handler methods that can be overridden by the developer to provide component-specific code as described below.","title":"Component Handlers"},{"location":"/framework/handling-lifecycle.html#component-lifecycle","text":"For each component, the CSW framework creates a Supervisor that creates the TLA, and along with the abstract behavior class provided by the framework, it starts up and initializes the component in a standardized way. At the conclusion of the startup of the component, it is ready to receive commands from the outside world. The following figure is used to describe the startup lifecycle interactions between the framework and the TLA.","title":"Component Lifecycle"},{"location":"/framework/handling-lifecycle.html#initialize","text":"As described in Creating a Component, a Supervisor is created based on the contents of the ComponentInfo file. The figure shows that the Supervisor in the framework creates the specified TLA. Once the TLA is created, the framework calls the initialize handler. This is the opportunity for the component to perform any initialization needed before it is ready to receive commands.\nThe implementation of the initialize handler is up to the developer. A common task will be for the component to fetch a configuration from the Configuration Service. It may also determine the location of components or services it needs from the Location Service.\nThe TLA indicates a successful initialize by returning normally. If it cannot initialize, the handler should throw an exception, which will be caught and logged. The Supervisor will retry the creation and initialization of the TLA three times. If it fails after three times, the Supervisor will log a message and stop.\nWhen initialize succeeds, the Supervisor in the framework and the component itself enter the Running state. When in the Running state, commands received from outside the component are passed to the TLA (see below).\nAssembly/Scala override def initialize(): Future[Unit] = async {\n\n  // Initialization could include following steps :\n\n  // 1. fetch config (preferably from configuration service)\n  val calculationConfig = await(getAssemblyConfig)\n\n  // 2. create a worker actor which is used by this assembly\n  val worker: ActorRef[WorkerActorMsg] = ctx.spawnAnonymous(WorkerActor.behavior(calculationConfig))\n\n  // 3. find a Hcd connection from the connections provided in componentInfo\n  val maybeConnection =\n    componentInfo.connections.find(connection => connection.componentId.componentType == ComponentType.HCD)\n\n  // 4. If an Hcd is found as a connection, resolve its location from location service and create other\n  // required worker actors required by this assembly\n\n  maybeConnection match {\n    case Some(_) =>\n      resolveHcd().map {\n        case Some(hcd) =>\n          runningHcds = runningHcds.updated(maybeConnection.get, Some(CommandServiceFactory.make(hcd)(ctx.system)))\n          diagnosticsPublisher = ctx.spawnAnonymous(DiagnosticsPublisher.behavior(runningHcds(maybeConnection.get).get, worker))\n          commandHandler = ctx.spawnAnonymous(CommandHandler.behavior(calculationConfig, runningHcds(maybeConnection.get)))\n        case None => // do something\n      }\n    case None => Future.successful(())\n  }\n\n} Assembly/Java @Override\npublic CompletableFuture<Void> jInitialize() {\n\n    // fetch config (preferably from configuration service)\n    CompletableFuture<ConfigData> configDataCompletableFuture = getAssemblyConfig();\n\n    // create a worker actor which is used by this assembly\n    CompletableFuture<ActorRef<WorkerActorMsg>> worker =\n            configDataCompletableFuture.thenApply(config -> ctx.spawnAnonymous(WorkerActor.behavior(config)));\n\n    // find a Hcd connection from the connections provided in componentInfo\n    Optional<Connection> mayBeConnection = componentInfo.getConnections().stream()\n            .filter(connection -> connection.componentId().componentType() == JComponentType.HCD)\n            .findFirst();\n\n    // If an Hcd is found as a connection, resolve its location from location service and create other\n    // required worker actors required by this assembly, also subscribe to HCD's filter wheel event stream\n    return mayBeConnection.map(connection ->\n            worker.thenAcceptBoth(resolveHcd(), (workerActor, hcdLocation) -> {\n                if (!hcdLocation.isPresent())\n                    throw new HcdNotFoundException();\n                else {\n                    runningHcds.put(connection, Optional.of(CommandServiceFactory.jMake(hcdLocation.orElseThrow(), ctx.getSystem())));\n                }\n                diagnosticPublisher = ctx.spawnAnonymous(JDiagnosticsPublisher.behavior(CommandServiceFactory.jMake(hcdLocation.orElseThrow(), ctx.getSystem()), workerActor));\n            })).orElseThrow();\n\n} Hcd/Scala override def initialize(): Future[Unit] = async {\n\n  // fetch config (preferably from configuration service)\n  val hcdConfig = await(getHcdConfig)\n\n  // create a worker actor which is used by this hcd\n  val worker: ActorRef[WorkerActorMsg] = ctx.spawnAnonymous(WorkerActor.behavior(hcdConfig))\n\n  // initialise some state by using the worker actor created above\n  current = await(worker ? InitialState)\n  stats = await(worker ? GetStatistics)\n\n} Hcd/Java @Override\npublic CompletableFuture<Void> jInitialize() {\n\n    // fetch config (preferably from configuration service)\n    getConfig().thenAccept(config -> hcdConfig = config);\n\n    // create a worker actor which is used by this hcd\n    worker = ctx.spawnAnonymous(WorkerActor.behavior(hcdConfig));\n\n    // initialise some state by using the worker actor created above\n    CompletionStage<Integer> askCurrent = AskPattern.ask(worker, WorkerActorMsgs.JInitialState::new, Duration.ofSeconds(5), ctx.getSystem().scheduler());\n    CompletableFuture<Void> currentFuture = askCurrent.thenAccept(c -> current = c).toCompletableFuture();\n\n    CompletionStage<Integer> askStats = AskPattern.ask(worker, WorkerActorMsgs.JInitialState::new, Duration.ofSeconds(5), ctx.getSystem().scheduler());\n    CompletableFuture<Void> statsFuture = askStats.thenAccept(s -> stats = s).toCompletableFuture();\n\n    return CompletableFuture.allOf(currentFuture, statsFuture);\n}","title":"initialize"},{"location":"/framework/handling-lifecycle.html#creation-timeout","text":"The Supervisor waits for the initialize to complete. If it times out, it will retry the creation of the TLA 3 times in the same way as with initialize failures. The timeout value is configurable by the TLA by setting the initializeTimeout value in ComponentInfo.","title":"Creation Timeout"},{"location":"/framework/handling-lifecycle.html#location-service-interactions","text":"Once the Supervisor and TLA are in the Running state, the Supervisor registers the component with the Location Service. This allows the component to be located so it can be contacted. Registration with Location Service happens only if locationServiceUsage in ComponentInfo is not set to DoNotRegister.\nIf the component has connections and locationServiceUsage in ComponentInfo is set to RegisterAndTrackServices, the framework will resolve the components and deliver TrackingEvents to the TLA through the onTrackingEvent onTrackingEvent handler.","title":"Location Service Interactions"},{"location":"/framework/handling-lifecycle.html#shutting-down","text":"A component may be shutdown by an external administrative program whether it is deployed in a container or standalone. Shutting down may occur when the component is in the Running state, either online or offline (see below).","title":"Shutting Down"},{"location":"/framework/handling-lifecycle.html#onshutdown","text":"The TLA provides a handler called onShutdown that is called by the Supervisor when shutting down to give the TLA an opportunity to perform any clean up it may require, such as freeing resources.\nAs with initialize, there is a timeout that the framework will wait for the component to return from onShutdown. This is currently set to 10 seconds and cannot be overridden. If it does not return, it is assumed that the TLA is damaged and the TLA is destroyed immediately. After a successful return from onShutdown, the Supervisor deletes the component.\nAssembly/Scala override def onShutdown(): Future[Unit] = async {\n  // clean up resources\n} Assembly/Java @Override\npublic CompletableFuture<Void> jOnShutdown() {\n    // clean up resources\n    return new CompletableFuture<>();\n} Hcd/Scala override def onShutdown(): Future[Unit] = async {\n  // clean up resources\n} Hcd/Java @Override\npublic CompletableFuture<Void> jOnShutdown() {\n    return CompletableFuture.runAsync(() -> {\n        // clean up resources\n    });\n}","title":"onShutdown"},{"location":"/framework/handling-lifecycle.html#restarting","text":"A component may be restarted by an external administrative program whether it is deployed in a container or standalone. A restart may occur when the component is in the Running state, either online or offline (see below).\nA restart causes the component to be destroyed and re-created with a new TLA. The onShutdown handler is called to allow the component to tidy up before it is destroyed. Then the Supervisor creates a new TLA and the startup proceeds as with initialize above.","title":"Restarting"},{"location":"/framework/handling-lifecycle.html#component-online-and-offline","text":"Online describes a component that is currently part of the observing system that is in use. When a component enters the Running state it is also “online”.\nA component is offline when it is operating and available for active observing but is not currently in use.\nIf a component is to transition from the online state to the offline state, the onGoOffLine handler is called. The component should make any changes in its operation for offline use.\nIf a component is to transition from the offline state to the online state, the onGoOnline handler is called. The component should make any changes in its operation needed for online use.\nOnline vs Offline Unless implemented by the developer, there is no fundamental difference in the inherent behavior of a component when in either state. These two states provide a standard way for code to be implemented via these handlers for the transition from one state to another, allowing the component to prepare itself to be online (ready for operations) or offline (stowed or dormant). Any call to transition to a online/offline state when the component is already in that state is a no op. However, when offline, a component should take actions that make sense when offline. For instance, it should not follow the telescope motion or take actions as if online.","title":"Component Online and Offline"},{"location":"/framework/handling-lifecycle.html#isonline","text":"A component has access to the isOnline boolean flag, which can be used to determine if the component is in the online or offline state.","title":"isOnline"},{"location":"/framework/handling-lifecycle.html#ongooffline","text":"A component can be notified to run in offline mode in case it is not in use. The component can change its behavior if needed as a part of this handler.\nAssembly/Scala override def onGoOffline(): Unit = {\n  // do something when going offline\n} Assembly/Java @Override\npublic void onGoOffline() {\n    // do something when going offline\n} Hcd/Scala override def onGoOffline(): Unit = {\n  // do something when going offline\n} Hcd/Java @Override\npublic void onGoOffline() {\n    // do something when going offline\n}","title":"onGoOffline"},{"location":"/framework/handling-lifecycle.html#ongoonline","text":"A component can be notified to run in online mode again in case it was put to run in offline mode. The component can change its behavior if needed as a part of this handler.\nAssembly/Scala override def onGoOnline(): Unit = {\n  // do something when going online\n} Assembly/Java @Override\npublic void onGoOnline() {\n    // do something when going online\n} Hcd/Scala override def onGoOnline(): Unit = {\n  // do something when going online\n} Hcd/Java @Override\npublic void onGoOnline() {\n    // do something when going online\n}","title":"onGoOnline"},{"location":"/framework/handling-lifecycle.html#handling-commands","text":"The remaining handlers are associated with handling incoming commands. There is a handler for submit commands called onSubmit and a handler for oneway called onOneway.\nThis section gives an introduction to the command handlers. For more information on how to send and monitor commands, see the Communication using Commands page.","title":"Handling commands"},{"location":"/framework/handling-lifecycle.html#validatecommand","text":"The validateCommand handler allows the component to inspect a command and its parameters to determine if the actions related to the command can be executed or started. If it is okay, an Accepted response is returned. If not, Invalid is returned. Validation may also take into consideration the state of the component. For instance, if an Assembly or HCD can only handle one command at a time, validateCommand should return an return Invalid if a second command is received. The Invalid returns a CommandIssue, which are a number of pre-defined reasons for failing validation. These pre-defined reasons should be used whenever possible, but there is also an OtherIssue defined.\nI have an issue you should add! If you run across a validation issue you think should be added to CommandIssue, please submit a ticket to the CSW maintenance JIRA page at this location.\nThe handler is called whenever a command is sent as a Submit, SubmitAndWait, or Oneway message to the component. If the handler returns Accepted, the corresponding onSubmit or onOneway handler is called. This handler can also be called when the Command Service method validateCommand is used, to preview the acceptance of a command before it is sent using submit or oneway. In this case, the onSubmit or onOneway handler is not called.\nAssembly/Scala override def validateCommand(runId: Id, command: ControlCommand): ValidateCommandResponse =\n  command match {\n    case setup: Setup =>\n      setup.commandName match {\n        case `sleep` =>\n          validateSleep(runId, setup)\n        case `immediateCommand` | `shortCommand` | `mediumCommand` | `longCommand` | `complexCommand` =>\n          Accepted(runId)\n        case _ =>\n          Invalid(runId, UnsupportedCommandIssue(s\"Command: ${setup.commandName.name} is not supported for sample Assembly.\"))\n      }\n    case _ =>\n      Invalid(runId, UnsupportedCommandIssue(s\"Command: ${command.commandName.name} is not supported for sample Assembly.\"))\n  }\n\nprivate def validateSleep(runId: Id, setup: Setup): ValidateCommandResponse =\n  if (setup.exists(sleepTimeKey)) {\n    val sleepTime: Long = setup(sleepTimeKey).head\n    if (sleepTime < maxSleep)\n      Accepted(runId)\n    else\n      Invalid(runId, ParameterValueOutOfRangeIssue(\"sleepTime must be < 2000\"))\n  }\n  else {\n    Invalid(runId, MissingKeyIssue(s\"required sleep command key: $sleepTimeKey is missing.\"))\n  } Assembly/Java @Override\npublic ValidateCommandResponse validateCommand(Id runId, ControlCommand command) {\n  CommandName cmd = command.commandName();\n  if (command instanceof Setup) {\n    Setup setup = (Setup) command;\n    if (cmd.equals(sleep)) {\n      return validateSleep(runId, setup);\n    }\n    if (cmd.equals(immediateCommand) || cmd.equals(shortCommand) || cmd.equals(mediumCommand) || cmd.equals(longCommand)|| cmd.equals(complexCommand)) {\n      return new Accepted(runId);\n    }\n  }\n  return new Invalid(runId, new UnsupportedCommandIssue(\"Command: \" + cmd.name() + \" is not supported for sample Assembly.\"));\n}\n\nprivate ValidateCommandResponse validateSleep(Id runId, Setup setup) {\n  if (setup.exists(sleepTimeKey)) {\n    Long sleepTime = setup.jGet(sleepTimeKey).get().head();\n    if (sleepTime < maxSleep)\n      return new Accepted(runId);\n    else\n      return new Invalid(runId, new ParameterValueOutOfRangeIssue(\"sleepTime must be < 2000\"));\n  } else {\n    return new Invalid(runId, new MissingKeyIssue(\"required sleep command key: \" + sleepTimeKey + \" is missing.\"));\n  }\n} Hcd/Scala override def validateCommand(runId: Id, command: ControlCommand): ValidateCommandResponse =\n  command.commandName match {\n    case `hcdSleep` | `hcdImmediate` =>\n      Accepted(runId)\n    case _ =>\n      log.error(s\"HCD: $prefix received an unsupported command: ${command.commandName.name}\")\n      Invalid(runId, UnsupportedCommandIssue(s\"Command: ${command.commandName.name} is not supported for HCD: $prefix.\"))\n  } Hcd/Java @Override\npublic ValidateCommandResponse validateCommand(Id runId, ControlCommand command) {\n  if (command.commandName().equals(hcdSleep) || command.commandName().equals(hcdImmediate)) {\n    return new Accepted(runId);\n  }\n  log.error(\"HCD: \" + prefix + \" received an unsupported command: \" + command.commandName().name());\n  return new Invalid(runId, new UnsupportedCommandIssue(\"Command \" + command.commandName().name() + \" is not supported fpr HCD: \" + prefix + \".\"));\n}","title":"validateCommand"},{"location":"/framework/handling-lifecycle.html#onsubmit","text":"On receiving a command sent using a submit or submitAndWait message, the onSubmit handler is invoked only if the validateCommand handler returns Accepted. The onSubmit handler returns a SubmitResponse indicating if the command is completed immediately, or if it is long-running, by returning a Started response. Completion of long-running commands is tracked using the CommandResponseManager, described in more detail in the Managing Command State page.\nThe example shows one way to split Setup and Observe commands into separate handlers. In this case, the Assembly does not support Observe and returns Invalid with the UnsupportedCommandIssue.\nAssembly/Scala override def onSubmit(runId: Id, command: ControlCommand): SubmitResponse =\n  command match {\n    case s: Setup => onSetup(runId, s)\n    case _: Observe =>\n      Invalid(runId, UnsupportedCommandIssue(\"Observe commands not supported\"))\n  } Assembly/Java @Override\npublic SubmitResponse onSubmit(Id runId, ControlCommand command) {\n  if (command instanceof Setup) {\n    return onSetup(runId, (Setup) command);\n  }\n  return new Invalid(runId, new UnsupportedCommandIssue(\"Observe commands not supported\"));\n}\nIdeally, in the above example, lack of support for Observe should be determined in the validateCommand handler and checking in onSetup should not be needed.\nInvalid or Error? The Invalid response should be used when an issue is found with validation of the command or other issues prior to starting actions. Error is reserved for issues with the actions started by the command.","title":"onSubmit"},{"location":"/framework/handling-lifecycle.html#ononeway","text":"On receiving a command as oneway, the onOneway handler is invoked for a component only if the validateCommand handler returns Accepted. The onOneway handler does not return a value and a command submitted with the oneway does not track completion of actions.\nAssembly/Scala override def onOneway(runId: Id, controlCommand: ControlCommand): Unit = controlCommand match {\n  case setup: Setup     => onewaySetup(runId, setup)     // includes logic to handle Oneway with Setup config command\n  case observe: Observe => onewayObserve(runId, observe) // includes logic to handle Oneway with Observe config command\n} Assembly/Java @Override\npublic void onOneway(Id runId, ControlCommand controlCommand) {\n    if (controlCommand instanceof Setup)\n        onewaySetup(runId, (Setup) controlCommand); // includes logic to handle Oneway with Setup config command\n    else if (controlCommand instanceof Observe)\n        onewayObserve(runId, (Observe) controlCommand); // includes logic to handle Oneway with Observe config command\n}","title":"onOneway"},{"location":"/framework/handling-lifecycle.html#handling-diagnostic-data","text":"","title":"Handling Diagnostic Data"},{"location":"/framework/handling-lifecycle.html#ondiagnosticmode","text":"A Component can receive a DiagnosticMode command from other components at any time. The onDiagnosticMode handler of the component is invoked on receiving the command. Inside the handler, the components can start publishing some diagnostic data as desired. The DiagnosticMode command can be received by a component only in the Running state and is ignored otherwise. The diagnosticMode handler contains a startTime which is a UTCTime and a String parameter called hint with the name of the technical data mode. The component should read the hint and publish events accordingly.\nThe startTime is included so a diagnosticMode can be synchronized in time with diagnosticMode starting in other components. The Time Service can be used to schedule execution of some tasks at the specified startTime. Event Service publish API can also be used in order to start publishing events at the specified startTime. A component can only be in one technical data mode at a time. If the component is in one technical data, then on receiving command to go in another technical mode, the component should stop/halt the previous diagnosticMode handler, and should enter the new technical data mode. Even if the component does not define any diagnostic modes, it must be prepared to receive and process diagnosticMode handler without an error by completing with no changes. This is equivalent to leaving the handler empty.\nNote A component developer should be careful to make any changes in the component’s internal state in any callbacks. For example, while calling timeServiceScheduler.schedule or eventPublisher.publish, if you are trying to mutate state in the callbacks passed to them, you might run into concurrency issues. Hence, in such scenarios, it is recommended to use a WorkerActor to manage the state.\nThe supported diagnostic mode hints of a component are published in the component’s model files. Unsupported hints should be rejected by a component.\nThe example shows one usage of onDiagnosticMode handler.\nScala // While dealing with mutable state, make sure you create a worker actor to avoid concurrency issues\n// For functionality demonstration, we have simply used a mutable variable without worker actor\nvar diagModeCancellable: Option[Cancellable] = None\n\noverride def onDiagnosticMode(startTime: UTCTime, hint: String): Unit = {\n  hint match {\n    case \"engineering\" =>\n      val event = SystemEvent(prefix, diagnosticDataEventName).add(diagnosticModeParam)\n      diagModeCancellable.foreach(_.cancel()) // cancel previous diagnostic publishing\n      diagModeCancellable = Some(eventService.defaultPublisher.publish(Some(event), startTime, 200.millis))\n    case _ =>\n  }\n} Java @Override\npublic void onDiagnosticMode(UTCTime startTime, String hint) {\n    if (hint.equals(\"engineering\")) {\n        var event = new SystemEvent(Prefix.apply(JSubsystem.TCS, \"prefix\"), new EventName(\"eventName\"))\n                .add(JKeyType.IntKey().make(\"diagnostic-data\", JUnits.NoUnits).set(1));\n        diagModeCancellable.map(Cancellable::cancel); // cancel previous diagnostic publishing\n        diagModeCancellable = Optional.of(\n                eventService.defaultPublisher().publish(\n                        () -> Optional.of(event),\n                        startTime,\n                        Duration.ofMillis(200)\n                )\n\n        );\n    }\n    // other supported diagnostic modes go here\n}","title":"onDiagnosticMode"},{"location":"/framework/handling-lifecycle.html#onoperationsmode","text":"Components can receive an OperationsMode command which is used to halt all diagnostic modes. The onOperationsMode handler of the component will be invoked on receiving this command. Similar to DiagnosticMode command, the OperationsMode command is also handled only in the Running state and is ignored otherwise. If in a technical data mode, the component should immediately halt its diagnostic mode and return to normal operations behavior. Even if the component does not define any diagnostic modes, it must be prepared to receive and process an onOperationsMode handler call. The component should return completion without error.\nThe example shows one usage of onOperationsMode handler.\nScala override def onOperationsMode(): Unit = {\n  diagModeCancellable.foreach(_.cancel())\n} Java @Override\npublic void onOperationsMode() {\n    diagModeCancellable.map(Cancellable::cancel); // cancel diagnostic mode\n}","title":"onOperationsMode"},{"location":"/framework/managing-command-state.html","text":"","title":"Managing Command State"},{"location":"/framework/managing-command-state.html#managing-command-state","text":"A component is provided with a commandResponseManager which is used to update the state of commands that start long-running actions. A long-running command is one that starts actions that take longer than 1 second.\nThe CommandResponseManager (CRM) is used to provide the final SubmitResponse in the following two scenarios. These scenarios require the developer to update the CRM.\nThe onSubmit handler returns Started indicating a long-running command that requires notification of completion at a later time. To process an onSubmit that starts long-running actions, the component needs to send one or more commands to other components that may also take time to complete.\nOn receiving a command as a part of onSubmit, and if the onSubmit handler returns Started, the framework adds the command to an internal CommandResponseManager that keeps track of the command and the sender of the command. The sender is then sent the final SubmitResponse when CRM updateCommand is called.","title":"Managing Command State"},{"location":"/framework/managing-command-state.html#updating-a-long-running-command","text":"In the first scenario, the developer has a long-running command. In this case, once the actions are completed, updateCommand is used to notify the CRM that the actions are complete. This will cause the original sender to be notified of completion using the SubmitResponse passed to updateCommand.","title":"Updating a Long-running Command"},{"location":"/framework/managing-command-state.html#using-updatecommand","text":"updateCommand is used to update the status of a Started command. The following example from the SampleAssembly shows the Assembly sends a command to SampleHcd. It then does a queryFinal and when it returns, it updates the parent runId with the response received from the HCD. The onSubmit handler (not shown) already has returned Started to the sender of the original command, and the asynchronous completion is used to update the parent command.\nScala private def sleepHCD(runId: Id, setup: Setup, sleepTime: Long): Unit =\n  hcdCS match {\n    case Some(cs) =>\n      val s = Setup(prefix, hcdSleep, None).add(setSleepTime(sleepTime))\n      cs.submit(s).foreach {\n        case started: Started =>\n          // Can insert extra code during execution here\n          cs.queryFinal(started.runId).foreach(sr => commandResponseManager.updateCommand(sr.withRunId(runId)))\n        case other =>\n          commandResponseManager.updateCommand(other.withRunId(runId))\n      }\n    case None =>\n      commandResponseManager.updateCommand(\n        Error(runId, s\"A needed HCD is not available: ${hcdConnection.componentId} for $prefix\")\n      )\n  } Java private void sleepHCD(Id runId, Setup setup, Long sleepTime) {\n  if (hcdCS.isPresent()) {\n    ICommandService cs = hcdCS.get();\n    Setup s = new Setup(prefix, hcdSleep, Optional.empty()).add(setSleepTime(sleepTime));\n    cs.submit(s).thenAccept(submitResponse -> {\n      if (submitResponse instanceof Started) {\n        Started started = (Started) submitResponse;\n        // Can insert extra code during execution here\n        cs.queryFinal(started.runId(), timeout).thenAccept(sr -> cswCtx.commandResponseManager().updateCommand(sr.withRunId(runId)));\n      } else {\n        cswCtx.commandResponseManager().updateCommand(submitResponse.withRunId(runId));\n      }\n    });\n  } else {\n    cswCtx.commandResponseManager().updateCommand(\n        new CommandResponse.Error(runId, \"A needed HCD is not available: \" + hcdConnection.componentId() + \" for \" + prefix)\n    );\n  }\n}","title":"Using updateCommand"},{"location":"/framework/managing-command-state.html#using-the-crm-with-subcommands","text":"If while processing a received command, the component needs to create and send commands to other components (e.g. an Assembly sending commands to one or more HCDs) it can use the CRM to help manage responses from the sub-commands. In this case, the typical use case is that the sender such as an Assembly needs to send one or more sub-commands to HCDs and needs to wait until all the sub-commands complete. It then makes a decision on how to update the original command received by the Assembly based on the results of the sub-commands. The CRM provides a helper method to wait for one or more sub-commands. Then the previous updateCommand CRM method is used to update the original command.","title":"Using the CRM with Subcommands"},{"location":"/framework/managing-command-state.html#using-queryfinalall","text":"The CRM provides a method called queryFinalAll. This method takes a list of responses from submit or submitAndWait and allows a block of code to be completed when all the commands in the list have completed, either successfully or unsuccessfully. A response is returned from queryFinalAll of type OverallSuccess, which can be OverallSuccess or OverallFailure. Each of these returns the individual responses from the original commands to allow a decision on how to proceed.\nIn this example of a complexCommand, the Assembly sends two sub-commands to HCDs. It then uses queryFinalAll to wait for the sub-commands to finish. In the OverallSuccess case commandResponseManager.updateCommand is used to return Completed to the parent. If one or more of the sub-commands fails, the negative response of the first failed command is returned to the parent.\nScala     case `complexCommand` =>\n      val medium = simpleHCD(runId, Setup(prefix, hcdSleep, setup.maybeObsId).add(setSleepTime(mediumSleepPeriod)))\n      val long   = simpleHCD(runId, Setup(prefix, hcdSleep, setup.maybeObsId).add(setSleepTime(longSleepPeriod)))\n\n      commandResponseManager\n        .queryFinalAll(medium, long)\n        .map {\n          case OverallSuccess(_) =>\n            // Don't care about individual responses with success\n            commandResponseManager.updateCommand(Completed(runId))\n          case OverallFailure(responses) =>\n            // There must be at least one error\n            val errors = responses.filter(isNegative)\n            commandResponseManager.updateCommand(errors.head.withRunId(runId))\n        }\n        .recover(ex => commandResponseManager.updateCommand(Error(runId, ex.toString)))\n\n      Started(runId)\n    case `sleep` =>\n      sleepHCD(runId, setup, setup(sleepTimeKey).head)\n      Started(runId)\n    case _ =>\n      Invalid(runId, CommandIssue.UnsupportedCommandIssue(s\"${setup.commandName.name}\"))\n  }\n\nprivate def simpleHCD(runId: Id, setup: Setup): Future[SubmitResponse] =\n  hcdCS match {\n    case Some(cs) =>\n      cs.submitAndWait(setup)\n    case None =>\n      Future(Error(runId, s\"A needed HCD is not available: ${hcdConnection.componentId}\"))\n  } Java   if (cmd.equals(complexCommand)) {\n    CompletableFuture<SubmitResponse> medium = simpleHCD(runId, new Setup(prefix, hcdSleep, setup.jMaybeObsId()).add(setSleepTime(mediumSleepPeriod)));\n    CompletableFuture<SubmitResponse> long_ = simpleHCD(runId, new Setup(prefix, hcdSleep, setup.jMaybeObsId()).add(setSleepTime(longSleepPeriod)));\n    cswCtx.commandResponseManager().queryFinalAll(Arrays.asList(medium, long_))\n        .thenAccept(response -> {\n          if (response instanceof OverallSuccess) {\n            // Don't care about individual responses with success\n            cswCtx.commandResponseManager().updateCommand(new Completed(runId));\n          } else if (response instanceof OverallFailure) {\n            // There must be at least one error\n            List<SubmitResponse> errors = ((OverallFailure) response).getResponses().stream().filter(CommandResponse::isNegative).collect(Collectors.toList());\n            cswCtx.commandResponseManager().updateCommand(errors.get(0).withRunId(runId));\n          }\n        }).exceptionally(ex -> {\n      cswCtx.commandResponseManager().updateCommand(new CommandResponse.Error(runId, ex.toString()));\n      return null;\n    });\n    return new Started(runId);\n  }\n  if (cmd.equals(sleep)) {\n    sleepHCD(runId, setup, setup.apply(sleepTimeKey).head());\n    return new Started(runId);\n  }\n  return new Invalid(runId, new CommandIssue.UnsupportedCommandIssue(setup.commandName().name()));\n}\n\nprivate CompletableFuture<SubmitResponse> simpleHCD(Id runId, Setup setup) {\n  if (hcdCS.isPresent()) {\n    ICommandService cs = hcdCS.get();\n    return cs.submitAndWait(setup, timeout);\n  }\n  return CompletableFuture.completedFuture(\n          new CommandResponse.Error(runId, \"A needed HCD is not available: \" + hcdConnection.componentId()));\n}","title":"Using queryFinalAll"},{"location":"/framework/tracking-connections.html","text":"","title":"Tracking Connections"},{"location":"/framework/tracking-connections.html#tracking-connections","text":"The component framework tracks the set of connections specified for a component in ComponentInfo if the locationServiceUsage property is set to RegisterAndTrackServices. The framework also provides a helper trackConnection method to track any connection other than those present in ComponentInfo.","title":"Tracking Connections"},{"location":"/framework/tracking-connections.html#onlocationtrackingevent","text":"The onLocationTrackingEvent handler can be used to take action on the TrackingEvent for a particular connection. This event could be for the connections in ComponentInfo tracked automatically or for the connections tracked explicitly using trackConnection method.\nAssembly/Scala override def onLocationTrackingEvent(trackingEvent: TrackingEvent): Unit = trackingEvent match {\n  case LocationUpdated(location)   => // do something for the tracked location when it is updated\n  case LocationRemoved(connection) => // do something for the tracked location when it is no longer available\n} Assembly/Java @Override\npublic void onLocationTrackingEvent(TrackingEvent trackingEvent) {\n    if (trackingEvent instanceof LocationUpdated) {\n        // do something for the tracked location when it is updated\n    } else if (trackingEvent instanceof LocationRemoved) {\n        // do something for the tracked location when it is no longer available\n    }\n} Hcd/Scala override def onLocationTrackingEvent(trackingEvent: TrackingEvent): Unit = trackingEvent match {\n  case LocationUpdated(location)   => // do something for the tracked location when it is updated\n  case LocationRemoved(connection) => // do something for the tracked location when it is no longer available\n} Hcd/Java @Override\npublic void onLocationTrackingEvent(TrackingEvent trackingEvent) {\n    if (trackingEvent instanceof LocationUpdated) {\n        // do something for the tracked location when it is updated\n    } else if (trackingEvent instanceof LocationRemoved) {\n        // do something for the tracked location when it is no longer available\n    }\n}","title":"onLocationTrackingEvent"},{"location":"/framework/publishing-state.html","text":"","title":"Publishing State"},{"location":"/framework/publishing-state.html#publishing-state","text":"A component has access to a currentStatePublisher Actor, which can be used to publish its CurrentState. Any subscriber of this component will receive the published state.\nNote This feature is provided for optimized communication between an Assembly and an HCD only. See Matching state for command completion.\nCurrentState can be used in a number of ways. Two use cases are:\nAn HCD can have detailed information that it publishes for its Assemblies periodically. The Assemblies use the CurrentState information to create events that describe the state of the Assembly and HCD. ‘CurrentState’ information published by the HCD can be used by an Assembly to complete commands that have been submitted to the Assembly.\nThe CommandService shows examples of the use of CurrentState for the second use case. See Matching state for command completion.\nA subscriber can subscribe to all CurrentState published by an HCD or to specific CurrentState specified by its StateName. The publisher does not need to do anything special to support these features.\nScala // Publish the CurrentState using parameter set created using a sample Choice parameter\ncurrentStatePublisher.publish(CurrentState(prefix, StateName(\"testStateName\"), Set(choiceKey.set(initChoice)))) Java CurrentState initState = currentState.add(SampleComponentState.choiceKey().set(SampleComponentState.initChoice()));\ncurrentStatePublisher.publish(initState);","title":"Publishing State"},{"location":"/framework/handling-exceptions.html","text":"","title":"Handling Exceptions"},{"location":"/framework/handling-exceptions.html#handling-exceptions","text":"A component should create exceptions belonging to one of the two following types:\nFailureRestart : As a part of any handler, if an exception can be handled by restarting the component, an exception of type FailureRestart should be thrown to let the framework restart the component. The component’s state will be cleared/reinitialized. The onInitialize handler will be invoked again. Scala case class HcdNotFoundException() extends FailureRestart(\"Could not resolve hcd location. Initialization failure.\")\n\nprivate def resolveHcd(): Future[Option[AkkaLocation]] = {\n  val maybeConnection = componentInfo.connections.find(connection => connection.componentId.componentType == ComponentType.HCD)\n  maybeConnection match {\n    case Some(hcd) =>\n      cswCtx.locationService.resolve(hcd.of[AkkaLocation], 5.seconds).map {\n        case loc @ Some(akkaLocation) => loc\n        case None                     =>\n          // Hcd connection could not be resolved for this Assembly. One option to handle this could be to automatic restart which can give enough time\n          // for the Hcd to be available\n          throw HcdNotFoundException()\n      }\n    case _ => Future.successful(None)\n  }\n} Java class HcdNotFoundException extends FailureRestart {\n    HcdNotFoundException() {\n        super(\"Could not resolve hcd location. Initialization failure.\");\n    }\n}\n\nprivate CompletableFuture<Optional<AkkaLocation>> resolveHcd() {\n    // find a Hcd connection from the connections provided in componentInfo\n    Optional<Connection> mayBeConnection = componentInfo.getConnections().stream()\n            .filter(connection -> connection.componentId().componentType() == JComponentType.HCD)\n            .findFirst();\n\n    // If an Hcd is found as a connection, resolve its location from location service and create other\n    // required worker actors required by this assembly\n    if (mayBeConnection.isPresent()) {\n        CompletableFuture<Optional<AkkaLocation>> resolve = locationService.resolve(mayBeConnection.orElseThrow().<AkkaLocation>of(), Duration.ofSeconds(5));\n        return resolve.thenCompose((Optional<AkkaLocation> resolvedHcd) -> {\n            if (resolvedHcd.isPresent())\n                return CompletableFuture.completedFuture(resolvedHcd);\n            else\n                throw new ConfigNotAvailableException();\n        });\n    } else\n        return CompletableFuture.completedFuture(Optional.empty());\n} FailureStop : As a part of any handler, an exception can be thrown of type FailureStop which will result in terminating the component. The onShutdown handler will be invoked to facilitate graceful shutdown. Scala case class ConfigNotAvailableException() extends FailureStop(\"Configuration not available. Initialization failure.\")\n\nprivate def getAssemblyConfig: Future[ConfigData] = {\n\n  configClientService.getActive(Paths.get(\"tromboneAssemblyContext.conf\")).flatMap {\n    case Some(config) => Future.successful(config) // do work\n    case None         =>\n      // required configuration could not be found in the configuration service. Component can choose to stop until the configuration is made available in the\n      // configuration service and started again\n      throw ConfigNotAvailableException()\n  }\n} Java class ConfigNotAvailableException extends FailureStop {\n    ConfigNotAvailableException() {\n        super(\"Configuration not available. Initialization failure.\");\n    }\n}\n\nprivate CompletableFuture<ConfigData> getAssemblyConfig() throws ConfigNotAvailableException {\n    // required configuration could not be found in the configuration service. Component can choose to stop until the configuration is made available in the\n    // configuration service and started again\n    return configClient.getActive(Paths.get(\"tromboneAssemblyContext.conf\"))\n            .thenApply((Optional<ConfigData> maybeConfigData) -> {\n                return maybeConfigData.<ConfigNotAvailableException>orElseThrow(() -> new ConfigNotAvailableException());\n            });\n}","title":"Handling Exceptions"},{"location":"/framework/deploying-components.html","text":"","title":"Deploying Components"},{"location":"/framework/deploying-components.html#deploying-components","text":"","title":"Deploying Components"},{"location":"/framework/deploying-components.html#containercmd","text":"ContainerCmd is a helper utility provided as a part of the framework. This helps component writers to start their components inside a container, but can also be used to start standalone components (not in a container).\nA main application needs to be created which uses the framework provided utility csw.framework.deploy.containercmd.ContainerCmd to start a container or standalone component. The utility supports the following parameters, which can be provided as arguments to the application :\nfully qualified path of the configuration file local if the above path is a path to a file available on local disk. If this argument is not provided the file will be looked up in the Configuration Service using the same path. standalone if the configuration file describes a component to be run in standalone mode. If this argument is not provided the application expects a configuration file describing a container component and will use it to start a container with all the components as described in the file.\nScala object ContainerCmdApp extends App {\n\n  ContainerCmd.start(\"Container-Cmd-App\", CSW, args)\n\n} Java public class JContainerCmdApp {\n\n    public static void main(String args[]) {\n        JContainerCmd.start(\"JContainer-Cmd-App\", JSubsystem.CSW,args, Optional.empty());\n    }\n\n}\nNote It is not necessary to have name of the application as ContainerCmdApp/JContainerCmdApp; the user can choose any name.\nStarting a standalone component from a local configuration file\n`./container-cmd-app --standalone --local /assembly/config/assembly.conf`\nStarting a container component from a configuration file available in configuration service\n`./container-cmd-app /assembly/config/assembly.conf`","title":"ContainerCmd"},{"location":"/framework/deploying-components.html#container-for-deployment","text":"A container is a component which starts one or more components and keeps track of the components within a single JVM process. When started, the container also registers itself with the Location Service. The components to be hosted by the container are defined using a ContainerInfo model which has a set of ComponentInfo objects. It is usually described in a configuration file but can also be created programmatically.\nSampleContainerInfo name = \"Sample_Container\"\ncomponents: [\n  {\n    componentType = assembly\n    behaviorFactoryClassName = package.component.SampleAssembly\n    prefix = wfos.SampleAssembly\n    locationServiceUsage = RegisterAndTrackServices\n    connections = [\n      {\n        prefix: wfos.Sample_Hcd_1\n        componentType: hcd\n        connectionType: akka\n      },\n      {\n        prefix: wfos.Sample_Hcd_2\n        componentType: hcd\n        connectionType: akka\n      },\n      {\n        prefix: wfos.Sample_Hcd_3\n        componentType: hcd\n        connectionType: akka\n      }\n    ]\n  },\n  {\n    prefix = \"wfos.Sample_Hcd_1\"\n    componentType = hcd\n    behaviorFactoryClassName = package.component.SampleHcd\n    prefix = abc.sample.prefix\n    locationServiceUsage = RegisterOnly\n  },\n  {\n    prefix = \"wfos.Sample_Hcd_2\"\n    componentType: hcd\n    behaviorFactoryClassName: package.component.SampleHcd\n    prefix: abc.sample.prefix\n    locationServiceUsage = RegisterOnly\n  }\n]","title":"Container for deployment"},{"location":"/framework/deploying-components.html#standalone-components","text":"A component can be run alone in a standalone mode without sharing its JVM space with any other component.\nSample Info for an assembly componentType = assembly\nbehaviorFactoryClassName = csw.common.components.command.ComponentBehaviorFactoryForCommand\nprefix = tcs.mobie.blue.monitor.Monitor_Assembly\nlocationServiceUsage = RegisterOnly","title":"Standalone components"},{"location":"/commons/command.html","text":"","title":"Communication using Commands"},{"location":"/commons/command.html#communication-using-commands","text":"The csw-command library provides support for command based communication between components.\nThis section describes how to communicate with any other component using commands. To check how to manage commands received, please visit Component Handlers and Managing Command State.","title":"Communication using Commands"},{"location":"/commons/command.html#dependencies","text":"sbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-command\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/commons/command.html#command-based-communication-between-components","text":"A component can send Commands to other components. The commands can be sent as one of the following three types of messages:\nsubmit - A command is sent using a submit when the result of completion is desired. oneway - A command is sent as oneway when the result of completion is not desired. validate - A command is sent, but it is only validated with no actions started.\nSubmit comes in two versions: submit and submitAndWait. The only difference is in how long-running commands are treated. If a command starts long-running actions, submit returns a Started response immediately while submitAndWait will wait for the long-running actions to complete before returning a final response. The submitAndWait is a composite of submit and queryFinal.\nA submitAndWait is the most convenient way of sending a command from one component to another. When received, a submitAndWait command is validated and if accepted, the actions associated with the command are executed or started. When the submit response that is part of the submitAndWait is received, submitAndWait immediately does a queryFinal. For simple scenarios, this is the right command and will be the most commonly used.\nA submit can be used when the sender of a command needs to do additional work before long-running actions are completed. For instance, send another command to execute in parallel. If commands are short, submit and submitAndWait behave the same way. When the actions started by submit are long-running, the caller can wait for the actions to complete if needed using the queryFinal call.\nA oneway is primarily meant to be used between an Assembly and an HCD when no completion information is desired. It is also useful when tracking completion using a Matcher and current state values (see below) or events.\nA validate message is used to ask a destination component to validate a command and determine if the command can be executed. It does not execute the command and only returns the result of validation. In some scenarios, it may be useful to test to see if a command can be executed prior to trying to execute the command.","title":"Command-based Communication Between Components"},{"location":"/commons/command.html#component-locking","text":"If a component is locked, any Command Service command will have a response of Locked. The corresponding handler will not be called until the component is unlocked. See Creating a Component for more information on locking.\nNote that the code in the receiving component’s handler does not need to return Locked. If the component has been Locked, the component’s Supervisor returns the Locked response to the caller and the handler is not called.","title":"Component Locking"},{"location":"/commons/command.html#command-validation","text":"Command validations occur in two scenarios. One is using the validate message as described above. For example, validate could be used by an Assembly when it needs to send multiple commands to different HCDs and it wants to first check that all the HCDs can execute their commands before sending a command to any of the HCDs. The second scenario is that it is always the first step in processing a command with submit, submitAndWait, or oneway.\nIf the receiving component is not locked, the component’s supervisor calls the validateCommand handler of the Top Level Actor. The developer code evaluates and returns a ValidateCommandResponse as shown in the following table.\nValidateCommandResponse Description Accepted The command is valid and can be executed by the component. Invalid The command is not valid and cannot be executed. The response includes a reason in the form of a CommandIssue Locked The component is locked by some other command sender. The validation could not occur.","title":"Command Validation"},{"location":"/commons/command.html#the-submit-message","text":"A submit message is sent with its command to a component destination. A SubmitResponse is returned to the caller when the submit message is used. If the validateCommand handler returns Accepted, the framework calls the onSubmit handler of the Top Level Actor. The onSubmit handler always returns a SubmitResponse.","title":"The Submit Message"},{"location":"/commons/command.html#immediate-completion-scenario","text":"If the actions of the submit command take a very short time to complete, they may be completed by the onSubmit handler. This is called immediate completion. The time for the actions to complete should be less than 1 second. (Note: The framework will timeout if the destination does not return a response within 1 second.) In this scenario with onSubmit, the values of SubmitResponse can be Completed or Error. Error is returned when the actions could not be accomplished. This is different than Invalid, which indicates that the command could not be validated.\nThe immediate completion behavior is similar to a remote procedure call although the execution is entirely asynchronous. If the actions are successful, the Completed SubmitResponse is returned. If there is a result, the Completed is returned with a parameter set of Result type that can be inspected by the sender.","title":"Immediate Completion Scenario"},{"location":"/commons/command.html#long-running-actions-scenario","text":"When actions take longer than 1 second, onSubmit should start the actions and return the Started SubmitResponse. The Started response indicates to the framework that long-running actions have been started.\nOnce the long-running actions have started, the receiving component code must notify the framework when the actions are completed. This is done be updating through the Command Response Manager.\nIn addition to the values returned for immediate completion, long running actions can return Cancelled. If the component supports a separate command to stop a long-running command, the stopped command should return Cancelled when successfully cancelled. The command that cancels the long running command should return Completed.","title":"Long Running Actions Scenario"},{"location":"/commons/command.html#the-submitandwait-message","text":"A submitAndWait message is sent with its command to a component destination. A SubmitResponse is returned to the caller when the submitAndWait message is used. If the validateCommand handler returns Accepted, the framework calls the onSubmit handler of the Top Level Actor. The onSubmit handler always returns a SubmitResponse.","title":"The SubmitAndWait Message"},{"location":"/commons/command.html#immediate-completion-scenario","text":"If the actions of the submitAndWait command take a very short time to complete, submitAndWait behaves exactly like submit as described above.\nThe immediate completion behavior is similar to a remote procedure call although the execution is entirely asynchronous. If the actions are successful, the Completed SubmitResponse is returned. If there is a result, the Completed is returned with a parameter set of Result type that can be inspected by the sender.","title":"Immediate Completion Scenario"},{"location":"/commons/command.html#long-running-actions-scenario","text":"When actions take longer than 1 second, onSubmit should start the actions and return the Started SubmitResponse. The Started response indicates to the framework that long-running actions have been started. In this case, submitAndWait immediately executes a queryFinal command in order to wait for the final response.\nAs with the submit description, the receiving component code must notify the framework when the actions are completed. This is done be updating the Command Response Manager.","title":"Long Running Actions Scenario"},{"location":"/commons/command.html#submitresponse-summary","text":"The following table summarizes all the possible values for SubmitResponse.\nSubmitResponse Description Invalid The command is not valid and cannot be executed. The response includes a reason in the form of a CommandIssue. onSubmit is not executed. Completed(Result) This response is returned when the actions associated with a command are completed. A result may or may not be returned. Started Returned by onSubmit when long-running actions have been started. Error Error is returned the the actions started by a command do not complete properly. A message is returned explaining the error. Cancelled The actions associated with a long-running command have been cancelled. Locked The component is locked by some other command sender. The validation could not occur.","title":"SubmitResponse Summary"},{"location":"/commons/command.html#the-oneway-message","text":"The other option for sending a command to a component destination is the oneway message. The central difference between submit and oneway is that oneway does not track or allow reporting of completion information. It supports a fire and forget type of communication approach.\nA OnewayResponse is returned to the caller when the oneway message is used. Oneway does validate the command. If the component is not locked, the validateCommand handler is called. If the validateCommand handler returns Accepted, the framework calls the onOneway handler of the Top Level Actor. However, the onOneway handler does not return a value. The sender of the oneway message receives the result of the validation or the Locked indication.\nThe following table summarizes all the possible values for OnewayResponse.\nOnewayResponse Description Invalid The command is not valid and cannot be executed. The response includes a reason in the form of a CommandIssue. onOneway is not executed. Accepted Returned when validation succeeds and the command was passed to the onOneway handler. Locked The component is locked by some other command sender. The validation could not occur.\nOneway is available as a higher performance option when an Assembly needs to send commands to an HCD but doesn’t really care about completion such as the case when demands are being sent to a motor. Validation is still present to ensure the HCD supports the standalone operation requirement that allows the HCD to check that its input values are not invalid and it is not getting out of range values.\nOneway can be used with a matcher. The matcher can use CurrentState or even events from the Event Service to determine completion. This can be more complicated than submit, but may be useful in some scenarios.","title":"The Oneway Message"},{"location":"/commons/command.html#commandservice","text":"A helper/wrapper is provided called CommandService that provides a convenient way to use the Command Service with a component from the Location Service.\nA CommandService instance is created using an AkkaLocation of the receiving component, discovered from the Location Service. This CommandService instance has methods for communicating with the component. A new CommandService is created for each component for which commands are to be sent.\nScala async library! Note that the Scala examples are using async/await which simplifies handling the Futures, but is not necessary. The async/await library is not available in Java.\nThe API can be exercised as follows for different scenarios of command-based communication:","title":"CommandService"},{"location":"/commons/command.html#submit","text":"Sending a submit message with a command returns a SubmitResponse as a Future. The Future returned by submit will be the final response in case of short-running command and may be a positive completion (Completed) or a negative completion (Invalid, Error, Cancelled, Locked). The response may also be the Started response in the case of long-running actions. In the case of a long-running command the Started response can be used with either query or queryFinal to obtain the final response.\nThis example shows a command using submit that returns Started followed by a queryFinal to get the final response.\nScala/submit w/queryFinal private def sleepHCD(runId: Id, setup: Setup, sleepTime: Long): Unit =\n  hcdCS match {\n    case Some(cs) =>\n      val s = Setup(prefix, hcdSleep, None).add(setSleepTime(sleepTime))\n      cs.submit(s).foreach {\n        case started: Started =>\n          // Can insert extra code during execution here\n          cs.queryFinal(started.runId).foreach(sr => commandResponseManager.updateCommand(sr.withRunId(runId)))\n        case other =>\n          commandResponseManager.updateCommand(other.withRunId(runId))\n      }\n    case None =>\n      commandResponseManager.updateCommand(\n        Error(runId, s\"A needed HCD is not available: ${hcdConnection.componentId} for $prefix\")\n      )\n  } Java/submit w/queryFinal private void sleepHCD(Id runId, Setup setup, Long sleepTime) {\n  if (hcdCS.isPresent()) {\n    ICommandService cs = hcdCS.get();\n    Setup s = new Setup(prefix, hcdSleep, Optional.empty()).add(setSleepTime(sleepTime));\n    cs.submit(s).thenAccept(submitResponse -> {\n      if (submitResponse instanceof Started) {\n        Started started = (Started) submitResponse;\n        // Can insert extra code during execution here\n        cs.queryFinal(started.runId(), timeout).thenAccept(sr -> cswCtx.commandResponseManager().updateCommand(sr.withRunId(runId)));\n      } else {\n        cswCtx.commandResponseManager().updateCommand(submitResponse.withRunId(runId));\n      }\n    });\n  } else {\n    cswCtx.commandResponseManager().updateCommand(\n        new CommandResponse.Error(runId, \"A needed HCD is not available: \" + hcdConnection.componentId() + \" for \" + prefix)\n    );\n  }\n}\nIf using submit and the validation fails in the destination component, the Invalid response is returned. Sometimes it is possible to handle an Invalid response locally, but in most cases it must simply be returned to the caller. The following example shows how to process individual responses from a submit:\nScala/submit w/invalid response val invalidSetup    = Setup(prefix, invalidCmd, obsId)\nval invalidCommandF = assemblyCmdService.submitAndWait(invalidSetup)\nasync {\n  await(invalidCommandF) match {\n    case Completed(_, _) =>\n      // Do Completed thing\n    case Invalid(_, _) =>\n      //issue shouldBe a[Invalid]\n    case other =>\n      // Unexpected result\n      log.error(s\"Some other response: $other\")\n  }\n}\nAwait.result(invalidCommandF, 5.seconds) shouldBe a[Invalid] Java/submit w/invalid response CompletableFuture<SubmitResponse> invalidSubmitCommandF =\n        hcdCmdService.submit(invalidSetup).thenApply(\n                response -> {\n                    if (response instanceof Started) {\n                        //do something with completed result\n                    } else if (response instanceof Invalid) {\n                        // Cast the response to get the issue\n                        Invalid invalid = (Invalid) response;\n                        assert (invalid.issue().reason().contains(\"failure\"));\n                    }\n                    return response;\n                }\n        );\nThe handling an immediate completion command looks the same from the command sender’s perspective, but can be challenging on the side of the component handling the command. Because submit returns a Future[SubmitResponse] and onSetup returns SubmitResponse, an immediate completion command must be handled in the component that receives the request. For instance, an Assembly can return a Completed without returning Started, but if an Assembly needs to contact an HCD to get a response, the Assembly must first return Started and then contact the HCD. The following snippet shows a portion of an onSetup handler that returns some local state as an immediate completion.\nScala/submit immediate command private def onSetup(runId: Id, setup: Setup): SubmitResponse =\n  setup.commandName match {\n    case `immediateCommand` =>\n      val localValue = 1000L\n      // Assembly preforms a calculation or reads state information storing in a result\n      Completed(runId, Result().add(resultKey.set(localValue))) Java/submit immediate command private SubmitResponse onSetup(Id runId, Setup setup) {\n  CommandName cmd = setup.commandName();\n  if (cmd.equals(immediateCommand)) {\n    // Assembly preforms a calculation or reads state information storing in a result\n    return new Completed(runId, new Result().madd(resultKey.set(1000L)));\n  }","title":"submit"},{"location":"/commons/command.html#submitandwait","text":"submitAndWait is a convenience method which sends a submit message and then, if the command is long-running, it waits for final completion. Sending a submit message with a command returns a SubmitResponse as a Future. The Future returned by submitAndWait will always be the final response, whether the actions are long-running or not. The final result may be a positive completion (Completed) or a negative completion (Invalid, Error, Cancelled, Locked). The Started response is never seen by the programmer when using submitAndWait of CommandService. The handling of long-running and immediate completion commands look the same from the command sender’s perspective when using submitAndWait.\nThis example shows an immediate completion command using submitAndWait.\nScala/submitAndWait w/immediate-response val immediateSetup = Setup(prefix, immediateCmd, obsId)\nval immediateCommandF = async {\n  await(assemblyCmdService.submitAndWait(immediateSetup)) match {\n    case response: Completed =>\n      //do something with completed result\n      response\n    case otherResponse =>\n      // do something with other response which is not expected\n      otherResponse\n  }\n} Java/submitAndWait w/immediate-response CompletableFuture<SubmitResponse> immediateCommandF =\n        hcdCmdService.submitAndWait(immediateCmd, timeout).thenApply(\n                response -> {\n                    if (response instanceof Completed) {\n                        //do something with completed result\n                    } else {\n                        // do something with unexpected response\n                    }\n                    return response;\n                }\n        );\nSeveral examples have been shown of sending a command that is long-running. The following examples show an Assembly that issues a command to an HCD with submitAndWait and returns Started. When submitAndWait returns with a final response, the parent is updated with the final response through the CommandResponseManager.\nScala/submitAndWait w/immediate-response case `longRunningCmdToAsm` =>\n  hcdComponent.submitAndWait(longRunning).foreach {\n    case _: Completed =>\n      commandResponseManager.updateCommand(Completed(runId))\n    case other =>\n      // Handle some other response besides Completed\n      commandResponseManager.updateCommand(other.withRunId(runId))\n  }\n  // Assembly starts long-running and returns started\n  Started(runId)\n Java/submitAndWait w/immediate-response Setup longRunningSetup = new Setup(prefix(), longRunningCmd(), Optional.empty()).add(encoderValue);\nCompletableFuture<Optional<Integer>> longRunningResultF =\n        hcdCmdService.submitAndWait(longRunningSetup, timeout)\n                .thenCompose(response -> {\n                    if (response instanceof Completed) {\n                        // This extracts and returns the the first value of parameter encoder\n                        Result result = ((Completed) response).result();\n                        Optional<Integer> rvalue = Optional.of(result.jGet(encoder).orElseThrow().head());\n                        return CompletableFuture.completedFuture(rvalue);\n                    } else {\n                        // For some other response, return empty\n                        return CompletableFuture.completedFuture(Optional.empty());\n                    }\n                });","title":"submitAndWait"},{"location":"/commons/command.html#oneway","text":"Oneway does not provide completion information but does return the result of validateCommand handler in the Top-Level-Actor (Accepted, Invalid, or Locked). When sending a command as a oneway message, a OnewayResponse is returned as a Future that can be used to check that it was validated if necessary.\nOneway is useful for communication between an Assembly when it needs to send commands to an HCD as quickly as possible. The command is validated on the destination and the validation response is returned, but no other responses are provided.\nScala // `onewayCmd` is a sample to demonstrate oneway without any actions\nval onewaySetup = Setup(prefix, onewayCmd, obsId)\n// Don't care about the futures from async\nval oneWayF = async {\n  await(assemblyCmdService.oneway(onewaySetup)) match {\n    case invalid: Invalid =>\n    // Log an error here\n    case _ =>\n    // Ignore anything other than invalid\n  }\n}\nAwait.ready(oneWayF, timeout.duration) Java Setup onewaySetup = new Setup(prefix(), onewayCmd(), Optional.empty()).add(encoderValue);\nCompletableFuture onewayF = hcdCmdService\n        .oneway(onewaySetup)\n        .thenAccept(onewayResponse -> {\n            if (onewayResponse instanceof Invalid) {\n                // log an error here\n            } else {\n                // Ignore anything other than invalid\n            }\n        });","title":"oneway"},{"location":"/commons/command.html#validate","text":"Sometimes it may be useful to test whether or not a component can execute a command without committing to executing its actions. The validate message can be used for this purpose. Validate returns a ValidateResponse of Accepted, Invalid, or Locked.\nScala val validateCommandF = async {\n  await(assemblyCmdService.validate(immediateSetup)) match {\n    case _: Accepted       => true\n    case Invalid(_, issue) =>\n      // do something with other response which is not expected\n      log.error(s\"Command failed to validate with issue: $issue\")\n      false\n    case _: Locked => false\n  }\n}\nAwait.result(validateCommandF, timeout.duration) shouldBe true Java CompletableFuture<Boolean> validateCommandF =\n        hcdCmdService.validate(immediateCmd)\n                .thenApply(\n                        response -> {\n                            if (response instanceof Accepted) {\n                                //do something with completed result\n                                return true;\n                            } else if (response instanceof Invalid) {\n                                // do something with unexpected response\n                                return false;\n                            } else {\n                                // Locked\n                                return false;\n                            }\n                        }\n                );\nAssert.assertTrue(validateCommandF.get());","title":"validate"},{"location":"/commons/command.html#query","text":"If a command returns a Started response indicating it has long-running actions, and the sender needs to determine that the actions have started properly, or wishes to poll the destination component for the final response, the query method of CommandService can be used as shown in the following example without using the Future returned by submitAndWait, which provides the final completion notification.\nScala/submit w/query val longRunningQueryResultF = async {\n  // The following val is set so we can do query and work and complete later\n  val longRunningF = assemblyCmdService.submit(longRunningSetup)\n  // This is used in a later test\n  longRunningRunId = await(longRunningF).runId\n\n  await(assemblyCmdService.query(longRunningRunId)) match {\n    case Started(runId) =>\n      runId shouldEqual longRunningRunId\n      // happy case - no action needed\n      // Do some other work\n    case a =>\n      // log.error. This indicates that the command probably failed to start.\n  }\n\n  // Now wait for completion and result\n  await(assemblyCmdService.queryFinal(longRunningRunId)) match {\n    case Completed(_, result) =>\n      Some(result(encoder).head)\n\n    case otherResponse =>\n      // log a message?\n      None\n  }\n}\nAwait.result(longRunningQueryResultF, timeout.duration) shouldBe Some(20) Java/submit w/query CompletableFuture<CommandResponse.SubmitResponse> longRunningCommandResultF =\n        hcdCmdService.submitAndWait(longRunningSetup, timeout);\n\n// do some work before querying for the result of above command as needed\nSubmitResponse sresponse = longRunningCommandResultF.get();\nCompletableFuture<SubmitResponse> queryResponseF = hcdCmdService.query(sresponse.runId());\nqueryResponseF.thenAccept(r -> {\n    if (r instanceof Started) {\n        // happy case - no action needed\n        // Do some other work\n    } else {\n        // log.error. This indicates that the command probably failed to start.\n    }\n});\n\nCompletableFuture<Optional<Integer>> intF =\n        longRunningCommandResultF.thenCompose(response -> {\n            if (response instanceof Completed) {\n                // This extracts and returns the the first value of parameter encoder\n                Result result = ((Completed) response).result();\n                Optional<Integer> rvalue = Optional.of(result.jGet(encoder).orElseThrow().head());\n                return CompletableFuture.completedFuture(rvalue);\n            } else {\n                // For some other response, return empty\n                return CompletableFuture.completedFuture(Optional.empty());\n            }\n        });\nAssert.assertEquals(Optional.of(20), intF.get());","title":"query"},{"location":"/commons/command.html#queryfinal","text":"When using submit and the submit returns a Started response, the final completion reponse can be obtained using queryFinal. Like query, queryFinal uses the Id returned by Started. But in this case, rather than returning immediately like query, it waits and only returns when the final SubmitResponse is sent. queryFinal is used exclusively with submit in the case where some other activity must be done before the actions started by the submit complete. submitAndWait is just a submit + queryFinal. It is also possible to just start actions with submitAndWait and use the returned Future when you are ready. The following examples show you can use the runId returned by submitAndWait with queryFinal.\nScala/submitAndWait long running/queryFinal val encoderValue:Future[Option[Int]] = async {\n  // The following submit is made without saving the Future!\n  val runId = await(assemblyCmdService.submitAndWait(longRunningSetup)).runId\n\n  // Use queryFinal and runId to wait for completion and result\n  await(assemblyCmdService.queryFinal(runId)) match {\n    case Completed(_, result) =>\n      Some(result(encoder).head)\n\n    case otherResponse =>\n      // log a message?\n      None\n  }\n}\nAwait.result(encoderValue, timeout.duration) shouldBe Some(20) Java/submitAndWait long running/queryFinal longRunningCommandResultF = hcdCmdService.submitAndWait(longRunningSetup, timeout);\nsresponse = longRunningCommandResultF.get();\n\n// longRunningSetup3 has already been submitted\nCompletableFuture<Optional<Integer>> int3F =\n        hcdCmdService.queryFinal(sresponse.runId(), timeout).thenCompose(response -> {\n            if (response instanceof Completed) {\n                // This extracts and returns the the first value of parameter encoder\n                Result result = ((Completed) response).result();\n                Optional<Integer> rvalue = Optional.of(result.jGet(encoder).orElseThrow().head());\n                return CompletableFuture.completedFuture(rvalue);\n            } else {\n                // For some other response, return empty\n                return CompletableFuture.completedFuture(Optional.empty());\n            }\n        });\nAssert.assertEquals(Optional.of(20), int3F.get());","title":"queryFinal"},{"location":"/commons/command.html#submitallandwait","text":"submitAllAndWait can be used to send multiple commands sequentially to the same component. This could be used to send initialization commands to an HCD, for instance. The argument for submitAllAndWait is a list of commands. submitAllAndWait returns a list of SubmitResponses – one for each command in the list. While submitAndWait returns a SubmitResponse as a Future, submitAllAndWait returns a list of SubmitResponses as a future, which completes when all the commands in the list have completed.\nScala/query usage val submitAllF = async {\n  await(assemblyCmdService.submitAllAndWait(List(submitAllSetup1, submitAllSetup2, submitAllinvalidSetup)))\n}\nval submitAllResponse = Await.result(submitAllF, timeout.duration)\nsubmitAllResponse.length shouldBe 3\nsubmitAllResponse(0) shouldBe a[Completed]\nsubmitAllResponse(1) shouldBe a[Completed]\nsubmitAllResponse(2) shouldBe a[Invalid] Java/query usage Setup submitAllSetup1 = new Setup(prefix(), immediateCmd(), Optional.empty()).add(encoderValue);\nSetup submitAllSetup2 = new Setup(prefix(), longRunningCmd(), Optional.empty()).add(encoderValue);\nSetup submitAllSetup3 = new Setup(prefix(), invalidCmd(), Optional.empty()).add(encoderValue);\n\nCompletableFuture<List<SubmitResponse>> submitAllF = hcdCmdService\n        .submitAllAndWait(List.of(submitAllSetup1, submitAllSetup2, submitAllSetup3), timeout);\n\nList<SubmitResponse> submitAllResponse = submitAllF.get();\nAssert.assertEquals(submitAllResponse.size(), 3);\nAssert.assertTrue(submitAllResponse.get(0) instanceof Completed);\nAssert.assertTrue(submitAllResponse.get(1) instanceof Completed);\nAssert.assertTrue(submitAllResponse.get(2) instanceof Invalid);\nIn the first example, three commands are sent and the result is a list with three SubmitResponses. The last one returned invalid and was not executed.\nThe commands in submitAllAndWait will execute sequentially, but each one must complete successfully for the subsequent commands to be executed. If any one of the commands fails, submitAllAndWait stops and the list is returned with the commands that are completed up to and including the command that failed.","title":"submitAllAndWait"},{"location":"/commons/command.html#subscribecurrentstate","text":"This method provided by CommandService can be used to subscribe to the CurrentState of a component by providing a callback that is called with the arrival of every CurrentState item. SubscribeCurrentState returns a handle of the CurrentStateSubscription which should be used to unsubscribe the subscription.\nNote Callbacks are not thread-safe on the JVM. If you are doing side effects/mutations inside the callback, you should ensure that it is done in a thread-safe way inside an actor. Here is an example of how it can be done.\nCurrentState can be an efficient way to keep the Aseembly up to date with the current internal state of the HCD.\nThe following example code shows an Assembly that subscribes to all CurrentState items of an HCD. The example sends a Setup with an encoder parameter value to the HCD as a oneway message. In this example, sending this command causes the HCD to publish CurrentState with the value that was sent to it.\nScala // Subscriber code\nval expectedEncoderValue = 234\nval currStateSetup       = Setup(prefix, hcdCurrentStateCmd, obsId).add(encoder.set(expectedEncoderValue))\n// Setup a callback response to CurrentState\nvar cstate: CurrentState = CurrentState(prefix, StateName(\"no cstate\"), Set.empty)\nval subscription         = hcdCmdService.subscribeCurrentState(cs => cstate = cs)\n// Send a oneway to the HCD that will cause it to publish a CurrentState with the encoder value\n// in the command parameter \"encoder\". Callback will store value into cstate.\nhcdCmdService.oneway(currStateSetup)\n\n// Wait for a bit for callback\nThread.sleep(100)\n// Test to see if value was received\ncstate(encoder).head shouldBe expectedEncoderValue\n\n// Unsubscribe to CurrentState\nsubscription.cancel() Java // Subscriber code\nint expectedEncoderValue = 234;\nSetup currStateSetup = new Setup(prefix(), hcdCurrentStateCmd(), Optional.empty()).add(encoder.set(expectedEncoderValue));\n// Setup a callback response to CurrentState - use AtomicInteger to capture final value\nfinal AtomicInteger cstate = new AtomicInteger((1));\nSubscription subscription = hcdCmdService.subscribeCurrentState(cs -> {\n    // Example sets variable outside scope of closure\n    cstate.set(cs.jGet(encoder).orElseThrow().head());\n});\n\n// Send a oneway to the HCD that will cause a publish of a CurrentState with the encoder value\n// in the command parameter \"encoder\"\nhcdCmdService.oneway(currStateSetup);\n\n// Wait for a bit for the callback\nThread.sleep(200);\n// Check to see if CurrentState has the value we sent\nAssert.assertEquals(expectedEncoderValue, cstate.get());\n\n// Unsubscribe from CurrentState\nsubscription.cancel();\n// subscribe to the current state of an assembly component and use a callback which forwards each received\n// element to a test probe actor\nSubscription subscription = hcdCmdService.subscribeCurrentState(currentState -> probe.ref().tell(currentState));\nThe second part of the example shows the code in the HCD. When the HCD receives the oneway message, it extracts the encoder value and publishes a CurrentState item with the encoder parameter.\nScala val currentState = CurrentState(prefix, StateName(\"HCDState\"), controlCommand.paramSet)\ncswCtx.currentStatePublisher.publish(currentState) Java Key<Integer> encoder = JKeyType.IntKey().make(\"encoder\", JUnits.encoder);\nint expectedEncoderValue = setup.jGet(encoder).orElseThrow().head();\n\nCurrentState currentState = new CurrentState(prefix(), new StateName(\"HCDState\")).add(encoder().set(expectedEncoderValue));\ncurrentStatePublisher.publish(currentState);\nThere are two subscribeCurrentState methods in CommandService. The method shown in the above examples subscribes the caller to all CurrentState published. Each CurrentState item has a StateName. A second signature for subscribeCurrentState can include a Set of StateName when the caller only needs some of the CurrentState published by a component.","title":"subscribeCurrentState"},{"location":"/commons/command.html#matching-state-for-command-completion","text":"The matcher is provided to allow a component sending a command to use CurrentState published by a component to determine when actions are complete. The expected case is an Assembly using the CurrentState published by an HCD. When using a submit or submitAndWait, completion is determined in the destination. In some scenarios, the Assembly may want to determine when actions are complete. This is what the matcher allows.\nTo use this feature, the oneway message is used rather than submit. A oneway command is validated but the framework does not provide completion. Doing a query with the runId of a oneway will always return CommandNotAvailable, but oneway is perfect for use with a matcher.\nThe matcher is created with the ActorRef of the component that is the source of CurrentState and an instance of StateMatcher, which defines the state and criteria for matching.\nSeveral types of StateMatcher are provided as part of CSW for common use. These are DemandMatcherAll for matching the entire DemandState against the current state, DemandMatcher for matching state with or without units against the current state, and PresenceMatcher which checks if a matching state is found with a provided prefix.\nThe developer is not limited to these StateMatchers. Any class the implements the StateMatcher interface can be provided to a Matcher.\nScala val param: Parameter[Int] = encoder.set(100)\nval setupWithMatcher      = Setup(prefix, matcherCmd, obsId)\n\n// create a StateMatcher which specifies the desired algorithm and state to be matched.\nval demandMatcher: StateMatcher =\n  DemandMatcher(DemandState(prefix, StateName(\"testStateName\")).add(param), withUnits = false, timeout)\n\n// Submit command as a oneway and if the command is successfully validated,\n// check for matching of demand state against current state\nval matchResponseF: Future[MatchingResponse] = assemblyCmdService.onewayAndMatch(setupWithMatcher, demandMatcher)\n\nval commandResponse = Await.result(matchResponseF, timeout.duration)\ncommandResponse shouldBe a[Completed] Java Parameter<Integer> param = JKeyType.IntKey().make(\"encoder\", JUnits.encoder).set(100);\nSetup setupWithMatcher = new Setup(prefix(), matcherCmd(), Optional.empty()).add(param);\n\n// create a StateMatcher which specifies the desired algorithm and state to be matched.\nDemandMatcher demandMatcher = new DemandMatcher(new DemandState(prefix(), new StateName(\"testStateName\")).add(param), false, timeout);\n\n// Submit command as a oneway and if the command is successfully validated,\n// check for matching of demand state against current state\nCompletableFuture<MatchingResponse> matchResponseF = hcdCmdService.onewayAndMatch(setupWithMatcher, demandMatcher);\n\nMatchingResponse actualResponse = matchResponseF.get();\nCompleted expectedResponse = new Completed(actualResponse.runId());\nAssert.assertEquals(expectedResponse, actualResponse);     // Not a great test for now\nOne important point is that the matcher is created and must be shutdown when you are finished with it using the stop method of the matcher as shown in the example.","title":"Matching State for Command Completion"},{"location":"/commons/command.html#onewayandmatch","text":"CommandService provides a short-cut called onewayAndMatch that combines a oneway and a matcher and implements much of the boilerplate of the previous example.\nScala val onewayMatchF = async {\n  await(assemblyCmdService.onewayAndMatch(setupWithMatcher, demandMatcher)) match {\n    case i: Invalid =>\n      // Command was not accepted\n      log.error(s\"Oneway match was not accepted: ${i.issue}\")\n      i\n    case c: Completed =>\n      // Do some completed work\n      c\n    case e: Error =>\n      // Match failed and timedout generating an error - log a message\n      println(\"Error\")\n      log.error(s\"Oeway match produced an error: ${e.message}\")\n      e\n    case l: Locked =>\n      // Destination component was locked, log a message\n      log.error(s\"Destination component was locked\")\n      l\n  }\n}\nAwait.result(onewayMatchF, timeout.duration) shouldBe a[Completed] Java // create a DemandMatcher which specifies the desired state to be matched.\nStateMatcher stateMatcher = new DemandMatcher(new DemandState(prefix(), new StateName(\"testStateName\")).add(param), false, timeout);\n\nCompletableFuture<MatchingResponse> matchedCommandResponseF =\n        hcdCmdService.onewayAndMatch(setupWithMatcher, stateMatcher);","title":"onewayAndMatch"},{"location":"/commons/logging_aggregator.html","text":"","title":"Logging Aggregator"},{"location":"/commons/logging_aggregator.html#logging-aggregator","text":"The logging aggregator provides aggregation of logs generated from TMT applications written in Scala, Java, Python, C, C++, modules like System logs, Redis logs, Postgres logs, ElasticSearch logs, Keycloak logs using the widely used Elastic ELK stack (Elasticsearch, Logstash, Kibana), and the Filebeat utility.\nNote The basic/elastic license of Elastic stack, which is a free license, should be sufficient for TMT’s purposes. To know more about what features are available in the basic license refer to this link.","title":"Logging Aggregator"},{"location":"/commons/logging_aggregator.html#architecture","text":"Logstash collects the JSON logs (e.g. from components) in string format, parses it to a valid JSON object and then feeds it to Elasticsearch. Elasticsearch is responsible for ingesting and indexing the JSON data. If logs are generated in non-JSON format, which will be the case for Redis, syslog, Postgres, and Elasticsearch logs, they will be parsed and indexed using the elastic modules.\nAfter the JSON data is indexed in Elasticearch, Kibana provides powerful visualization tools and dashboards that offer various interactive diagrams to visualize complex queries.\nFilebeat is responsible for watching log files and shipping them to the centralized Logstash component. As shown in the above architecture diagram, all machines in TMT can run Filebeat to watch log files. Filebeat maintains a marker in a registry for the last read position in a file, so that it can resume where it left off should it need to be restarted. Applications can keep emitting logs to the file agnostic to whether Filebeat is running or not.\nNote All the files required for logging aggregator configuration can be found here. How to use each of these files and its significance will be explained further in the document. For rapid use in a development environment, a Docker image has been created. See below for more information. It is assumed that in production, Elasticsearch, Logstash and Kibana will be registered with TMT intranet DNS setup. Hence, all the configuration files for production are provided by referring to the DNS host name. It is strongly recommended to run the same version (v6.6.0 or higher) of elastic components so as to avoid any compatibility issues. By default, the elastic stack exposes the following ports on which the configurations rely. 5044: Logstash TCP input. 9200: Elasticsearch HTTP port 5601: Kibana","title":"Architecture"},{"location":"/commons/logging_aggregator.html#logging-from-tmt-applications-scala-java-python-c-c-","text":"In order to aggregate logs generated from TMT apps, the Filebeat application is used to watch them. The recommended practice is for apps to generate log files at a common place so that Filebeat can find them. This common place is defined by an environment variable TMT_LOG_HOME, e.g. TMT_LOG_HOME = /<<user-accessible-space>>/tmt/logs/csw.\nNote For convenience during development, you may choose to use /tmp for TMT_LOG_HOME. However, for production, this variable should be set to something more permanent, since all the files in /tmp will be lost on machine reboot.\nThe upcoming sections will explain how each TMT application can generate log files at TMT_LOG_HOME and how Filebeat can read them:\nNote Things to keep in mind while writing C++/C/Python apps The structure of JSON logs should adhere to this format Log files should be generated at the path set by TMT_LOG_HOME Time should be logged in UTC It is recommended to use rotating files for logging Scala/Java applications using the CSW Logging framework follow these conventions out of the box.","title":"Logging from TMT Applications (Scala/Java/Python/C++/C)"},{"location":"/commons/logging_aggregator.html#scala-java","text":"For Scala/Java applications to dump logs in a file, it is important that developers enable the FileAppender in application.conf. To know more about how to configure FileAppender please refer to the logging documentation. Once, the FileAppender is enabled, the log files will be generated under TMT_LOG_HOME. If TMT_LOG_HOME is not set as an environment variable then a BaseLogPathNotDefined exception will be thrown. For tests, you can hard code a logging directory without setting the TMT_LOG_HOME environment variable by overriding the baseLogPath configuration setting in the logging configuration in your application.conf file.","title":"Scala/Java"},{"location":"/commons/logging_aggregator.html#c-","text":"For C++ developers, it is recommended to use the spdlog library along with spdlog-setup add-on library for logging in files. This add-on allows spdlog to be configured using a file instead of hardcoding it.\nThe following code snippet will explain how to use spdlog:\nmain.cpp #include <iostream>\n#include <cstdlib>\n#include <string>\n#include <spdlog/spdlog.h>\n#include <spdlog/sinks/basic_file_sink.h>\n#include <spdlog_setup/conf.h>\n\nusing namespace std;\n\n// take the path for storing logs from env var, if not set throw exception\nstring setFilePath()\n{\n    char* path;\n    try\n    {\n        if((path = getenv(\"TMT_LOG_HOME\")) == NULL)\n            throw new exception;\n    }\n    catch(...)\n    {\n        cerr<<\"Environment variable(TMT_LOG_HOME) not set.\"<<endl;\n    }\n    return path;\n}\n\n// set global pattern for all logs and note the json string format for generating logs\ninline void setGlobalPattern()\n{\n    spdlog::set_pattern(R\"({\"timestamp\":\"%Y-%m-%dT%H:%M:%S.%fZ\", \"logger\":\"%n\", \"@severity\":\"%l\", \"file\":\"%s\", \"line\":\"%#\", \"message\":\"%v\"})\",\n            spdlog::pattern_time_type::utc); // time should be in UTC\n}\n\nint main()\n{\n    string path = setFilePath();\n\n    const auto path_arg = fmt::arg(\"TMT_LOG_HOME\",path);\n    spdlog_setup::from_file_with_tag_replacement(\n        \"logging_default.toml\",\n        path_arg\n    );\n    auto file_logger = spdlog::get(\"root\");\n    spdlog::set_default_logger(file_logger);\n\n    setGlobalPattern();\n    SPDLOG_INFO(\"I am INFO level CPP log\");\n    SPDLOG_CRITICAL(\"I am CRITICAL CPP log\");\n    SPDLOG_DEBUG(\"I am DEBUG level CPP log {}\",\"message\");\n    SPDLOG_ERROR(\"I am an ERROR CPP log\");\n    SPDLOG_WARN(\"I am WARN level CPP log\");\n    SPDLOG_TRACE(\"I am TRACE level CPP log\");\n\n    spdlog::drop_all();\n}\n\n/*\n    Run the following commands on terminal to execute `main.cpp`\n\n    $   clang++ -std=c++17 main.cpp -o main\n    $   ./main\n*/ logging_default.toml # Various sinks can be created in the following way. More informations can be found at https://github.com/guangie88/spdlog_setup#toml-configuration-example\n[[sink]]\nname = \"basic_logger\"\ntype = \"basic_file_sink_st\"\n# Tagged based configuration. Programmatically determine path.\nfilename = \"{TMT_LOG_HOME}/spdlog.log\"\ncreate_parent_dir = true\n\n# Various loggers can be created\n[[logger]]\nname = \"root\"\nsinks = [\"basic_logger\"]\nlevel = \"trace\"\n\n# More examples of configuration files can be found at https://github.com/guangie88/spdlog_setup/tree/v0.3.0-alpha.2/config\nThe source code for above code can be found here\nNote spdlog-setup uses the .toml file format for its configuration files. Log levels could be specified in this configuration file so that they can be changed without re-compiling C/C++ code.","title":"C++"},{"location":"/commons/logging_aggregator.html#c","text":"For C developers, it is recommended to use zlog logging library. The following code snippet will explain how to use zlog:\nmain.c #include <stdio.h>\n#include <zlog.h>\n\nint main(int argc, char **argv)\n{\n\n    int rc;\n    zlog_category_t* cat;\n    rc = zlog_init(\"logging_default.conf\");\n\n    if (rc)\n    {\n        printf(\"Init failed. Either file name is incorrect or their is syntax error in configuration file.\\n\");\n        return -1;\n    }\n    cat = zlog_get_category(\"my_cat\");\n    zlog_info(cat,\"I am Info C Log\");\n    zlog_debug(cat,\"I am Debug C Log\");\n    zlog_fatal(cat,\"I am Fatal C Log\");\n    zlog_warn(cat,\"I am warn C Log\");\n    zlog_error(cat, \"I am Error C Log\");\n    zlog_fini();\n    return 0;\n}\n\n/*\n    Run the following commands on terminal to execute `main.c`\n\n   $ cc -c -o main.o main.c -I/usr/local/include\n   $ cc -o main main.o -L/usr/local/lib -lzlog\n   $ ./main\n*/ logging_default.conf # More information on configuration settings can be found http://hardysimpson.github.io/zlog/UsersGuide-EN.html#htoc14\n[global]\n# Highly recommended to keep the default format as-is for a meaningful logging aggregation and analysis\ndefault format = \"{\"timestamp\" : \"%d(%FT%T).%ms%d(%z)\", \"@severity\" : \"%V\", \"file\" : \"%F\", \"line\" : \"%L\", \"@host\" : \"%H\", \"message\" : \"%m\"} %n\"\n\n[rules]\n# Relies on environment variable TMT_LOG_HOME to be set otherwise there will be exception\n# *.* = category.severity For e.g., my_category.error\n*.* \"%E(TMT_LOG_HOME)/zlog.log\"\nThe source code for above code can be found here\nNote zlog uses a custom DSL for its configuration. See this for more information.","title":"C"},{"location":"/commons/logging_aggregator.html#python","text":"For python developers it is recommended to use the default python-logging module that comes with python. The following code snippet will explain how to use python-logging with python 3.7:\nmain.py from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom __future__ import absolute_import\n\nimport logging.config\nimport codecs\nimport json\nimport logging.config\nimport os\nimport pathlib\nimport tmt_formatter\n\ndef main():\n\n    logging.basicConfig(level=\"INFO\")\n    logger = logging.getLogger()\n    logger.info(\"This is the logger configured by `logging.basicConfig()`.\")\n\n    config_file = \"logging_default.json\"\n    with codecs.open(config_file, \"r\", encoding=\"utf-8\") as fd:\n        config = json.load(fd)\n\n\n    dirPath = os.environ.get(\"TMT_LOG_HOME\")\n    pathlib.Path(dirPath).mkdir(parents=True, exist_ok=True)\n    logPath = dirPath + \"/python.log\"\n    config[\"logging\"][\"handlers\"][\"file_handler\"][\"filename\"]=logPath\n    logging.config.dictConfig(config[\"logging\"])\n\n    # each time we need to log something we can create a logger object\n    # The operation of creating a logger should be quite cheap.\n    # getLogger() without arguments returns the \"root\" logger.\n    logger = logging.getLogger()\n    logger.info(\"This is an INFO message on the root logger.\")\n    logger.debug(\"This is an INFO message on the root logger.\")\n\n    # If we need to separate things, we can always create child loggers:\n    child = logging.getLogger().getChild(\"child\")\n    child.warning(\"This is a WARNING message on the child logger.\")\n\n    # let's create an error. This will send an email\n    child.error(\"This is an ERROR message.\")\n\nmain() logging_default.json {\n  \"logging\": {\n    \"version\": 1,\n    \"disable_existing_loggers\": true,\n    \"formatters\": {\n      \"single-line\": {\n        \"class\": \"logging.Formatter\",\n        \"datefmt\": \"%I:%M:%S\",\n        \"format\": \"%(levelname)-8s; %(asctime)s; %(name)-15s; %(module)s:%(funcName)s;%(lineno)d: %(message)s\"\n      },\n      \"verbose\": {\n        \"class\": \"tmt_formatter.UTCFormatter\",\n        \"datefmt\": \"%Y-%m-%dT%H:%M:%SZ\",\n        \"format\": \"{\\\"@name\\\":\\\"%(name)s\\\", \\\"@severity\\\":\\\"%(levelname)s\\\", \\\"message\\\": \\\"%(message)s\\\", \\\"file\\\": \\\"%(filename)s\\\", \\\"line\\\": \\\"%(lineno)d\\\", \\\"process\\\": \\\"%(processName)s\\\", \\\"class\\\": \\\"%(pathname)s\\\", \\\"timestamp\\\": \\\"%(asctime)s\\\"}\"\n      }\n    },\n    \"handlers\": {\n      \"console\":{\n        \"level\": \"DEBUG\",\n        \"class\": \"logging.StreamHandler\",\n        \"formatter\": \"single-line\",\n        \"stream\" : \"ext://sys.stdout\"\n      },\n      \"file_handler\": {\n        \"level\": \"INFO\",\n        \"class\": \"logging.handlers.RotatingFileHandler\",\n        \"formatter\": \"verbose\",\n        \"filename\": \"/tmp/file_handler.log\",\n        \"mode\": \"a\",\n        \"encoding\": \"utf-8\"\n      }\n    },\n    \"loggers\": { },\n    \"root\": {\n      \"handlers\": [\"console\", \"file_handler\"],\n      \"level\": \"DEBUG\"\n    }\n  }\n} tmt_formatter.py import logging\nimport time\n\nclass UTCFormatter(logging.Formatter):\n    converter = time.gmtime\nThe source code for above code can be found here\nNote The above example shows how tmt_formatter.py in logging_default.json is used to log with UTC timestamps.\nNote YAML can be used instead of JSON, as well as the simple INI format used in previous versions of python (although with less flexibility). See the python documentation for more information.","title":"Python"},{"location":"/commons/logging_aggregator.html#setting-up-filebeat-to-watch-tmt-app-logs-and-system-generated-logs","text":"Once TMT applications generate log files under TMT_LOG_HOME, Filebeat needs to start watching them. In order for Filebeat to be aware of TMT_LOG_HOME, filebeat.yml should be used to start Filebeat.\nAll machines running TMT applications also need system generated logs to be watched by Filebeat so that it gets shipped to Logstash. This can be achieved by enabling the System module in Filebeat and making Elasticsearch aware of receiving system logs (text based logs) to parse and index them.\nIn order to achieve this, follow the steps given below:\nRun Elasticsearch using elasticsearch.yml. Place elasticsearch.yml in «Elasticsearch installation folder»/config/ and execute ./bin/elasticsearch (on Mac) or bin/elasticsearch (on Linux). Run LogStash using logstash.conf. Place logstash.conf in «Logstash installation folder»/config/ and execute ./bin/logstash -f config/logstash.conf (on mac) or bin/logstash -f config/logstash.conf (on Linux). Place filebeat.yml in «Filebeat installation folder» and execute filebeat-init.sh from «Filebeat installation folder». This will make Elasticsearch aware of System module. Run Filebeat from «Filebeat installation folder» by executing ./filebeat -e (on Mac) or filebeat -e (on Linux). This will enable system module for watching system logs from its default path i.e. /var/log/system.log as well as watching log files under TMT_LOG_HOME. Run Kibana using kibana.yml. Place kibana.yml in «Kibana installation folder»/config/ and execute ./bin/kibana (on mac) or bin/kibana (on Linux). This will give GUI over logs aggregated in Elasticsearch.\nNote It is assumed that in production, Elasticsearch, Logstash and Kibana will be registered with TMT intranet DNS setup. Hence, all the configuration files for production are provided by referring to DNS host name.","title":"Setting up Filebeat to watch TMT app logs and system generated logs"},{"location":"/commons/logging_aggregator.html#redis-logs","text":"In production, Redis will be started in sentinel mode, master mode and slave mode on different machines for Event and Alarm service. Configuration files are provided for Redis sentinel, master and slave to log in file /var/log/redis/redis-server.log (user needs to uncomment logfile path in conf files). Filebeat will also be watching this file once the Redis module is enabled.\nNote that system generated logs on Redis machines also needs to be watched by Filebeat and aggregated. In order to enable Redis and System module follow the below given steps:\nRun Elasticsearch using elasticsearch.yml. Place elasticsearch.yml in «Elasticsearch installation folder»/config/ and execute ./bin/elasticsearch (on Mac) or bin/elasticsearch (on Linux). Run LogStash using logstash.conf. Place logstash.conf in «Logstash installation folder»/config/ and execute ./bin/logstash -f config/logstash.conf (on mac) or bin/logstash -f config/logstash.conf (on Linux). Place filebeat.yml in «Filebeat installation folder» and execute filebeat-init.sh from «Filebeat installation folder». This will make Elasticsearch aware of Redis and System module. Run Filebeat from «Filebeat installation folder» by executing ./filebeat -e (on Mac) or filebeat -e (on Linux). Run Kibana using kibana.yml. Place kibana.yml in «Kibana installation folder»/config/ and execute ./bin/kibana (on mac) or bin/kibana (on Linux). This will give GUI over logs aggregated in Elasticsearch.","title":"Redis logs"},{"location":"/commons/logging_aggregator.html#postgres-logs","text":"Logs generated by Postgres needs to be watched by Filebeat and aggregated. Hence, use postgres.conf to start the PostgreSQL server which will enable logging in postgres to its default location /usr/local/var/postgres and use UTC time for logging.\nNote that system generated logs on Postgres machine also needs to be watched by Filebeat and aggregated. In order to enable Postgres and System module follow the below given steps:\nRun Elasticsearch using elasticsearch.yml. Place elasticsearch.yml in «Elasticsearch installation folder»/config/ and execute ./bin/elasticsearch (on Mac) or bin/elasticsearch (on Linux). Run LogStash using logstash.conf. Place logstash.conf in «Logstash installation folder»/config/ and execute ./bin/logstash -f config/logstash.conf (on mac) or bin/logstash -f config/logstash.conf (on Linux). Place filebeat.yml in «Filebeat installation folder» and execute filebeat-init.sh from «Filebeat installation folder». This will make Elasticsearch aware of Postgres and System module. Run Filebeat from «Filebeat installation folder» by executing ./filebeat -e (on Mac) or filebeat -e (on Linux). Run Kibana using kibana.yml. Place kibana.yml in «Kibana installation folder»/config/ and execute ./bin/kibana (on mac) or bin/kibana (on Linux). This will give GUI over logs aggregated in Elasticsearch.","title":"Postgres logs"},{"location":"/commons/logging_aggregator.html#elasticsearch-logs","text":"It is important to also aggregate logs generated by Elasticsearch. There can be situations where indexing generates error and it will be useful to have those errors aggregated and viewed in Kibana. Hence, use elaticsearch.yml to start Elasticsearch which will configure log file location under TMT_LOG_HOME.\nNote that system generated logs on Elasticsearch machine also needs to be watched by Filebeat and aggregated. In order to enable Elasticsearch and System module follow the below given steps:\nRun Elasticsearch using elasticsearch.yml. Place elasticsearch.yml in «Elasticsearch installation folder»/config/ and execute ./bin/elasticsearch (on Mac) or bin/elasticsearch (on Linux). Run LogStash using logstash.conf. Place logstash.conf in «Logstash installation folder»/config/ and execute ./bin/logstash -f config/logstash.conf (on mac) or bin/logstash -f config/logstash.conf (on Linux). Place filebeat.yml in «Filebeat installation folder» and execute filebeat-init.sh from «Filebeat installation folder». This will make Elasticsearch aware of it’s own module and System module. Run Filebeat from «Filebeat installation folder» by executing ./filebeat -e (on Mac) or filebeat -e (on Linux). This will enable elasticsearch and system module. Run Kibana using kibana.yml. Place kibana.yml in «Kibana installation folder»/config/ and execute ./bin/kibana (on mac) or bin/kibana (on Linux). This will give GUI over logs aggregated in Elasticsearch.","title":"Elasticsearch logs"},{"location":"/commons/logging_aggregator.html#keycloak-logs","text":"Logs generated from Keycloak needs to be watched by Filebeat and aggregated. Hence, use standalone.xml to start JBoss server which will make JBoss server to log in JSON format, enable keycloak logs, and generate log files under TMT_LOG_HOME.\nNote that system generated logs on Keycloak machine also needs to be watched by Filebeat and aggregated. In order to watch Keycloak logs from TMT_LOG_HOME and enable system module, refer to the steps from here.","title":"Keycloak logs"},{"location":"/commons/logging_aggregator.html#system-logs-on-logstash-and-kibana-machines","text":"Machines running Logstash and Kibana will require to aggregate system logs from their machines too. Hence, in order to enable system module on their machines follow the below given steps:\nRun Elasticsearch using elasticsearch.yml. Place elasticsearch.yml in «Elasticsearch installation folder»/config/ and execute ./bin/elasticsearch (on Mac) or bin/elasticsearch (on Linux). Go to «Filebeat installation folder» place Logstash/filebeat.yml and execute Logstash/filebeat-init.sh or place Kibana/filebeat.yml and execute Kibana/filebeat-init.sh from Logstash or Kibana machines respectively. This will make Elasticsearch aware of System module. Run Filebeat from «Filebeat installation folder» by executing ./filebeat -e (on Mac) or filebeat -e (on Linux). Run LogStash using logstash.conf. Place logstash.conf in «Logstash installation folder»/config/ and execute ./bin/logstash -f config/logstash.conf (on mac) or bin/logstash -f config/logstash.conf (on Linux). Run Kibana using kibana.yml. Place kibana.yml in «Kibana installation folder»/config/ and execute ./bin/kibana (on mac) or bin/kibana (on Linux). This will give GUI over logs aggregated in Elasticsearch.","title":"System logs on Logstash and Kibana machines"},{"location":"/commons/logging_aggregator.html#explore-kibana","text":"Once Kibana is up and running, open http://localhost:5601/ in a browser and go to:\nManagement -> Kibana -> Index Patterns and create an index pattern as per the requirement. Discover -> Select the index pattern created and explore\nFor Modules like System, Postgres, Redis and Elasticsearch go to Dashboard and explore.","title":"Explore Kibana"},{"location":"/commons/logging_aggregator.html#running-elastic-stack-for-developers","text":"For development purposes Docker compose is used to start Elasticsearch, Logstash, Kibana and Filebeat in a container. Hence, make sure that latest Docker setup is installed and running before starting the Elastic stack. The Docker container is responsible for aggregating logs generated in tmp/tmt/logs/csw. Hence, developers writing Scala/Java/Python/C++/C applications need to generate log files under /tmp/tmt/logs/csw.\nAlso, note that csw apps started via csw-services.sh will generate log files under /tmp/tmt/logs/csw and thus, it will be aggregated by the Elastic docker container. To know more about setting up docker and starting Elastic, please refer to Starting Elastic logging aggregator for Development.\nNote By default, Elasticsearch will run in a read-only mode if the disk it is using is more than 90% full. This can be configured. See the Elasticsearch reference documentation","title":"Running Elastic Stack for Developers"},{"location":"/commons/services.html","text":"","title":"Services"},{"location":"/commons/services.html#services","text":"Location Service Configuration Service Logging Service Event Service Alarm Service Time Service Database Service Authentication and Authorization Service (AAS) Sequencer Command Service","title":"Services"},{"location":"/services/location.html","text":"","title":"Location Service"},{"location":"/services/location.html#location-service","text":"The Location Service handles component (i.e., Applications, Sequencers, Assemblies, HCDs, and Services) registration and discovery in the distributed TMT software system.\nThe CSW Location Service cluster must be running, and appropriate environment variables set to run apps. See CSW Location Server.\nA component’s location information can be used by other components and services to connect to it and use it. An example of location information is:\nhost address/port pairs URL/URIs paths connection protocols","title":"Location Service"},{"location":"/services/location.html#dependencies","text":"To use the Location Service without using the framework, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-location-server\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/location.html#create-location-service","text":"Note that before using this API, the csw-location-server application should be running at a known location in the network (or at multiple locations) and the necessary configuration, environment variables or system properties should be defined to point to the correct host and port number(s).\nLocationServiceFactory provides a make method to create an instance of the LocationService API. This call will look for configuration or environment variable settings.\nScala private val locationService = HttpLocationServiceFactory.makeLocalClient(typedSystem) Java private akka.actor.ActorSystem system = context().system();\nprivate ILocationService locationService = JHttpLocationServiceFactory.makeLocalClient(Adapter.toTyped(system));","title":"Create Location Service"},{"location":"/services/location.html#creating-components-connections-and-registrations","text":"An Application, Sequencer, Assembly, HCD, or Service may need to be used by another component as part of normal observatory operations. It must register its location information with Location Service so that other components can find it. Location information is comprised of:\nComponentId : A component ID consisting of ComponentName : a name describing the component. ComponentType : such as Container, Sequencer, HCD, Assembly, Service. ConnectionType : the means to reach components. These are categorized as Akka, HTTP, or Tcp type connections.\nThe location information is stored in the Location Service as Registrations.\nSome of the examples of the string representation of a connection are:\nTromboneAssembly-assembly-akka TromboneHcd-hcd-akka ConfigServer-service-http EventService-service-tcp\nThe register API takes a Registration parameter and returns a future registration result. If registration fails for some reason, the returned future will fail with an exception. (Registration will fail if the csw-location-server application is not running or could not be found, or if the given component name was already registered.)\nThe following example shows registration of both an UnTyped ActorRef and a Typed ActorRef:\nScala // add some dummy registrations for illustrative purposes\n\n// dummy http connection\nval httpPort                          = 8080\nval httpConnection                    = HttpConnection(api.models.ComponentId(Prefix(Subsystem.CSW, \"configuration\"), ComponentType.Service))\nval httpRegistration                  = HttpRegistration(httpConnection, httpPort, \"path123\")\nval httpRegResult: RegistrationResult = Await.result(locationService.register(httpRegistration), 2.seconds)\n\n// ************************************************************************************************************\n\n// import scaladsl adapter to implicitly convert UnTyped ActorRefs to Typed ActorRef[Nothing]\nimport akka.actor.typed.scaladsl.adapter._\n\n// dummy HCD connection\nval hcdConnection = AkkaConnection(api.models.ComponentId(Prefix(Subsystem.NFIRAOS, \"hcd1\"), ComponentType.HCD))\nval hcdRegistration: AkkaRegistration = AkkaRegistrationFactory.make(\n  hcdConnection,\n  context\n    .actorOf(Props(new Actor {\n      override def receive: Receive = {\n        case \"print\" => log.info(\"hello world\")\n      }\n    }), name = \"my-actor-1\")\n    .toTyped\n    .toURI\n)\n\n// Register UnTyped ActorRef with Location service. Import scaladsl adapter to implicitly convert\n// UnTyped ActorRefs to Typed ActorRef[Nothing]\nval hcdRegResult: RegistrationResult = Await.result(locationService.register(hcdRegistration), 2.seconds)\n\n// ************************************************************************************************************\n\ndef behavior(): Behavior[String] = Behaviors.setup { ctx =>\n  Behaviors.receiveMessage { msg =>\n    Behaviors.same\n  }\n}\nval typedActorRef: ActorRef[String] = context.system.spawn(behavior(), \"typed-actor-ref\")\n\nval assemblyConnection = AkkaConnection(ComponentId(Prefix(Subsystem.NFIRAOS, \"assembly1\"), ComponentType.Assembly))\n\n// Register Typed ActorRef[String] with Location Service\nval assemblyRegistration: AkkaRegistration =\n  AkkaRegistrationFactory.make(assemblyConnection, typedActorRef.toURI)\n\nval assemblyRegResult: RegistrationResult = Await.result(locationService.register(assemblyRegistration), 2.seconds) Java // dummy http connection\nHttpConnection httpConnection = new HttpConnection(new ComponentId(new Prefix(JSubsystem.CSW, \"configuration\"), JComponentType.Service));\nHttpRegistration httpRegistration = new HttpRegistration(httpConnection, 8080, \"path123\");\nhttpRegResult = locationService.register(httpRegistration).get();\n\n// ************************************************************************************************************\n\n// dummy HCD connection\nAkkaConnection hcdConnection = new AkkaConnection(new ComponentId(new Prefix(JSubsystem.NFIRAOS, \"hcd1\"), JComponentType.HCD));\nActorRef actorRef = getContext().actorOf(Props.create(AbstractActor.class, () -> new AbstractActor() {\n            @Override\n            public Receive createReceive() {\n                return ReceiveBuilder.create().build();\n            }\n        }),\n        \"my-actor-1\"\n);\n\n// Register UnTyped ActorRef with Location service. Use javadsl Adapter to convert UnTyped ActorRefs\n// to Typed ActorRef[Nothing]\n\nURI actorRefURI = ActorExtension.RichActor(Adapter.toTyped(actorRef)).toURI();\nAkkaRegistration hcdRegistration = csw.location.api.AkkaRegistrationFactory.make(hcdConnection, actorRefURI);\nhcdRegResult = locationService.register(hcdRegistration).get();\n\n// ************************************************************************************************************\n\nBehavior<String> behavior = Behaviors.setup(ctx -> Behaviors.same());\nakka.actor.typed.ActorRef<String> typedActorRef = Adapter.spawn(context(), behavior, \"typed-actor-ref\");\n\nAkkaConnection assemblyConnection = new AkkaConnection(new ComponentId(new Prefix(JSubsystem.NFIRAOS, \"assembly1\"), JComponentType.Assembly));\n\n// Register Typed ActorRef[String] with Location Service\nAkkaRegistration assemblyRegistration = new RegistrationFactory().akkaTyped(assemblyConnection, typedActorRef);\n\n\nassemblyRegResult = locationService.register(assemblyRegistration).get();\nNote The AkkaRegistration api takes only Typed ActorRefs. Hence, to register an UnTyped ActorRef for an akka connection, it needs to be adapted to Typed ActorRef[Nothing], using adapters provided by Akka. The usage of the adapter is shown in the above snippet for both Scala and Java. Also, note that for components, the registration will be taken care of via csw-framework. Hence, component developers won’t register any connections during their development. So, the above demonstration of registering connections is for explanatory and testing purposes only.","title":"Creating Components, Connections and Registrations"},{"location":"/services/location.html#creating-actorref-for-registration","text":"The ActorSystem used to start actors that will be registered in the Location Service must be created using an ActorSystemFactory as follows:\nScala implicit val typedSystem: typed.ActorSystem[SpawnProtocol.Command] =\n  ActorSystemFactory.remote(SpawnProtocol(), \"csw-examples-locationServiceClient\") Java ActorSystem<SpawnProtocol.Command> typedSystem = ActorSystemFactory.remote(SpawnProtocol.create(), \"csw-examples-locationServiceClient\");\nThis is required to start a remote ActorSystem on the same network interface where the csw-cluster is running. All the ActorRefs created using this ActorSystem will be available for communication from other components that are part of the CSW Cluster.","title":"Creating ActorRef for Registration"},{"location":"/services/location.html#resolving-connections","text":"The list API returns a list of the currently registered connections from the Location Service.\nA connection of interest can be looked up using the resolve or find methods:\nresolve gets the location for a connection from the local cache. If not found in the cache, it waits for the event to arrive within the specified time limit and returns None on failure.\nfind returns the location for a connection from the local cache and returns None if not immediately found there.\nScala // find connection to LocationServiceExampleComponent in location service\n// [do this before starting LocationServiceExampleComponent.  this should return Future[None]]\nval exampleConnection: AkkaConnection = LocationServiceExampleComponent.connection\n\nlog.info(\n  s\"Attempting to find $exampleConnection\",\n  Map(Keys.OBS_ID -> \"foo_obs_id\", \"exampleConnection\" -> exampleConnection.name)\n)\nval findResult: Option[AkkaLocation] = Await.result(locationService.find(exampleConnection), timeout)\n\nlog.info(s\"Result of the find call: $findResult\") Java // find connection to LocationServiceExampleComponent in location service\n// [do this before starting LocationServiceExampleComponent.  this should return Future[None]]\n\nlog.info(\"Attempting to find \" + exampleConnection,\n        Map.of(\n                JKeys.OBS_ID, \"foo_obs_id\",\n                \"exampleConnection\", exampleConnection.name()\n        ));\n\nOptional<AkkaLocation> findResult = locationService.find(exampleConnection).get();\nif (findResult.isPresent()) {\n    log.info(\"Find result: \" + connectionInfo(findResult.orElseThrow().connection()));\n} else {\n    log.info(() -> \"Result of the find call : None\");\n}\nThe logging output from the above example when the given component is not registered should include:\n[INFO] Attempting to find connection AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly))\n[INFO] Result of the find call: None\nAn example of the resolve command is shown in the following:\nScala // resolve connection to LocationServiceExampleComponent\n// [start LocationServiceExampleComponent after this command but before timeout]\nlog.info(s\"Attempting to resolve $exampleConnection with a wait of $waitForResolveLimit ...\")\n\nval resolveResultF: Future[Option[AkkaLocation]] = locationService.resolve(exampleConnection, waitForResolveLimit)\nval resolveResult: Option[AkkaLocation]          = Await.result(resolveResultF, waitForResolveLimit + timeout)\nresolveResult match {\n  case Some(result) =>\n    log.info(s\"Resolve result: ${locationInfoToString(result)}\")\n  case None =>\n    log.info(s\"Timeout waiting for location $exampleConnection to resolve.\")\n} Java // resolve connection to LocationServiceExampleComponent\n// [start LocationServiceExampleComponent after this command but before timeout]\nDuration waitForResolveLimit = Duration.ofSeconds(30);\n\nlog.info(() -> \"Attempting to resolve \" + exampleConnection + \" with a wait of \" + waitForResolveLimit + \"...\", () -> Map.of(\n        JKeys.OBS_ID, \"foo_obs_id\",\n        \"exampleConnection\", exampleConnection.name()\n));\n\nOptional<AkkaLocation> resolveResult = locationService.resolve(exampleConnection, waitForResolveLimit).get();\nif (resolveResult.isPresent()) {\n    log.info(() -> \"Resolve result: \" + connectionInfo(resolveResult.orElseThrow().connection()));\n} else {\n    log.info(() -> \"Timeout waiting for location \" + exampleConnection + \" to resolve.\");\n}\nThe logging output from the above example should include:\n[INFO] Attempting to resolve AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly)) with a wait of 30 seconds ...\nIf you then start the LocationServiceExampleComponentApp, the following message will be logged:\n[INFO] Resolve result: LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\nIf not, eventually the operation will timeout and the output should read:\n[INFO] Timeout waiting for location AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly)) to resolve.\nNote The resolve and find api returns the concrete Location type i.e. Akkalocation, HttpLocation or TcpLocation as demonstrated in this section. Once the Akka location is found or resolved, we need to ascribe the type to the ActorRef, since the explicit type annotation is removed from the program before it is executed at run-time (see type erasure). Use following AkkaLocation API to get the correct Typed ActorRef: Scala // If the component type is HCD or Assembly, use this to get the correct ActorRef\nval typedComponentRef: ActorRef[ComponentMessage] = akkaLocation.componentRef\n\n// If the component type is Container, use this to get the correct ActorRef\nval typedContainerRef: ActorRef[ContainerMessage] = akkaLocation.containerRef Java // If the component type is HCD or Assembly, use this to get the correct ActorRef\nakka.actor.typed.ActorRef<ComponentMessage> typedComponentRef = AkkaLocationExt.RichAkkaLocation(akkaLocation).componentRef(typedSystem);\n\n// If the component type is Container, use this to get the correct ActorRef\nakka.actor.typed.ActorRef<ContainerMessage> typedContainerRef = AkkaLocationExt.RichAkkaLocation(akkaLocation).containerRef(typedSystem);","title":"Resolving Connections"},{"location":"/services/location.html#filtering","text":"The list API and its variants offer means to inquire about available connections with the Location Service. The parameter-less list returns all available connections\nScala // list connections in location service\nval connectionList: List[Location] = Await.result(locationService.list, timeout)\nlog.info(\"All Registered Connections:\")\nconnectionList.foreach(c => log.info(s\"--- ${locationInfoToString(c)}\")) Java // list connections in location service\nList<Location> connectionList = locationService.list().get();\nlog.info(\"All Registered Connections:\");\nfor (Location loc : connectionList) {\n    log.info(\"--- \" + connectionInfo(loc.connection()));\n}\nThe log output from the above should contain:\n[INFO] All Registered Connections:\n[INFO] --- configuration-service-http, component type=Service, connection type=HttpType\n[INFO] --- hcd1-hcd-akka, component type=HCD, connection type=AkkaType\n[INFO] --- LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\n[INFO] --- redis-service-tcp, component type=Service, connection type=TcpType\n[INFO] --- assembly1-assembly-akka, component type=Assembly, connection type=AkkaType\nOther variants are filters using ConnectionType, ComponentType, and hostname.\nFiltering by component type is shown below:\nScala // filter connections based on component type\nval componentList: List[Location] = Await.result(locationService.list(ComponentType.Assembly), timeout)\nlog.info(\"Registered Assemblies:\")\ncomponentList.foreach(c => log.info(s\"--- ${locationInfoToString(c)}\")) Java // filter connections based on component type\nList<Location> componentList = locationService.list(JComponentType.Assembly).get();\nlog.info(\"Registered Assemblies:\");\nfor (Location loc : componentList) {\n    log.info(\"--- \" + connectionInfo(loc.connection()));\n}\nThe log output from the above code should contain:\n[INFO] Registered Assemblies:\n[INFO] --- LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\n[INFO] --- assembly1-assembly-akka, component type=Assembly, connection type=AkkaType\nFiltering by connection type is shown below:\nScala // filter connections based on connection type\nval akkaList: List[Location] = Await.result(locationService.list(ConnectionType.AkkaType), timeout)\nlog.info(\"Registered Akka connections:\")\nakkaList.foreach(c => log.info(s\"--- ${locationInfoToString(c)}\")) Java // filter connections based on connection type\nList<Location> akkaList = locationService.list(JConnectionType.AkkaType).get();\nlog.info(\"Registered Akka connections:\");\nfor (Location loc : akkaList) {\n    log.info(\"--- \" + connectionInfo(loc.connection()));\n}\nThe log output should contain:\n[INFO] Registered Akka connections:\n[INFO] --- hcd1-hcd-akka, component type=HCD, connection type=AkkaType\n[INFO] --- LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\n[INFO] --- assembly1-assembly-akka, component type=Assembly, connection type=AkkaType\nFiltering akka connections by prefix is shown below:\nScala // filter akka locations based on prefix\nval akkaLocations: List[Location] = Await.result(locationService.listByPrefix(\"NFIRAOS.ncc\"), timeout)\nlog.info(\"Registered akka locations for nfiraos.ncc\")\nakkaLocations.foreach(c => log.info(s\"--- ${locationInfoToString(c)}\")) Java List<Location> locations = locationService.listByPrefix(\"NFIRAOS.ncc\").get();\nlog.info(\"Registered akka locations for nfiraos.ncc\");\nfor (Location loc : locations) {\n    log.info(\"--- \" + connectionInfo(loc.connection()));\n}\nThe log output should contain:\n[INFO] Registered akka locations for nfiraos.ncc\n[INFO] --- nfiraos.ncc.hcd1-hcd-akka, component type=HCD, connection type=AkkaType\n[INFO] --- nfiraos.ncc.assembly1-assembly-akka, component type=Assembly, connection type=AkkaType","title":"Filtering"},{"location":"/services/location.html#tracking-and-subscribing","text":"The lifecycle of a connection of interest can be followed using either the track API or the subscribe API.\nThese methods take a Connection instance as a parameter. A Connection need not already be registered with Location Service. It’s okay to track connections that will be registered in future.\nThe track API returns two values: * A source that will emit a stream of TrackingEvents for the connection. * A KillSwitch to turn off the stream when no longer needed.\nThe Akka stream API provides many building blocks to process this stream, such as Flow and Sink. In the example below, Sink.actorRef is used to forward any location messages received to the current actor (self).\nA consumer can shut down the stream using the KillSwitch.\nThe subscribe API allows the caller to track a connection and receive the TrackingEvent notifications via a callback.\nThe API expects following parameters: * An existing connection or a connection to be registered in the future. * A callback that implements Consumer and receives the TrackEvent as a parameter.\nNote Callbacks are not thread-safe on the JVM. If you are doing side effects/mutations inside the callback, you should ensure that it is done in a thread-safe way inside an actor. Here is an example of how it can be done.\nThis API returns a KillSwitch that can be used to turn off the event notifications and release the supplied callback, if required.\nScala def sinkBehavior: Behaviors.Receive[ExampleMessages] = Behaviors.receive[ExampleMessages] { (ctx, msg) =>\n  {\n    val log: Logger = new LoggerFactory(Prefix(\"csw.my-component-name\")).getLogger(ctx)\n\n    msg match {\n      case TrackingEventAdapter(LocationUpdated(loc)) => log.info(s\"Location updated ${locationInfoToString(loc)}\")\n      case TrackingEventAdapter(LocationRemoved(conn)) =>\n        log.warn(s\"Location removed $conn\", Map(\"connection\" -> conn.toString))\n      case AllDone(exampleConnection) => log.info(s\"Tracking of $exampleConnection complete.\")\n      case CustomException(throwable) => log.error(throwable.getMessage, ex = throwable)\n    }\n    Behaviors.same\n  }\n}\n  // the following two methods are examples of two ways to track a connection.\n  // both are implemented but only one is really needed.\n\n  // Method1: track connection to LocationServiceExampleComponent\n  // Calls track method for example connection and forwards location messages to this actor\n  //\n  log.info(s\"Starting to track $exampleConnection\")\n  val sinfActorRef = typedSystem.spawn(LocationServiceExampleClient.sinkBehavior, \"\")\n  locationService\n    .track(exampleConnection)\n    .map(TrackingEventAdapter)\n    .to(ActorSink.actorRef[ExampleMessages](sinfActorRef, AllDone(exampleConnection), CustomException))\n    .run()\n  //track returns a Killswitch, that can be used to turn off notifications arbitarily\n  //in this case track a connection for 5 seconds, after that schedule switching off the stream\n  val killswitch = locationService\n    .track(httpConnection)\n    .toMat(Sink.foreach(println))(Keep.left)\n    .run()\n  context.system.scheduler.scheduleOnce(5.seconds) {\n    killswitch.cancel()\n  }\n\n  // Method2: subscribe to LocationServiceExampleComponent events\n  log.info(s\"Starting a subscription to $exampleConnection\")\n  locationService.subscribe(\n    exampleConnection,\n    trackingEvent => {\n      // the following println is to distinguish subscription events from tracking events\n      log.info(\"subscription event\")\n      self ! trackingEvent\n    }\n  ) Java // the following two methods are examples of two ways to track a connection.\n// both are implemented but only one is really needed.\n\n// track connection to LocationServiceExampleComponent\n// Calls track method for example connection and forwards location messages to this actor\nlog.info(\"Starting to track \" + exampleConnection);\nlocationService.track(exampleConnection).toMat(Sink.actorRef(getSelf(), AllDone.class), Keep.both()).run(typedSystem);\n\n//track returns a Killswitch, that can be used to turn off notifications arbitarily\n//in this case track a connection for 5 seconds, after that schedule switching off the stream\nPair pair = locationService.track(exampleConnection).toMat(Sink.ignore(), Keep.both()).run(typedSystem);\ncontext().system().scheduler().scheduleOnce(\n        Duration.ofSeconds(5),\n        () -> ((KillSwitch) pair.first()).shutdown(),\n        context().system().dispatcher());\n\n// subscribe to LocationServiceExampleComponent events\nlog.info(\"Starting a subscription to \" + exampleConnection);\nlocationService.subscribe(exampleConnection, trackingEvent -> {\n    log.info(\"subscription event\");\n    getSelf().tell(trackingEvent, ActorRef.noSender());\n});\nThe log output should contain the following:\n[INFO] Starting to track AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly)) LocationUpdated(HttpLocation(HttpConnection(ComponentId(configuration,Service)),http://131.215.210.170:8080/path123))\n[INFO] Starting a subscription to AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly))\n[INFO] Subscribing to connection LocationServiceExampleComponent-assembly-akka\n[INFO] Location updated LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\n[INFO] subscription event\n[INFO] Location updated LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\nIf you now stop the LocationServiceExampleComponentApp, it would log:\n[INFO] subscription event\n[INFO] Location removed AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly))\n[INFO] Location removed AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly))\nIf you start the LocationServiceExampleComponentApp again, the log output should be:\n[INFO] subscription event\n[INFO] Location updated LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\n[INFO] Location updated LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\nNote: the line after the words “subscription event” in our example is generated by the subscription, and the other line is from tracking. These two events could come in any order.","title":"Tracking and Subscribing"},{"location":"/services/location.html#unregistering","text":"One of the ways to unregister a service is by calling unregister on registration result received from register API.\nScala val unregisterF = async {\n  httpRegResult.unregister()\n  hcdRegResult.unregister()\n  assemblyRegResult.unregister()\n}\nAwait.result(unregisterF, 5.seconds) Java httpRegResult.unregister();\nhcdRegResult.unregister();\nassemblyRegResult.unregister();","title":"Unregistering"},{"location":"/services/location.html#technical-description","text":"See Location Service Technical Description.","title":"Technical Description"},{"location":"/services/location.html#source-code-for-examples","text":"Scala Example JavaBlocking Example","title":"Source code for examples"},{"location":"/services/config.html","text":"","title":"Configuration Service"},{"location":"/services/config.html#configuration-service","text":"The Configuration Service provides a centralized persistent store for any configuration file used in the TMT Software System. All versions of configuration files are retained, providing a historical record of each configuration file.\nNote that in order to use the APIs described here, the Location Service (csw-location-server) and Configuration Service Server needs to be running somewhere in the local network and the necessary configuration, environment variables or system properties should be defined to point to the correct host and port number(s) for the Location Service nodes.\nThis service will be part of the observatory cluster and exposes Rest endpoints that can be accessed over HTTP. Component developers can use the csw-config-client library in their code. The library wraps the low level communication with Configuration Service Server and exposes simple to use methods to access and manage configuration files.","title":"Configuration Service"},{"location":"/services/config.html#dependencies","text":"To use the Configuration Service without using the framework, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-config-client\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/config.html#rules-and-checks","text":"The config file path must not contain !#<>$%&'@^``~+,;= or any whitespace character If the input file is > 10MB or has lot of non ASCII characters, then for optimization, server will archive it in annex store. Large and binary files can be forced to go to the ‘annex’ store by using a annex=true flag in the create operation. API functions accept date-time values in UTC timezone. (e.g. 2017-05-17T08:00:24.246Z)","title":"Rules and Checks"},{"location":"/services/config.html#model-classes","text":"ConfigData : Represents the contents of the files being managed. It wraps a stream of ByteString. ConfigFileInfo : Represents information about a config file stored in the Config Service. ConfigFileRevision : Represents information about a specific version of a config file. ConfigId : Represents an identifier associated with a revision of a configuration file, often generated by create or update methods. ConfigMetadata : Represents metadata information about the Config Server. FileType : Represents the type of storage for a configuration file. Currently two types are supported Normal(small, text files) and Annex(Large, Binary files).","title":"Model Classes"},{"location":"/services/config.html#api-flavors","text":"The Configuration Service is used to provide the runtime settings for components. When a component is started, it will use a limited “clientAPI” to obtain the “active” configuration from the Configuration Service, and use those settings for its execution.\nTo change the active configuration, an administrative tool with access to the full “admin API” must be used. These tools would have the ability to create, delete, and update configurations, as well as retrieve past configurations and their history. Any time a new configuration is to be used by a component, the user must use one of these tools (via CLI, perhaps) to set the active configuration for a component. Since a history of active configurations is maintained by the service, the settings of each component each time it is run can be retrieved, and the system configuration at any moment can be recreated.\nSome of the methods provided by the admin API, the methods that change the Config Server content, are protected so that a record can be kept about who is making the modifications. This is done by using a token obtained from the Authentication and Authorization Service. See Admin Protected Routes for more information.\nclientAPI : Must be used in Assembly and HCD components. Available functions are: {exists | getActive} adminAPI : Full functionality exposed by Configuration Service Server is available with this API. Expected to be used administrators. Available functions are: {create | update | getById | getLatest | getByTime | delete | list | history | historyActive | setActiveVersion | resetActiveVersion | getActiveVersion | getActiveByTime | getMetadata | exists | getActive}","title":"API Flavors"},{"location":"/services/config.html#accessing-clientapi-and-adminapi","text":"The ConfigClientFactory exposes functions to get the clientAPI and adminAPI. Both the functions require the Location Service instance which is used to resolve the ConfigServer.\nScala //config client API\nval clientApi: ConfigClientService = ConfigClientFactory.clientApi(actorSystem, locationService)\n//config admin API\nval adminApi: ConfigService = ConfigClientFactory.adminApi(actorSystem, locationService, factory) Java //config client API\nIConfigClientService clientApi = JConfigClientFactory.clientApi(actorSystem, clientLocationService);\n//config admin API\nIConfigService adminApi = JConfigClientFactory.adminApi(actorSystem, clientLocationService, mocks.factory());\nNote Creating adminAPI requires instance of TokenFactory. TokenFactory has a getToken method which returns a raw access token string which is used by the config client to provide access to the Config Server. For more details, refer to bottom section on Admin Protected Routes","title":"Accessing clientAPI and adminAPI"},{"location":"/services/config.html#exists","text":"This function checks if the file exists at specified path in the repository. It returns Future of a Boolean, whether the file exists or not.\nScala //construct the path\nval filePath = Paths.get(\"/tmt/trmobone/assembly/hcd.conf\")\n\nval doneF = async {\n  // create file using admin API\n  await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"First commit\"))\n\n  //check if file exists with config service\n  val exists: Boolean = await(clientApi.exists(filePath))\n  exists shouldBe true\n}\nAwait.result(doneF, 5.seconds) Java Path filePath = Paths.get(\"/tmt/trmobone/assembly/hcd.conf\");\n\n// create file using admin API\nadminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"commit config file\").get();\n\nBoolean exists = clientApi.exists(filePath).get();\nAssert.assertTrue(exists);","title":"exists"},{"location":"/services/config.html#getactive","text":"This function retrieves the currently active file for a given path from config service. It returns a Future of Option of ConfigData.\nScala val defaultStrConf: String = \"foo { bar { baz : 1234 } }\"\nval doneF = async {\n  // construct the path\n  val filePath = Paths.get(\"/tmt/trmobone/assembly/hcd.conf\")\n\n  await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"First commit\"))\n\n  val activeFile: Option[ConfigData] = await(clientApi.getActive(filePath))\n  await(activeFile.get.toStringF(actorSystem)) shouldBe defaultStrConf\n}\nAwait.result(doneF, 5.seconds) Java String defaultStrConf = \"foo { bar { baz : 1234 } }\";\n// construct the path\nPath filePath = Paths.get(\"/tmt/trmobone/assembly/hcd.conf\");\n\nadminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"First commit\").get();\n\nConfigData activeFile = clientApi.getActive(filePath).get().orElseThrow();\nAssert.assertEquals(activeFile.toJConfigObject(actorSystem).get().getString(\"foo.bar.baz\"), \"1234\");","title":"getActive"},{"location":"/services/config.html#create","text":"Takes input ConfigData and creates the configuration in the repository at a specified path\nScala   async {\n    //construct ConfigData from String containing ASCII text\n    val configString: String =\n      \"\"\"\n    // Name: ComponentType ConnectionType\n    {\n      name: lgsTromboneHCD\n      type: Hcd\n      connectionType: [akka]\n    }\n    \"\"\".stripMargin\n    val config1: ConfigData = ConfigData.fromString(configString)\n\n    //construct ConfigData from a local file containing binary data\n    val srcFilePath         = ResourceReader.copyToTmp(\"/smallBinary.bin\")\n    val config2: ConfigData = ConfigData.fromPath(srcFilePath)\n\n    //construct ConfigData from Array[Byte] by reading a local file\n    val stream: InputStream    = getClass.getClassLoader.getResourceAsStream(\"smallBinary.bin\")\n    def byteArray: Array[Byte] = Stream.continually(stream.read).takeWhile(_ != -1).map(_.toByte).toArray\n    val config3                = ConfigData.fromBytes(byteArray)\n\n    //store the config, at a specified path as normal text file\n    val id1: ConfigId =\n      await(adminApi.create(Paths.get(\"/hcd/trombone/overnight.conf\"), config1, annex = false, \"review done\"))\n\n    //store the config, at a specified path as a binary file in annex store\n    val id2: ConfigId =\n      await(adminApi.create(Paths.get(\"/hcd/trombone/firmware.bin\"), config2, annex = true, \"smoke test done\"))\n\n    //store the config, at a specified path as a binary file in annex store\n    val id3: ConfigId =\n      await(adminApi.create(Paths.get(\"/hcd/trombone/debug.bin\"), config3, annex = true, \"new file from vendor\"))\n\n    //CAUTION: for demo example setup these IDs are returned. Don't assume them in production setup.\n    id1 shouldEqual ConfigId(1)\n    id2 shouldEqual ConfigId(3)\n    id3 shouldEqual ConfigId(5)\n  }\nAwait.result(futC, 2.seconds) Java //construct ConfigData from String containing ASCII text\nString configString = \"axisName11111 = tromboneAxis\\naxisName22222 = tromboneAxis2\\naxisName3 = tromboneAxis3333\";\nConfigData config1 = ConfigData.fromString(configString);\n\n//construct ConfigData from a local file containing binary data\nURI srcFilePath = getClass().getClassLoader().getResource(\"smallBinary.bin\").toURI();\nConfigData config2 = ConfigData.fromPath(Paths.get(srcFilePath));\n\nConfigId id1 = adminApi.create(Paths.get(\"/hcd/trombone/overnight.conf\"), config1, false, \"review done\").get();\nConfigId id2 = adminApi.create(Paths.get(\"/hcd/trombone/firmware.bin\"), config2, true, \"smoke test done\").get();\n\n//CAUTION: for demo example setup these IDs are returned. Don't assume them in production setup.\nAssert.assertEquals(id1, new ConfigId(\"1\"));\nAssert.assertEquals(id2, new ConfigId(\"3\"));","title":"create"},{"location":"/services/config.html#update","text":"Takes input ConfigData and overwrites the configuration specified in the repository\nScala val futU = async {\n  val destPath = Paths.get(\"/hcd/trombone/debug.bin\")\n  val newId = await(\n    adminApi\n      .update(destPath, ConfigData.fromString(defaultStrConf), comment = \"debug statements\")\n  )\n\n  //validate the returned id\n  newId shouldEqual ConfigId(7)\n}\nAwait.result(futU, 2.seconds) Java Path destPath = Paths.get(\"/hcd/trombone/overnight.conf\");\nConfigId newId = adminApi.update(destPath, ConfigData.fromString(defaultStrConf), \"added debug statements\").get();\n\n//validate the returned id\nAssert.assertEquals(newId, new ConfigId(\"5\"));","title":"update"},{"location":"/services/config.html#delete","text":"Deletes a file located at specified path in the repository\nScala val futD = async {\n  val unwantedFilePath = Paths.get(\"/hcd/trombone/debug.bin\")\n  await(adminApi.delete(unwantedFilePath, \"no longer needed\"))\n  //validates the file is deleted\n  await(adminApi.getLatest(unwantedFilePath)) shouldBe None\n}\nAwait.result(futD, 2.seconds) Java Path unwantedFilePath = Paths.get(\"/hcd/trombone/overnight.conf\");\nadminApi.delete(unwantedFilePath, \"no longer needed\").get();\nAssert.assertEquals(adminApi.getLatest(unwantedFilePath).get(), Optional.empty());","title":"delete"},{"location":"/services/config.html#getbyid","text":"Returns the file at a given path and matching revision Id\nScala val doneF = async {\n  // create a file using API first\n  val filePath = Paths.get(\"/tmt/trmobone/assembly/hcd.conf\")\n  val id: ConfigId =\n    await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"First commit\"))\n\n  //validate\n  val actualData = await(adminApi.getById(filePath, id)).get\n  await(actualData.toStringF(actorSystem)) shouldBe defaultStrConf\n}\nAwait.result(doneF, 2.seconds) Java Path filePath = Paths.get(\"/tmt/trombone/assembly/hcd.conf\");\nConfigId id = adminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"First commit\").get();\n\n//validate\nConfigData actualData = adminApi.getById(filePath, id).get().orElseThrow();\nAssert.assertEquals(defaultStrConf, actualData.toJStringF(actorSystem).get());","title":"getById"},{"location":"/services/config.html#getlatest","text":"Returns the latest version of the file stored at the given path.\nScala val assertionF: Future[Assertion] = async {\n  //create a file\n  val filePath = Paths.get(\"/test.conf\")\n  await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"initial configuration\"))\n\n  //override the contents\n  val newContent = \"I changed the contents!!!\"\n  await(adminApi.update(filePath, ConfigData.fromString(newContent), \"changed!!\"))\n\n  //get the latest file\n  val newConfigData = await(adminApi.getLatest(filePath)).get\n  //validate\n  await(newConfigData.toStringF(actorSystem)) shouldBe newContent\n}\nAwait.result(assertionF, 2.seconds) Java //create a file\nPath filePath = Paths.get(\"/test.conf\");\nadminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"initial configuration\").get();\n\n//override the contents\nString newContent = \"I changed the contents!!!\";\nadminApi.update(filePath, ConfigData.fromString(newContent), \"changed!!\").get();\n\n//get the latest file\nConfigData newConfigData = adminApi.getLatest(filePath).get().orElseThrow();\n//validate\nAssert.assertEquals(newConfigData.toJStringF(actorSystem).get(), newContent);","title":"getLatest"},{"location":"/services/config.html#getbytime","text":"Gets the file at the given path as it existed at a given time-instance. Note:\nIf time-instance is before the file was created, the initial version is returned. If time-instance is after the last change, the most recent version is returned.\nScala val assertionF = async {\n  val tInitial = Instant.MIN\n  //create a file\n  val filePath = Paths.get(\"/a/b/c/test.conf\")\n  await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"initial configuration\"))\n\n  //override the contents\n  val newContent = \"I changed the contents!!!\"\n  await(adminApi.update(filePath, ConfigData.fromString(newContent), \"changed!!\"))\n\n  val initialData: ConfigData = await(adminApi.getByTime(filePath, tInitial)).get\n  await(initialData.toStringF(actorSystem)) shouldBe defaultStrConf\n\n  val latestData = await(adminApi.getByTime(filePath, Instant.now())).get\n  await(latestData.toStringF(actorSystem)) shouldBe newContent\n}\nAwait.result(assertionF, 2.seconds) Java Instant tInitial = Instant.now();\n\n//create a file\nPath filePath = Paths.get(\"/test.conf\");\nadminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"initial configuration\").get();\n\n//override the contents\nString newContent = \"I changed the contents!!!\";\nadminApi.update(filePath, ConfigData.fromString(newContent), \"changed!!\").get();\n\nConfigData initialData = adminApi.getByTime(filePath, tInitial).get().orElseThrow();\nAssert.assertEquals(defaultStrConf, initialData.toJStringF(actorSystem).get());\n\nConfigData latestData = adminApi.getByTime(filePath, Instant.now()).get().orElseThrow();\nAssert.assertEquals(newContent, latestData.toJStringF(actorSystem).get());","title":"getByTime"},{"location":"/services/config.html#list","text":"For a given FileType (Annex or Normal) and an optional pattern string, it will list all files whose path matches the given pattern. Some pattern examples are: “/path/hcd/*.*”, “a/b/c/d.*”, “.*.conf”, “.*hcd.*”\nScala //Here's a list of tuples containing FilePath and FileTyepe(Annex / Normal)\nval paths: List[(Path, FileType)] = List[(Path, FileType)](\n  (Paths.get(\"a/c/trombone.conf\"), Annex),\n  (Paths.get(\"a/b/c/hcd/hcd.conf\"), Normal),\n  (Paths.get(\"a/b/assembly/assembly1.fits\"), Annex),\n  (Paths.get(\"a/b/c/assembly/assembly2.fits\"), Normal),\n  (Paths.get(\"testing/test.conf\"), Normal)\n)\n\n//create config files at those paths\npaths map {\n  case (path, fileType) =>\n    val createF = async {\n      await(\n        adminApi.create(path, ConfigData.fromString(defaultStrConf), Annex == fileType, \"initial commit\")\n      )\n    }\n    Await.result(createF, 2.seconds)\n}\n\nval assertionF = async {\n  //retrieve list of all files; for demonstration purpose show validate return values\n  await(adminApi.list()).map(info => info.path).toSet shouldBe paths.map {\n    case (path, _) => path\n  }.toSet\n\n  //retrieve list of files based on type; for demonstration purpose validate return values\n  await(adminApi.list(Some(Annex))).map(info => info.path).toSet shouldBe paths.collect {\n    case (path, fileType) if fileType == Annex => path\n  }.toSet\n  await(adminApi.list(Some(FileType.Normal))).map(info => info.path).toSet shouldBe paths.collect {\n    case (path, fileType) if fileType == FileType.Normal => path\n  }.toSet\n\n  //retrieve list using pattern; for demonstration purpose validate return values\n  await(adminApi.list(None, Some(\".*.conf\"))).map(info => info.path.toString).toSet shouldBe Set(\n    \"a/b/c/hcd/hcd.conf\",\n    \"a/c/trombone.conf\",\n    \"testing/test.conf\"\n  )\n  //retrieve list using pattern and file type; for demonstration purpose validate return values\n  await(adminApi.list(Some(FileType.Normal), Some(\".*.conf\"))).map(info => info.path.toString).toSet shouldBe\n  Set(\"a/b/c/hcd/hcd.conf\", \"testing/test.conf\")\n  await(adminApi.list(Some(Annex), Some(\"a/c.*\"))).map(info => info.path.toString).toSet shouldBe\n  Set(\"a/c/trombone.conf\")\n  await(adminApi.list(Some(FileType.Normal), Some(\"test.*\"))).map(info => info.path.toString).toSet shouldBe\n  Set(\"testing/test.conf\")\n}\nAwait.result(assertionF, 2.seconds) Java Path trombonePath = Paths.get(\"a/c/trombone.conf\");\nPath hcdPath = Paths.get(\"a/b/c/hcd/hcd.conf\");\nPath fits1Path = Paths.get(\"a/b/assembly/assembly1.fits\");\nPath fits2Path = Paths.get(\"a/b/c/assembly/assembly2.fits\");\nPath testConfPath = Paths.get(\"testing/test.conf\");\n\nString comment = \"initial commit\";\n\n//create files\nConfigId tromboneId = adminApi.create(trombonePath, ConfigData.fromString(defaultStrConf), true, comment).get();\nConfigId hcdId = adminApi.create(hcdPath, ConfigData.fromString(defaultStrConf), false, comment).get();\nConfigId fits1Id = adminApi.create(fits1Path, ConfigData.fromString(defaultStrConf), true, comment).get();\nConfigId fits2Id = adminApi.create(fits2Path, ConfigData.fromString(defaultStrConf), false, comment).get();\nConfigId testId = adminApi.create(testConfPath, ConfigData.fromString(defaultStrConf), true, comment).get();\n\n//retrieve full list; for demonstration purpose validate return values\nAssert.assertEquals(Set.of(tromboneId, hcdId, fits1Id, fits2Id, testId),\n        adminApi.list().get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));\n\n//retrieve list of files based on type; for demonstration purpose validate return values\nAssert.assertEquals(Set.of(tromboneId, fits1Id, testId),\n        adminApi.list(JFileType.Annex).get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));\nAssert.assertEquals(Set.of(hcdId, fits2Id),\n        adminApi.list(JFileType.Normal).get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));\n\n//retrieve list using pattern; for demonstration purpose validate return values\nAssert.assertEquals(Set.of(tromboneId, hcdId, testId),\n        adminApi.list(\".*.conf\").get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));\n//retrieve list using pattern and file type; for demonstration purpose validate return values\nAssert.assertEquals(Set.of(tromboneId, testId),\n        adminApi.list(JFileType.Annex, \".*.conf\").get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));\nAssert.assertEquals(Set.of(tromboneId),\n        adminApi.list(JFileType.Annex, \"a/c.*\").get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));\nAssert.assertEquals(Set.of(testId),\n        adminApi.list(JFileType.Annex, \"test.*\").get().stream().map(ConfigFileInfo::id).collect(Collectors.toSet()));","title":"list"},{"location":"/services/config.html#history","text":"Returns the history of revisions of the file at the given path for a range of period specified by from and to. The size of the list can be restricted using maxResults.\nScala val assertionF = async {\n  val filePath = Paths.get(\"/a/test.conf\")\n  val id0      = await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"first commit\"))\n\n  //override the contents twice\n  val tBeginUpdate = Instant.now()\n  val id1          = await(adminApi.update(filePath, ConfigData.fromString(\"changing contents\"), \"second commit\"))\n  val id2          = await(adminApi.update(filePath, ConfigData.fromString(\"changing contents again\"), \"third commit\"))\n  val tEndUpdate   = Instant.now()\n\n  //full file history\n  val fullHistory = await(adminApi.history(filePath))\n  fullHistory.map(_.id) shouldBe List(id2, id1, id0)\n  fullHistory.map(_.comment) shouldBe List(\"third commit\", \"second commit\", \"first commit\")\n\n  //drop initial revision and take only update revisions\n  await(adminApi.history(filePath, tBeginUpdate, tEndUpdate)).map(_.id) shouldBe List(id2, id1)\n\n  //take last two revisions\n  await(adminApi.history(filePath, maxResults = 2)).map(_.id) shouldBe List(id2, id1)\n}\nAwait.result(assertionF, 3.seconds) Java Path filePath = Paths.get(\"/a/test.conf\");\nConfigId id0 = adminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"first commit\").get();\n\n//override the contents twice\nInstant tBeginUpdate = Instant.now();\nConfigId id1 = adminApi.update(filePath, ConfigData.fromString(\"changing contents\"), \"second commit\").get();\nConfigId id2 = adminApi.update(filePath, ConfigData.fromString(\"changing contents again\"), \"third commit\").get();\nInstant tEndUpdate = Instant.now();\n\n//full file history\nList<ConfigFileRevision> fullHistory = adminApi.history(filePath).get();\nAssert.assertEquals(List.of(id2, id1, id0),\n        fullHistory.stream().map(ConfigFileRevision::id).collect(Collectors.toList()));\nAssert.assertEquals(List.of(\"third commit\", \"second commit\", \"first commit\"),\n        fullHistory.stream().map(ConfigFileRevision::comment).collect(Collectors.toList()));\n\n//drop initial revision and take only update revisions\nAssert.assertEquals(List.of(id2, id1),\n        adminApi.history(filePath, tBeginUpdate, tEndUpdate).get().stream().map(ConfigFileRevision::id).collect(Collectors.toList()));\n//take last two revisions\nAssert.assertEquals(List.of(id2, id1),\n        adminApi.history(filePath, 2).get().stream().map(ConfigFileRevision::id).collect(Collectors.toList()));","title":"history"},{"location":"/services/config.html#managing-active-versions","text":"Following API functions are available to manage the active version of a config file. In its lifetime, a config file undergoes many revisions. An active version is a specific revision from a file’s history and it is set by administrators.\nhistoryActive : Returns the history of active revisions of the file at the given path for a range of period specified by from and to. The size of the list can be restricted using maxResults. setActiveVersion : Sets the “active version” to be the version provided for the file at the given path. If this method is never called in a config’s lifetime, the active version will always be the version returned by create function. resetActiveVersion : Resets the “active version” of the file at the given path to the latest version. getActiveVersion : Returns the revision ID which represents the “active version” of the file at the given path. getActiveByTime : Returns the content of active version of the file existed at given instant\nScala val assertionF = async {\n  val tBegin   = Instant.now()\n  val filePath = Paths.get(\"/a/test.conf\")\n  //create will make the 1st revision active with a default comment\n  val id1 = await(adminApi.create(filePath, ConfigData.fromString(defaultStrConf), annex = false, \"first\"))\n  await(adminApi.historyActive(filePath)).map(_.id) shouldBe List(id1)\n  //ensure active version is set\n  await(adminApi.getActiveVersion(filePath)).get shouldBe id1\n\n  //override the contents four times\n  await(adminApi.update(filePath, ConfigData.fromString(\"changing contents\"), \"second\"))\n  val id3 = await(adminApi.update(filePath, ConfigData.fromString(\"changing contents again\"), \"third\"))\n  val id4 = await(adminApi.update(filePath, ConfigData.fromString(\"final contents\"), \"fourth\"))\n  val id5 = await(adminApi.update(filePath, ConfigData.fromString(\"final final contents\"), \"fifth\"))\n\n  //update doesn't change the active revision\n  await(adminApi.historyActive(filePath)).map(_.id) shouldBe List(id1)\n\n  //play with active version\n  await(adminApi.setActiveVersion(filePath, id3, s\"$id3 active\"))\n  await(adminApi.setActiveVersion(filePath, id4, s\"$id4 active\"))\n  await(adminApi.getActiveVersion(filePath)).get shouldBe id4\n  val tEnd = Instant.now()\n  //reset active version to latest\n  await(adminApi.resetActiveVersion(filePath, \"latest active\"))\n  await(adminApi.getActiveVersion(filePath)).get shouldBe id5\n  //finally set initial version as active\n  await(adminApi.setActiveVersion(filePath, id1, s\"$id1 active\"))\n  await(adminApi.getActiveVersion(filePath)).get shouldBe id1\n\n  //validate full history\n  val fullHistory = await(adminApi.historyActive(filePath))\n  fullHistory.map(_.id) shouldBe List(id1, id5, id4, id3, id1)\n  fullHistory.map(_.comment) shouldBe List(\n    s\"$id1 active\",\n    \"latest active\",\n    s\"$id4 active\",\n    s\"$id3 active\",\n    \"initializing active file with the first version\"\n  )\n\n  //drop initial revision and take only update revisions\n  val fragmentedHistory = await(adminApi.historyActive(filePath, tBegin, tEnd))\n  fragmentedHistory.size shouldBe 3\n\n  //take last three revisions\n  await(adminApi.historyActive(filePath, maxResults = 3)).map(_.id) shouldBe List(id1, id5, id4)\n\n  //get contents of active version at a specified instance\n  val initialContents = await(adminApi.getActiveByTime(filePath, tBegin)).get\n  await(initialContents.toStringF(actorSystem)) shouldBe defaultStrConf\n}\nAwait.result(assertionF, 5.seconds) Java Instant tBegin = Instant.now();\nPath filePath = Paths.get(\"/a/test.conf\");\n\n//create will make the 1st revision active with a default comment\nConfigId id1 = adminApi.create(filePath, ConfigData.fromString(defaultStrConf), false, \"first commit\").get();\nAssert.assertEquals(List.of(id1),\n        adminApi.historyActive(filePath).get().stream().map(ConfigFileRevision::id).collect(Collectors.toList()));\n//ensure active version is set\nAssert.assertEquals(id1, adminApi.getActiveVersion(filePath).get().orElseThrow());\n\n//override the contents four times\nadminApi.update(filePath, ConfigData.fromString(\"changing contents\"), \"second\").get();\nConfigId id3 = adminApi.update(filePath, ConfigData.fromString(\"changing contents again\"), \"third\").get();\nConfigId id4 = adminApi.update(filePath, ConfigData.fromString(\"final contents\"), \"fourth\").get();\nConfigId id5 = adminApi.update(filePath, ConfigData.fromString(\"final final contents\"), \"fifth\").get();\n\n//update doesn't change the active revision\nAssert.assertEquals(id1, adminApi.getActiveVersion(filePath).get().orElseThrow());\n\n//play with active version\nadminApi.setActiveVersion(filePath, id3, \"id3 active\").get();\nadminApi.setActiveVersion(filePath, id4, \"id4 active\").get();\nAssert.assertEquals(id4, adminApi.getActiveVersion(filePath).get().orElseThrow());\nInstant tEnd = Instant.now();\n\n//reset active version to latest\nadminApi.resetActiveVersion(filePath, \"latest active\").get();\nAssert.assertEquals(id5, adminApi.getActiveVersion(filePath).get().orElseThrow());\n//finally set initial version as active\nadminApi.setActiveVersion(filePath, id1, \"id1 active\").get();\nAssert.assertEquals(id1, adminApi.getActiveVersion(filePath).get().orElseThrow());\n\n//validate full history\nList<ConfigFileRevision> fullHistory = adminApi.historyActive(filePath).get();\nAssert.assertEquals(List.of(id1, id5, id4, id3, id1),\n        fullHistory.stream().map(ConfigFileRevision::id).collect(Collectors.toList()));\nAssert.assertEquals(List.of(\"id1 active\", \"latest active\", \"id4 active\", \"id3 active\",\n        \"initializing active file with the first version\"),\n        fullHistory.stream().map(ConfigFileRevision::comment).collect(Collectors.toList()));\n\n//drop initial revision and take only update revisions\nList<ConfigFileRevision> fragmentedHistory = adminApi.historyActive(filePath, tBegin, tEnd).get();\nAssert.assertEquals(3, fragmentedHistory.size());\n\n//take last three revisions\nAssert.assertEquals(List.of(id1, id5, id4),\n        adminApi.historyActive(filePath, 3).get().stream().map(ConfigFileRevision::id).collect(Collectors.toList()));\n\n//get contents of active version at a specified instance\nString initialContents = adminApi.getActiveByTime(filePath, tBegin).get().orElseThrow().toJStringF(actorSystem).get();\nAssert.assertEquals(defaultStrConf, initialContents);","title":"Managing active versions"},{"location":"/services/config.html#getmetadata","text":"Used to get metadata information about the Config Service. It includes:\nrepository directory annex directory min annex file size max config file size\nScala val assertF = async {\n  val metaData: ConfigMetadata = await(adminApi.getMetadata)\n  //repository path must not be empty\n  metaData.repoPath should not be empty\n}\nAwait.result(assertF, 2.seconds) Java ConfigMetadata metadata = adminApi.getMetadata().get();\n//repository path must not be empty\nAssert.assertNotEquals(metadata.repoPath(), \"\");","title":"getMetaData"},{"location":"/services/config.html#admin-protected-routes","text":"The following Config Server routes are Admin Protected. To use these routes, the use must be authenticated and authorized.\ncreate update delete setActiveVersion resetActiveVersion\ncsw-config-client provides factory to create admin config service which allows access to these protected routes. This requires you to implement TokenFactory interface. Currently csw-config-cli is the only user of config service admin api. Refer to CliTokenFactory which implements TokenFactory interface.\nNote Refer to csw-aas docs to know more about how to authenticate and authorize with AAS and get an access token.","title":"Admin Protected Routes"},{"location":"/services/config.html#technical-description","text":"See Configuration Service Technical Description.","title":"Technical Description"},{"location":"/services/config.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source code for examples"},{"location":"/services/logging.html","text":"","title":"Logging Service"},{"location":"/services/logging.html#logging-service","text":"The Logging Service library provides an advanced logging facility for CSW components and services.\nNote This page explains local logging only. To understand centralized logging facility, refer to this page.","title":"Logging Service"},{"location":"/services/logging.html#dependencies","text":"To use the Logging Service without using the framework, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-logging-client\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/logging.html#configuration","text":"These are the relevant default configuration values for logging\nlogging.conf csw-logging {\n\n  # Default appender and is recommended only for dev setup.\n  # To use multiple appenders give comma separated list of appenders.\n  appenders = [\"csw.logging.client.appenders.StdOutAppender$\"]\n\n  # Recommended for production setup - Uncomment below line and comment above to enable FileAppender\n  #appenders = [\"csw.logging.client.appenders.FileAppender$\"]\n\n  component-log-levels {\n    # By default each compoent will use log level specified by `csw-logging.logLevel`. If required, this block allows\n    # overriding log level configuration for one/more components.\n    # -- Example --\n    # tcs.filter = debug\n    # nfiraos {\n    #      tromboneassembly = error\n    #      trombonehcd = info\n    #  }\n    #\n  }\n\n  appender-config {\n    file {\n      # Include standard headers\n      fullHeaders = true\n      # Sort the fields in Json objects\n      sorted = true\n      # Base path directory to hold log files from tmt apps. The value is picked from env var `TMT_LOG_HOME`. It is necessary\n      # to have `TMT_LOG_HOME` set as env var otherwise an exception will be thrown when FileAppender gets initialized.\n      baseLogPath = ${?TMT_LOG_HOME}\n      # Directory to hold log files under base path $TMT_LOG_HOME i.e. basePath/logPath\n      logPath = \"\"\n      # Common log messages below this level are removed, none removed when set to trace\n      logLevelLimit = trace\n      # Flag to turn file rotation ON or OFF. Currently log file is rotated daily at 12:00 pm UTC time. By turning this flag off\n      # file rotation will not happen.\n      rotate = true\n    }\n    stdout {\n      # Include standard headers\n      fullHeaders = false\n      # Use colors for log levels\n      color = true\n      # The maximum number of character that should be on a line\n      # This is only a goal; in practice some may contain more characters.\n      width = 80\n      # Print summary counts when logger is closed\n      summary = true\n      # pretty output - multiple line json\n      pretty = false\n      # Messages below this level are removed, none removed when set to trace\n      logLevelLimit = trace\n      # false Json output; true simple one line text output\n      oneLine = false\n    }\n  }\n  # If component does not specify their log level in component-log-levels block,\n  # Then this will be considered as default log level for that component.\n  logLevel = info\n  # Log level for slf4j messages\n  slf4jLogLevel = info\n  # Log level for Akka messages\n  akkaLogLevel = warn\n  # Enable timing logging\n  time = false\n  # Enable garbage collection logging\n  gc = false\n}\nIMPORTANT !!! It is required to include logging.conf that is shipped with this library in application.conf as follows: include \"logging.conf\"\n Default configuration values can be then overridden in application.conf.\nNote Logs generated by components will be by default stored at $TMT_LOG_HOME/tmt/logs. In case $TMT_LOG_HOME is not set as environment variable, then they will be stored at /tmp/tmt/logs. When logs are stored in the /tmp folder, they will not be persisted in case of machine reboot. Hence, for production, it is recommended to have $TMT_LOG_HOME set as environment variable. csw-logging.appender-config.file.logPath from logging.conf will treated as subdirectory under the base directory ($TMT_LOG_HOME/tmt/logs or /tmp/tmt/logs) to store the logs. It can be also overridden in application.conf for each component.\nAlso logLevel for each component can be set in application.conf as follows:\ncomponent-log-levels {\n    tcs {\n        trombonehcd = debug\n        tromboneassembly = error\n    }\n  }\nNote Here trombonehcd and tromboneassembly is the name of component that will be registered with LocationService, which is the name field in the ComponentInfo file (see DeployingComponents). By default, all components will log at level specified by csw-logging.logLevel.\nThe library provides StdOutAppender as the default logging appender. To use FileAppender or some custom appender along with StdOutAppender, override the appenders property to include multiple appenders in CSV format as follows:\nappenders = [\"csw.logging.client.appenders.FileAppender$\", \"csw.logging.client.appenders.StdOutAppender$\"]\nNote Make sure to provide full path of the appender since it will be spawned using Java reflection. In the CSW code base, a working example of a custom appender can be found at: Custom Appender\nFor StdOutAppender, specify the format of log statements in csw-logging.stdout via csw-logging.stdout.pretty and csw-logging.stdout.oneLine.\nTurning pretty on or off will produce log statements in following format:\npretty=true {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"LocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53618/user/$a\",\n \"class\":\"csw.location.LocationServiceExampleClient\",\n \"file\":\"LocationServiceExampleClientApp.scala\",\n \"line\":149,\n \"message\":\"Result of the find call: None\",\n \"timestamp\":\"2017-11-30T10:58:03.102Z\"\n }\n pretty=false {\"@prefix\":\"my-subsystem.my-component-name\",\"@subsystem\":\"my-subsystem\",\"@componentName\":\"my-component-name\",\"@host\":\"INsaloni.local\",\"@name\":\"LocationServiceExampleClient\",\"@severity\":\"INFO\",\"@version\":\"0.1\",\"actor\":\"akka://csw-examples-locationServiceClient@10.131.23.195:53618/user/$a\",\"class\":\"csw.location.LocationServiceExampleClient\",\"file\":\"LocationServiceExampleClientApp.scala\",\"line\":149,\"message\":\"Result of the find call: None\",\"timestamp\":\"2017-11-24T04:16:42.108Z\"}\nSimilarly, turning oneLine on will produce log statements in following format:\noneLine=true [INFO] Attempting to find connection (LocationServiceExampleClientApp.scala 131)\nNote If oneLine is set to true then value of pretty will be ignored","title":"Configuration"},{"location":"/services/logging.html#log-levels","text":"Following Log levels are supported by csw-logging library\nFATAL ERROR WARN INFO DEBUG TRACE\nThe library allows separate log levels for the component logging API (logLevel), Akka logging (akkaLogLevel), and Slf4J (slf4jLogLevel). The initial values of these are set in the configuration file as seen above. These can be overridden in the application.conf file.\nThese values can also be changed dynamically by calling methods on LoggingSystem class.","title":"Log Levels"},{"location":"/services/logging.html#log-structure","text":"All messages are logged by default as Json. Logs can contain following fields:\n@prefix: The combination of @subsystem and @componentName of the component for e.g. tcs.filter.wheel @subsystem: The name of the subsystem, if present @componentName: The name of the component, if present @host: The local host name @name: The name of the application being run @version: The version of the application being run @severity: The message level: trace, debug, info, warn, error or fatal actor: The path for an actor when using ActorLogging class: The class for ClassLogging or ActorLogging file: The file containing the log call kind: Either slf4j or akka. Not present for logger API line: The line where the message was logged message: The log message timestamp: The UTC time when the message was logged trace: Information for any exception specified in the logging call\nNote @host, @name and @version will appear in log statements only if fullHeaders is set as true in the configuration file and line will appear only in log statements from Scala classes/actors. This is typical for Java logging tools, and is due to the lack of macro support in Java.","title":"Log Structure"},{"location":"/services/logging.html#enable-component-logging","text":"Component developers will have an instance of LoggerFactory available from csw-framework. This instance will already have a prefix set by csw-framework which will appear in log statements in the @prefix,@subsystem and @componentName tags. To get the Logger instance from the LoggerFactory, use one of the following:\nScala Class class SampleClass(loggerFactory: LoggerFactory) {\n\n  val log: Logger = loggerFactory.getLogger\n} Java Class public class JSampleClass {\n\n    public JSampleClass(JLoggerFactory loggerFactory) {\n        ILogger log = loggerFactory.getLogger(getClass());\n    }\n}\nScala Actor class SampleActor(loggerFactory: LoggerFactory) extends akka.actor.Actor {\n\n  //context is available from akka.actor.Actor\n  val log: Logger = loggerFactory.getLogger(context)\n\n  override def receive = ???\n} Java Actor class JSampleActor extends akka.actor.AbstractActor {\n\n    public JSampleActor(JLoggerFactory loggerFactory) {\n\n        //context() is available from akka.actor.AbstractActor\n        ILogger log = loggerFactory.getLogger(context(), getClass());\n    }\n\n    @Override\n    public Receive createReceive() {\n        return null;\n    }\n}\nScala Typed Actor class SampleTypedActor(loggerFactory: LoggerFactory, ctx: ActorContext[ComponentMessage]) {\n\n  val log: Logger = loggerFactory.getLogger(ctx)\n} Java Typed Actor class JSampleTypedActor {\n\n    public JSampleTypedActor(JLoggerFactory loggerFactory, ActorContext<ComponentMessage> ctx) {\n        ILogger log = loggerFactory.getLogger(ctx, getClass());\n    }\n}\nNote The LoggerFactory can be changed to JLoggerFactory by using asJava method and JLoggerFactory can be changed to LoggerFactory by using asScala method","title":"Enable Component Logging"},{"location":"/services/logging.html#enable-generic-logging","text":"In case there is a need to log statements without having a @prefix, @subsystem and @componentName tags, which can be due to unavailability of componentName in some utility code, then use the GenericLoggerFactory as follows:\nScala Class class GenericClass {\n\n  val log: Logger = GenericLoggerFactory.getLogger\n} Java Class public class JGenericClass {\n\n    ILogger log = JGenericLoggerFactory.getLogger(getClass());\n}\nScala Actor object GenericActor {\n\n  def behavior[T]: Behavior[T] = Behaviors.setup[T] { context =>\n    val log: Logger = GenericLoggerFactory.getLogger(context)\n    // actor setup\n\n    Behaviors.receiveMessage {\n      case _ => // handle messages and return new behavior\n        Behaviors.same\n    }\n  }\n} Java Actor class JGenericActor extends akka.actor.AbstractActor {\n\n    //context() is available from akka.actor.AbstractActor\n    ILogger log = JGenericLoggerFactory.getLogger(context(), getClass());\n\n    @Override\n    public Receive createReceive() {\n        return null;\n    }\n}\nScala Typed Actor class GenericTypedActor(ctx: ActorContext[ComponentMessage]) {\n\n  val log: Logger = GenericLoggerFactory.getLogger(ctx)\n} Java Typed Actor class JGenericTypedActor {\n\n    public JGenericTypedActor(ActorContext<ComponentMessage> ctx) {\n        ILogger log = JGenericLoggerFactory.getLogger(ctx, getClass());\n    }\n}\nThe Logger log is now available in Scala and Java classes to write log statements as explained in next section.","title":"Enable Generic Logging"},{"location":"/services/logging.html#log-statements","text":"Logging statements are used very much like existing logging services such as log4j. For Java, there is an additional way of writing log messages using Supplier methods (lambdas). The use of lambdas is more efficient since the computations in a message (e.g. string concatenation) are not performed unless the message is actually being logged. Therefore, these supplier methods should be used in cases where high performance is required (see performance results below).\nA basic info statement can be written as follows:\nScala log.info(s\"Result of the find call: $findResult\") Java log.info(\"Find result: \" + connectionInfo(findResult.orElseThrow().connection())); Java (Supplier) log.info(() -> \"Resolve result: \" + connectionInfo(resolveResult.orElseThrow().connection()));\nThe output of log statement will be:\nScala {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"LocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53618/user/$a\",\n \"class\":\"csw.location.LocationServiceExampleClient\",\n \"file\":\"LocationServiceExampleClientApp.scala\",\n \"line\":149,\n \"message\":\"Result of the find call: None\",\n \"timestamp\":\"2017-11-30T10:58:03.102Z\"\n }\n Java {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"JLocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53625/user/LocationServiceExampleClient\",\n \"class\":\"csw.location.JLocationServiceExampleClient\",\n \"message\":\"Result of the find call : None\",\n \"timestamp\":\"2017-11-30T11:02:54.691Z\"\n}\n Java (Supplier) {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"JLocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53625/user/LocationServiceExampleClient\",\n \"class\":\"csw.location.JLocationServiceExampleClient\",\n \"message\":\n   \"Resolve result: LocationServiceExampleComponent-assembly-akka, component type=Assembly, connection type=AkkaType\",\n \"timestamp\":\"2017-07-26T19:44:58.636Z\"\n}\nThe library allows usage of Map in message as follows:\nScala log.info(\n  s\"Attempting to find $exampleConnection\",\n  Map(Keys.OBS_ID -> \"foo_obs_id\", \"exampleConnection\" -> exampleConnection.name)\n) Java log.info(\"Attempting to find \" + exampleConnection,\n        Map.of(\n                JKeys.OBS_ID, \"foo_obs_id\",\n                \"exampleConnection\", exampleConnection.name()\n        )); Java (Supplier) log.info(() -> \"Attempting to resolve \" + exampleConnection + \" with a wait of \" + waitForResolveLimit + \"...\", () -> Map.of(\n        JKeys.OBS_ID, \"foo_obs_id\",\n        \"exampleConnection\", exampleConnection.name()\n));\nThe output of log statement will be:\nScala {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"LocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53618/user/$a\",\n \"class\":\"csw.location.LocationServiceExampleClient\",\n \"exampleConnection\":\"LocationServiceExampleComponent-assembly-akka\",\n \"file\":\"LocationServiceExampleClientApp.scala\",\n \"line\":143,\n \"message\":\n   \"Attempting to find AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly))\",\n \"obsId\":\"foo_obs_id\",\n \"timestamp\":\"2017-11-30T10:58:03.097Z\"\n}\n Java {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"JLocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53625/user/LocationServiceExampleClient\",\n \"class\":\"csw.location.JLocationServiceExampleClient\",\n \"exampleConnection\":\"LocationServiceExampleComponent-assembly-akka\",\n \"message\":\n   \"Attempting to find AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly))\",\n \"obsId\":\"foo_obs_id\",\n \"timestamp\":\"2017-11-30T11:02:54.686Z\"\n}\n Java (Supplier) {\"@prefix\":\"my-subsystem.my-component-name\",\n \"@subsystem\":\"my-subsystem\",\n \"@componentName\":\"my-component-name\",\n \"@host\":\"INsaloni.local\",\n \"@name\":\"JLocationServiceExampleClient\",\n \"@severity\":\"INFO\",\n \"@version\":\"0.1\",\n \"actor\":\n   \"akka://csw-examples-locationServiceClient@10.131.23.195:53625/user/LocationServiceExampleClient\",\n \"class\":\"csw.location.JLocationServiceExampleClient\",\n \"exampleConnection\":\"LocationServiceExampleComponent-assembly-akka\",\n \"message\":\n   \"Attempting to resolve AkkaConnection(ComponentId(LocationServiceExampleComponent,Assembly)) with a wait of 30 seconds...\",\n \"obsId\":\"foo_obs_id\",\n \"timestamp\":\"2017-11-24T04:22:02.589Z\"\n}\nThe library allows you to log an error with its full stacktrace as follows:\nScala val runtimeException = new RuntimeException(s\"Received unexpected message $x\")\nlog.error(runtimeException.getMessage, ex = runtimeException) Java } catch (InterruptedException | ExecutionException ex) {\n    log.info(ex.getMessage(), ex);\n    throw ex;\n} Java (Supplier) RuntimeException runtimeException = new RuntimeException(\"Received unexpected message \" + x);\nlog.info(runtimeException::getMessage, runtimeException);","title":"Log Statements"},{"location":"/services/logging.html#create-loggerfactory","text":"Note This functionality is included in the framework code and users should not have to implement it themselves. Documentation is included for testing or further understanding.\nIn order to create a LoggerFactory with a custom prefix, refer to the following code:\nScala val loggerFactory: LoggerFactory = new LoggerFactory(Prefix(\"csw.my-component-name\"))\n\n// convert a scala LoggerFactory to java JLoggerFactory\nval jLoggerFactory: JLoggerFactory = loggerFactory.asJava Java JLoggerFactory jLoggerFactory = new JLoggerFactory(Prefix.apply(\"csw.my-component-name\"));\n\n// convert a java JLoggerFactory to scala LoggerFactory\nLoggerFactory loggerFactory = jLoggerFactory.asScala();","title":"Create LoggerFactory"},{"location":"/services/logging.html#create-loggingsystem","text":"Note This functionality is included in the framework code and users should not have to implement it themselves. Documentation is included for testing or further understanding.\nFor logging statements to appear in the program, start LoggingSystem at an earliest location in an application. Also note, LoggingSystem should be started only once in an application. The name used while creating LoggingSystem will be used to create the folder that will contain all logging files.\nScala private val host = InetAddress.getLocalHost.getHostName\n// Only call this once per application\nval loggingSystem: LoggingSystem = LoggingSystemFactory.start(\"LocationServiceExampleClient\", \"0.1\", host, typedSystem) Java String host = InetAddress.getLocalHost().getHostName();\nloggingSystem = JLoggingSystemFactory.start(\"JLocationServiceExampleClient\", \"0.1\", host, typedSystem);\nNote The hostname that is provided while creating LoggingSystem will appear in log statements against @host tag","title":"Create LoggingSystem"},{"location":"/services/logging.html#stop-loggingsystem","text":"Note This functionality is included in the framework code and users should not have to implement it themselves. Documentation is included for testing or further understanding.\nPlease ensure the LoggingSystem is stopped before the application exits.\nScala // Only call this once per application\nAwait.result(loggingSystem.stop, 30.seconds) Java // Only call this once per application\nloggingSystem.javaStop().get();","title":"Stop LoggingSystem"},{"location":"/services/logging.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source code for examples"},{"location":"/services/logging.html#performance","text":"Performance results documented on this page is measured in terms of throughput and obtained with the JMH Java benchmark harness. Sourcecode for all the JMH benchmarks can be found here at CSW Benchmarks\nWikipedia definition of “throughput” is the maximum rate of production or the maximum rate at which something can be processed. In our case, throughput is defined as how many messages can be logged in a certain period of time.","title":"Performance"},{"location":"/services/logging.html#test-machine-configuration-","text":"MacBook Pro (Retina, 15-inch, Mid 2015) Processor Name: Intel Core i7 Processor Speed: 2.8 GHz No. of Processors: 1 No. of Cores: 4 L2 Cache (per Core): 256 KB L3 Cache: 6 MB Memory: 16 GB 1600 MHz DDR3 JDK: 1.8.0_121","title":"Test Machine Configuration :"},{"location":"/services/logging.html#scala-logging-appenders-throughput","text":"Above graph includes the throughput results for File and StdOut appenders using the Scala logging API. The Java logging API throughput results are included further down this page. As shown in the above graph, the experiment was carried out for 1, 2, 4, 8 and 16 threads. That means, multiple threads are logging messages concurrently.\nNote Numbers indicated in the graph does not mean that those number of messages are actually written to file or console. These are the number of logging messages sent to LogActor asynchronously for the period of one second. It is the responsibility of LogActor to write these messages to file or console. LogActor uses BoundedMailbox with a mailbox-capacity = 262144, messages are dropped if mailbox gets full.","title":"Scala Logging : Appender’s Throughput"},{"location":"/services/logging.html#scala-and-java-logging-throughput-comparision-log-level-enabled-vs-disabled-","text":"Below graph depicts the throughput of the Java logging API (String and Supplier) and the Scala logging API when the log level was enabled and disabled.\nNote Log Level Enabled : Results are obtained for log.info when the default log level was set to info, which means the logging messages were sent to LogActor for writing it to file. Log Level Disabled : Results are obtained for log.trace when the default log level was set to info, which means the logging messages were not written to file.\nThis graph is produced based on the result generated by JE2ELoggingBenchmark and E2ELoggingBenchmark\nNote As you can see in the above graph, the Supplier version of the Java log API and the “by name” version of Scala API are very efficient and throughput is much higher than the String version. Therefore, it is recommended that Java developers use the Supplier API.","title":"Scala And Java Logging : Throughput Comparision (Log Level Enabled vs Disabled)"},{"location":"/services/logging.html#technical-description","text":"See Logging Service Technical Description.","title":"Technical Description"},{"location":"/services/logging.html#acknowledgement","text":"The codebase in csw-logging module is based on persist-logging library. We appreciate efforts put in by authors of the persist-logging library which made our development fast and easy.","title":"Acknowledgement"},{"location":"/services/event.html","text":"","title":"Event Service"},{"location":"/services/event.html#event-service","text":"The Event Service implements the publish/subscribe messaging paradigm where one component publishes an event and all clients that have subscribed receive the event. In CSW, the events published are described under the messages documentation here. One advantage of this type of message system for the Event Service is that publishers and subscribers are decoupled. This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology. Publishers can publish regardless of whether there are subscribers, and subscribers can subscribe even if there are no publishers. The relationship between publishers and subscribers can be one-to-one, one-to-many, many to one, or even many-to-many.\nThe Event Service is optimized for the high performance requirements of events as demands with varying rates, (e.g. 100 Hz, 50 Hz etc.), but can also be used with events that are published infrequently or when values change. In the TMT control system, events may be created as the output of a calculation by one component for the input to a calculation in one or more other components. Demand events often consist of events that are published at a specific rate.\nThe Event Service also stores the most recent published event for every unique event by prefix and name. This allows components needing to check the state of another component can do so without the overhead of subscribing.","title":"Event Service"},{"location":"/services/event.html#dependencies","text":"If you already have a dependency on csw-framework in your build.sbt, then you can skip this as csw-framework depends on csw-event-client Otherwise add below dependency in your build.sbt\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-event-client\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/event.html#accessing-event-service","text":"When you create component handlers using csw-framework as explained here, you get a handle to EventService which is created by csw-framework, which provides the following features:\nAccess to defaultPublisher: Using the defaultPublisher in EventService, you can publish a single event or a stream of demand events to Event Service. In most cases, you should use the defaultPublisher, because you can then pass the instance of EventService in worker actors or different places in your code and call eventService.defaultPublisher to access the publisher. Each EventPublisher has its own TCP connection to the Event Service. When you reuse the defaultPublisher instance of EventPublisher, all events published go through same TCP connection.\nAccess to defaultSubscriber: Using the defaultSubscriber, you can subscribe to specific event keys. You can share the defaultSubscriber in the same way as defaultPublisher by passing an instance of EventService to different parts of your code. Unlike defaultPublisher, each subscription with defaultSubscriber.subscribe creates a new TCP connection for just that subscription. This behavior is the same whether you use the defaultSubscriber or the makeNewSubscriber call on EventService.\nEach EventSubscriber also has one TCP connection that is used to provide the latest event from the event server when subscribing and for the explicit get calls. That means, with defaultSubscriber, you are sharing same connection for getting latest events and creating a new connection for each subscribe call. The underlying event server can handle many connections, but it is good to understand how connections are used and reused.\nCreating a new Publisher or Subscriber: The makeNewPublisher API of Event Service can be used to create a new publisher which would internally create a new TCP connection to the Event Store. One of the use cases of this API could be to publish high frequency event streams in order to dedicate a separate connection to demanding streams without affecting the performance of all other low frequency (for ex. 1Hz, 20Hz etc.) event streams.\nHowever, makeNewSubscriber API does not really have any specific use cases. Both defaultSubscriber and makeNewSubscriber APIs behave almost similar since the subscribe API of EventService itself creates a new connection for every subscription. Prefer using defaultSubscriber over makeNewSubscriber.","title":"Accessing Event Service"},{"location":"/services/event.html#usage-of-eventpublisher","text":"Below examples demonstrate the usage of multiple variations of the publish API.","title":"Usage of EventPublisher"},{"location":"/services/event.html#for-single-event","text":"This is the simplest API to publish a single event. It returns a Future which will complete successfully if the event is published or fail immediately with a PublishFailure exception if the component cannot publish the event.\nScala {\n  val publisher = eventService.defaultPublisher\n  val event     = SystemEvent(componentInfo.prefix, EventName(\"filter_wheel\"))\n  publisher.publish(event)\n} Java Event event = new SystemEvent(componentInfo.prefix(), new EventName(\"filter_wheel\"));\neventService.defaultPublisher().publish(event);","title":"For Single Event"},{"location":"/services/event.html#with-generator","text":"A generator is useful when component code needs to publish events with a specific frequency. The following example demonstrates the usage of publish API with event generator which will publish one event at each interval. eventGenerator is a function responsible for generating events. It can hold domain specific logic of generating new events based on certain conditions.\nScala {\n  val publisher        = eventService.defaultPublisher\n  val baseEvent: Event = SystemEvent(componentInfo.prefix, EventName(\"filter_wheel\"))\n  val interval         = 100.millis\n\n  // this holds the logic for event generation, could be based on some computation or current state of HCD\n  def eventGenerator(): Option[Event] = baseEvent match {\n    case e: SystemEvent  => Some(e.copy(eventId = Id(), eventTime = UTCTime.now()))\n    case e: ObserveEvent => Some(e.copy(eventId = Id(), eventTime = UTCTime.now()))\n  }\n\n  publisher.publish(eventGenerator(), interval)\n} Java private Cancellable startPublishingEvents(ComponentInfo componentInfo) {\n    Event baseEvent = new SystemEvent(componentInfo.prefix(), new EventName(\"filter_wheel\"));\n    return eventService.defaultPublisher().publish(() -> eventGenerator(baseEvent), Duration.ofMillis(100));\n}\n\n// this holds the logic for event generation, could be based on some computation or current state of HCD\nprivate Optional<Event> eventGenerator(Event baseEvent) {\n    // add logic here to create a new event and return the same\n    return Optional.ofNullable(baseEvent);\n}\nNote Callbacks like eventGenerator are not thread-safe on the JVM. If you are doing side effects/mutations inside the callback, you should ensure that it is done in a thread-safe way inside an actor. Here is an example of how it can be done.","title":"With Generator"},{"location":"/services/event.html#with-event-stream","text":"In order to publish a continuous stream of events, this stream-based API can also be used. If an infinite stream is provided, shutdown of the stream needs to be taken care by the users. (Note that streams discussed here are an Akka feature that is supported in event publisher and subscriber APIs. See Akka stream documentation.)\nScala def onError(publishFailure: PublishFailure): Unit =\n  log.error(s\"Publish failed for event: [${publishFailure.event}]\", ex = publishFailure.cause)\n\nval publisher = eventService.defaultPublisher\nval eventStream: Source[Event, Future[Done]] = Source(1 to n)\n  .map(id => makeEvent(id, componentInfo.prefix, EventName(\"filter_wheel\")))\n  .watchTermination()(Keep.right)\n\npublisher.publish(eventStream, failure => onError(failure)) Java Source<Event, CompletionStage<Done>> eventStream = Source\n        .range(1, n)\n        .map(id -> makeEvent(id, componentInfo.prefix(), new EventName(\"filter_wheel\")))\n        .watchTermination(Keep.right());\n\nreturn eventService.defaultPublisher().<CompletionStage<Done>>publish(eventStream, failure -> { /*do something*/ });\nThis API also demonstrates the usage of an onError callback which can be used to handle events that failed while being published.\nYou can find complete list of APIs supported by EventPublisher and IEventPublisher with detailed description of each API here:\nEventPublisher IEventPublisher","title":"With Event Stream"},{"location":"/services/event.html#usage-of-eventsubscriber","text":"The EventSubscriber API has several options available that are useful in different situations. Examples below demonstrate the usage of multiple variations available in the subscribe API.","title":"Usage of EventSubscriber"},{"location":"/services/event.html#with-callback","text":"The example shown below takes a set of event keys to subscribe to and a callback function which will be called on each event received by the event stream. This is the simplest and most commonly used API. The example below uses an inline function, but that is not necessary.\nScala {\n  val subscriber = eventService.defaultSubscriber\n\n  subscriber.subscribeCallback(\n    Set(EventKey(hcd.prefix, EventName(\"filter_wheel\"))),\n    event => { /*do something*/ }\n  )\n} Java IEventSubscriber subscriber = eventService.defaultSubscriber();\n\nEventKey filterWheelEventKey = new EventKey(hcdLocation.prefix(), new EventName(\"filter_wheel\"));\nreturn subscriber.subscribeCallback(Set.of(filterWheelEventKey), event -> { /*do something*/ });\nNote Callbacks are not thread-safe on the JVM. If you need to do side effects/mutations, prefer using subscribeActorRef API.","title":"With Callback"},{"location":"/services/event.html#with-asynchronous-callback","text":"This API is useful when you want to subscribe to events with a callback that has an asynchronous behavior. The callback is of type Event => Future and it ensures that the event callbacks are called sequentially in such a way that the subsequent execution will start only after the prior one finishes. This API gives the guarantee of ordered execution of the asynchronous callbacks.\nScala def subscribe(): EventSubscription = {\n  val subscriber = eventService.defaultSubscriber\n  subscriber.subscribeAsync(Set(EventKey(hcd.prefix, EventName(\"filter_wheel\"))), callback)\n}\n\nprivate def callback(event: Event): Future[String] = {\n  /* do something */\n  Future.successful(\"some value\")\n} Java public IEventSubscription subscribe() {\n    IEventSubscriber subscriber = eventService.defaultSubscriber();\n\n    EventKey filterWheelEventKey = new EventKey(hcdLocation.prefix(), new EventName(\"filter_wheel\"));\n    return subscriber.subscribeAsync(Set.of(filterWheelEventKey), this::callback);\n}\n\nprivate CompletableFuture<String> callback(Event event) {\n    /* do something */\n    return CompletableFuture.completedFuture(\"some value\");\n}","title":"With Asynchronous Callback"},{"location":"/services/event.html#with-actorref","text":"If there is a need to mutate state on receiving each event, then it is recommended to use this API and send a message to an actor. To use this API, you have to create an actor which takes event and then you can safely keep mutable state inside this actor. In the example shown below, eventHandler is the actorRef which accepts events.\nScala def subscribe(ctx: ActorContext[TopLevelActorMessage]): EventSubscription = {\n  val subscriber                    = eventService.defaultSubscriber\n  val eventHandler: ActorRef[Event] = ctx.spawnAnonymous(EventHandler.behavior)\n\n  subscriber.subscribeActorRef(Set(EventKey(hcd.prefix, EventName(\"filter_wheel\"))), eventHandler)\n}\n\nobject EventHandler {\n  val behavior: Behavior[Event] = Behaviors.setup { ctx =>\n    //setup required for the actor\n\n    Behaviors.receiveMessage {\n      case _ => //handle messages and return new behavior with changed state\n        Behaviors.same\n    }\n  }\n} Java public IEventSubscription subscribe(ActorContext<TopLevelActorMessage> ctx) {\n\n    IEventSubscriber subscriber = eventService.defaultSubscriber();\n    ActorRef<Event> eventHandler = ctx.spawnAnonymous(JEventHandler.behavior());\n\n    EventKey filterWheelEventKey = new EventKey(hcdLocation.prefix(), new EventName(\"filter_wheel\"));\n    return subscriber.subscribeActorRef(Set.of(filterWheelEventKey), eventHandler);\n}\n\npublic static class JEventHandler {\n\n    public static Behavior<Event> behavior() {\n        // handle messages\n        return null;\n    }\n}","title":"With ActorRef"},{"location":"/services/event.html#receive-event-stream","text":"This API takes a set of Event keys to subscribe to and returns a Source of events (see Akka stream documentation). This API gives more control to the user to customize the behavior of an event stream.\nScala {\n  val subscriber = eventService.defaultSubscriber\n\n  subscriber\n    .subscribe(Set(EventKey(hcd.prefix, EventName(\"filter_wheel\"))))\n    .to(Sink.foreach { event => /*do something*/\n    })\n    .run()\n} Java IEventSubscriber subscriber = eventService.defaultSubscriber();\n\nEventKey filterWheelEventKey = new EventKey(hcdLocation.prefix(), new EventName(\"filter_wheel\"));\nreturn subscriber.subscribe(Set.of(filterWheelEventKey)).to(Sink.foreach(event -> { /*do something*/ })).run(system);","title":"Receive Event Stream"},{"location":"/services/event.html#controlling-subscription-rate","text":"In all the examples shown above, events are received by the subscriber as soon as they are published. There will be scenarios where you would like to control the rate of events received by your code. For instance, slow subscribers can receive events at their own specified speed rather than being overloaded with events to catch up with the publisher’s speed.\nAll the APIs in EventSubscriber can be provided with an interval and a SubscriptionMode to control the subscription rate. Following example demonstrates this with the subscribeCallback API.\nScala {\n  val subscriber = eventService.defaultSubscriber\n\n  subscriber.subscribeCallback(\n    Set(EventKey(hcd.prefix, EventName(\"filter_wheel\"))),\n    event => { /*do something*/ },\n    1.seconds,\n    SubscriptionModes.RateAdapterMode\n  )\n} Java IEventSubscriber subscriber = eventService.defaultSubscriber();\n\nEventKey filterWheelEventKey = new EventKey(hcdLocation.prefix(), new EventName(\"filter_wheel\"));\nreturn subscriber.subscribeCallback(Set.of(filterWheelEventKey), event -> { /* do something*/ }, Duration.ofMillis(1000), SubscriptionModes.jRateAdapterMode());\nThere are two types of Subscription modes:\nRateAdapterMode which ensures that an event is received exactly at each tick of the specified interval. RateLimiterMode which ensures that events are received as they are published along with the guarantee that no more than one event is delivered within a given interval.\nRead more about Subscription Mode here","title":"Controlling Subscription Rate"},{"location":"/services/event.html#pattern-subscription","text":"The following example demonstrates the usage of the pattern subscribe API with a callback. Events with keys that match the specified pattern and belong to the specified subsystem are received by the subscriber. The callback function provided is called on each event received.\nScala {\n  val subscriber = eventService.defaultSubscriber\n  subscriber.pSubscribeCallback(subsystem, \"*\", event => { /*do something*/ })\n} Java IEventSubscriber subscriber = eventService.defaultSubscriber();\nsubscriber.pSubscribeCallback(subsystem, \"*\", event -> { /* do something*/ });\nWarning DO NOT include subsystem in the provided pattern. Final pattern generated will be provided pattern prepended with subsystem. For Ex. pSubscribe(Subsytem.WFOS, *) will subscribe to event keys matching pattern : wfos.*\nWarning The pattern-based subscribe API is provided because it is useful in testing, but should not be used in production code. The use of certain patterns and many pattern-based subscriptions can impact the overall-performance of the Event Service.","title":"Pattern Subscription"},{"location":"/services/event.html#event-subscription","text":"On subscription to event keys, you receive an EventSubscription which provides following APIs:\nunsubscribe: Used to unsubscribe by destroying the event stream and releasing the the connection to the Event Server. ready: check if event subscription is successful or not.\nYou can find complete list of APIs supported by EventSubscriber and IEventSubscriber with detailed description of each API here:\nEventSubscriber IEventSubscriber","title":"Event Subscription"},{"location":"/services/event.html#create-event-service","text":"If you are not using csw-framework, you can create the EventService using an EventServiceFactory.\nScala // create event service using location service\nval eventService1: EventService = new EventServiceFactory().make(locationService)\n\n// create event service using host and port of event server.\nval eventService2: EventService = new EventServiceFactory().make(\"localhost\", 26379) Java // create event service using host and port of event server.\nIEventService eventService1 = new EventServiceFactory().jMake(locationService, actorSystem);\n\n// create event service using host and port of event server.\nIEventService eventService2 = new EventServiceFactory().jMake(\"localhost\", 26379, actorSystem);\nThe provided implementation of Event Service is backed up by Redis. The above example demonstrates creation of Event Service with default Redis client options. You can optionally supply a RedisClient to the EventStore from outside which allows you to customize the behavior of the RedisClient used by Event Service, which will usually only be required in testing.\nRedisClient is an expensive resource. Reuse this instance as much as possible.\nNote that it is the responsibility of the consumer of this API to shutdown the Redis Client when it is no longer in use.\nScala val clientOptions = ClientOptions.builder().disconnectedBehavior(DisconnectedBehavior.REJECT_COMMANDS).build\nval redisClient   = RedisClient.create()\nredisClient.setOptions(clientOptions)\n\n// create event service using location service\nval eventService1: EventService = new EventServiceFactory(RedisStore(redisClient)).make(locationService)\n\n// create event service using host and port of event server.\nval eventService2: EventService = new EventServiceFactory(RedisStore(redisClient)).make(\"localhost\", 26379) Java ClientOptions clientOptions = ClientOptions.builder().disconnectedBehavior(ClientOptions.DisconnectedBehavior.REJECT_COMMANDS).build();\nRedisClient redisClient   = RedisClient.create();\nredisClient.setOptions(clientOptions);\n\nEventStores.RedisStore redisStore = new EventStores.RedisStore(redisClient);\n// create event service using location service\nIEventService eventService1 = new EventServiceFactory(redisStore).jMake(locationService, actorSystem);\n\n// create event service using host and port of event server.\nIEventService eventService2 = new EventServiceFactory(redisStore).jMake(\"localhost\", 26379, actorSystem);","title":"Create Event Service"},{"location":"/services/event.html#technical-description","text":"See Event Service Technical Description.","title":"Technical Description"},{"location":"/services/event.html#source-code-for-examples-of-creation","text":"Scala Example Java Example","title":"Source code for examples of creation"},{"location":"/services/event.html#source-code-for-examples-of-publishing","text":"Scala Example Java Example","title":"Source code for examples of publishing"},{"location":"/services/event.html#source-code-for-examples-of-subscribing","text":"Scala Example Java Example","title":"Source code for examples of subscribing"},{"location":"/services/alarm.html","text":"","title":"Alarm Service"},{"location":"/services/alarm.html#alarm-service","text":"The Alarm Service provides an API to manage alarms in the TMT software system. The service uses Redis to store Alarm data, including the alarm status and associated metadata. Alarm “keys” are used to access information about an alarm.","title":"Alarm Service"},{"location":"/services/alarm.html#dependencies","text":"The Alarm Service comes bundled with the Framework, no additional dependency needs to be added to your build.sbt file if using it. To use the Alarm service without using the framework, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-alarm-client\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/alarm.html#api-flavors","text":"There are two APIs provided in the Alarm Service: a client API, and an administrative (admin) API. The client API is the API used by component developers to set the severity of an alarm. This is the only functionality needed by component developers. As per TMT policy, the severity of an alarm must be set periodically (within some time limit) in order to maintain the integrity of the alarm status. If an alarm severity is not refreshed within the time limit, currently set at 9 seconds, the severity is set to Disconnected by the Alarm Service, which indicates to the operator that there is some problem with the component’s ability to evaluate the alarm status.\nThe admin API provides all of the functions needed manage the alarm store, as well as providing access to monitor alarms for use by an operator or instrument specialist. The admin API provides the ability to load alarm data into the alarm store, set the severity of an alarm, acknowledge alarms, shelve or unshelve alarms, reset a latched alarm, get the metadata/status/severity of an alarm, and get or subscribe to aggregations of severity and health of the alarm, a component’s alarms, a subsystem’s alarms, or the alarms of the whole TMT System.\nA command line tool is provided as part of the Alarm Service that implements this API and provides low level control over the Alarm Service. More details about alarm CLI can be found here: CSW Alarm Client CLI application\nEventually, operators will use Graphical User Interfaces that access the admin API through a UI gateway. This will be delivered as part of the ESW HCMS package.\nNote Since the admin API will primarily be used with the CLI and HCMS applications, it is only supported in Scala, and not Java.\nTo summarize, the APIs are as follows: * client API (AlarmService) : Must be used by component. Available method is: {setSeverity} * admin API (AlarmAdminService) : Expected to be used by administrator. Available methods are: {initAlarm | setSeverity | acknowledge | shelve | unshelve | reset | getMetaData | getStatus | getCurrentSeverity | getAggregatedSeverity | getAggregatedHealth | subscribeAggregatedSeverityCallback | subscribeAggregatedSeverityActorRef | subscribeAggregatedHealthCallback | subscribeAggregatedHealthActorRef }","title":"API Flavors"},{"location":"/services/alarm.html#creating-clientapi-and-adminapi","text":"For component developers, the client API is provided as an AlarmService object in the CswContext object injected into the ComponentHandlers class provided by the framework.\nIf you are not using csw-framework, you can create AlarmService using AlarmServiceFactory.\nScala // create alarm client using host and port of alarm server\nprivate val clientAPI1 = new AlarmServiceFactory().makeClientApi(\"localhost\", 5225)\n\n// create alarm client using location service\nprivate val clientAPI2 = new AlarmServiceFactory().makeClientApi(locationService)\n\n// create alarm admin using host and port of alarm server\nprivate val adminAPI1 = new AlarmServiceFactory().makeAdminApi(\"localhost\", 5226)\n\n// create alarm admin using location service\nprivate val adminAPI2 = new AlarmServiceFactory().makeAdminApi(locationService) Java // create alarm client using host and port of alarm server\nIAlarmService jclientAPI1 = new AlarmServiceFactory().jMakeClientApi(\"localhost\", 5227, actorSystem);\n\n// create alarm client using location service\nIAlarmService jclientAPI2 = new AlarmServiceFactory().jMakeClientApi(jLocationService, actorSystem);","title":"Creating clientAPI and adminAPI"},{"location":"/services/alarm.html#rules-and-checks","text":"When representing a unique alarm, the alarm name or component name must not have * [ ] ^ - or any whitespace characters","title":"Rules and checks"},{"location":"/services/alarm.html#model-classes","text":"AlarmKey : Represents the unique alarm in the TMT system. It is composed of subsystem, component and alarm name. ComponentKey : Represents all alarms of a component. Used for getting severity or health of an entire component. SubsystemKey : Represents all alarms of a subsystem Used for getting severity or health of an entire subsystem. GlobalKey : Represents all alarms present in the TMT system. Used for getting severity or health of an entire observatory. AlarmMetadata : Represents static metadata of an alarm, which will not change in its entire lifespan. AlarmStatus : Represents dynamically changing data of the an alarm, which will be changing depending on the severity change or manually changed by an operator AlarmSeverity : Represents severity levels that can be set by the component developer e.g. Okay, Indeterminate, Warning, Major and Critical FullAlarmSeverity : Represents all possible severity levels of the alarm i.e. Disconnected (cannot be set by the developer) plus other severity levels that can be set by the developer AlarmHealth : Represents possible health of an alarm or component or subsystem or whole TMT system","title":"Model Classes"},{"location":"/services/alarm.html#client-api","text":"","title":"Client API"},{"location":"/services/alarm.html#setseverity","text":"Sets the severity of the given alarm. The severity must be refreshed by setting it at a regular interval or it will automatically be changed to Disconnected after a specific time.\nScala val alarmKey              = AlarmKey(Prefix(NFIRAOS, \"trombone\"), \"tromboneAxisLowLimitAlarm\")\nval resultF: Future[Done] = clientAPI.setSeverity(alarmKey, Okay) Java private AlarmKey alarmKey = new AlarmKey(Prefix.apply(JSubsystem.NFIRAOS, \"trombone\"), \"tromboneAxisLowLimitAlarm\");\nFuture<Done> doneF = jclientAPI1.setSeverity(alarmKey, JAlarmSeverity.Okay);\nNote If the alarm is not refreshed within 9 seconds, it will be inferred as Disconnected If the alarm is auto-acknowledgable and the severity is set to Okay then, the alarm will be auto-acknowledged and will not require any explicit admin action in terms of acknowledging","title":"setSeverity"},{"location":"/services/alarm.html#admin-api","text":"","title":"Admin API"},{"location":"/services/alarm.html#initalarms","text":"Loads the given alarm data in alarm store, passing in the alarm configuration file.\nScala val resource               = \"test-alarms/valid-alarms.conf\"\nval alarmsConfig: Config   = ConfigFactory.parseResources(resource)\nval result2F: Future[Done] = adminAPI.initAlarms(alarmsConfig)\nAlarm configuration files are written in the HOCON format using the following fields:\nsubsystem: subsystem name the alarm belongs to component: name of component for the alarm, matching the name in the componentInfo file (see Describing Components) name: name of the alarm description: a description of what the alarm represents location: physical location within observatory or instrument in which the alarm condition is occuring alarmType: the general category for the alarm. Must be one of the following: Absolute: An alarm generated when a setpoint is exceeded. BitPattern: An alarm generated when a pattern of digital signals matches a predetermined pattern. Calculated: An alarm generated from a calculated value instead of a direct process measurement. Deviation: An alarm generated when the difference between two analog values exceeds a limit (e.g., deviation between primary and redundant instruments or a deviation between process variable and setpoint). Discrepancy: An alarm generated by error between the comparison of an expected plant or device state to its actual state (e.g., when a motor fails to start after it is command to the on state). Instrument: An alarm generated by a field device to indicate a fault (e.g., a sensor failure). RateChange: An alarm generated when the change in a process variable per unit time, (dPV/dt), exceeds a defined limit. RecipeDriven: An alarm with limits that depend on the recipe that is currently being executed. Safety: An alarm that is tied to and echoing an action or interlock in the subsystem’s safety controller. (Note: At TMT Alarm Service can not a primary hazard control for severe hazards). Statistical: An alarm generated based on statistical properties of one or more process variables. System: An alarm generated by the control system to indicate a fault within the system hardware, software, or components (e.g., unrecoverable communication error). supportedSeverities: list of non-Okay severities the alarm may become (Warning, Major, Critical). All alarms are assumed to support Okay, Disconnected, and Indeterminate. probableCause: a description of the likely cause of the alarm reaching each severity level operatorResponse: instructions or information to help the operator respond to the alarm. isAutoAcknowledgable: true/false flag for whether the alarm automatically acknowledges alarm when alarm severity returns to Okay. isLatchable: true/false flag whether alarm latches at highest severity until reset. activationStatus: true/false flag for whether alarm is currently active (and considered in aggregated severity and health calculations)\nalarms.conf alarms: [\n  {\n    prefix = nfiraos.trombone\n    name = tromboneAxisLowLimitAlarm\n    description = \"Warns when trombone axis has reached the low limit\"\n    location = \"south side\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major, Critical]\n    probableCause = \"the trombone software has failed or the stage was driven into the low limit\"\n    operatorResponse = \"go to the NFIRAOS engineering user interface and select the datum axis command\"\n    isAutoAcknowledgeable = false\n    isLatchable = true\n    activationStatus = Active\n  },\n  {\n    prefix = nfiraos.trombone\n    name = tromboneAxisHighLimitAlarm\n    description = \"Warns when trombone axis has reached the high limit\"\n    location = \"south side\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major]\n    probableCause = \"the trombone software has failed or the stage was driven into the high limit\"\n    operatorResponse = \"go to the NFIRAOS engineering user interface and select the datum axis command\"\n    isAutoAcknowledgeable = true\n    isLatchable = true\n    activationStatus = Active\n  },\n  {\n    prefix = tcs.tcspk\n    name = cpuExceededAlarm\n    description = \"This alarm is activated when the tcsPk Assembly can no longer calculate all of its pointing values in the time allocated. The CPU may lock power, or there may be pointing loops running that are not needed. Response: Check to see if pointing loops are executing that are not needed or see about a more powerful CPU.\"\n    location = \"in computer...\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major, Critical]\n    probableCause = \"too fast...\"\n    operatorResponse = \"slow it down...\"\n    isAutoAcknowledgeable = true\n    isLatchable = false\n    activationStatus = Active\n  },\n  {\n    prefix = lgsf.tcspkinactive\n    name = cpuIdleAlarm\n    description = \"This alarm is activated CPU is idle\"\n    location = \"in computer...\"\n    alarmType = Absolute\n    supportedSeverities = [Warning, Major, Critical]\n    probableCause = \"too fast...\"\n    operatorResponse = \"slow it down...\"\n    isAutoAcknowledgeable = true\n    isLatchable = false\n    activationStatus = Inactive\n  }\n]","title":"initAlarms"},{"location":"/services/alarm.html#acknowledge","text":"Acknowledges the given alarm which is raised to a higher severity\nScala val result3F: Future[Done] = adminAPI.acknowledge(alarmKey)","title":"acknowledge"},{"location":"/services/alarm.html#shelve","text":"Shelves the given alarm. Alarms will be unshelved automatically at a specific time (8 AM local time by default) if it is not unshelved manually before that. The time to automatically un-shelve can be configured in application.conf for e.g csw-alarm.shelve-timeout = h:m:s a.\nScala val result4F: Future[Done] = adminAPI.shelve(alarmKey)\nNote Shelved alarms are also considered in aggregation severity or health calculation of alarms.","title":"shelve"},{"location":"/services/alarm.html#unshelve","text":"Unshelves the given alarm\nScala val result5F: Future[Done] = adminAPI.unshelve(alarmKey)","title":"unshelve"},{"location":"/services/alarm.html#reset","text":"Resets the status of the given latched alarm by updating the latched severity same as current severity and acknowledgement status to acknowledged without changing any other properties of the alarm.\nScala val result6F: Future[Done] = adminAPI.reset(alarmKey)","title":"reset"},{"location":"/services/alarm.html#getmetadata","text":"Gets the metadata of an alarm, component, subsystem, or whole TMT system. The following information is returned for each alarm:\nsubsystem component name description location alarmType supported severities probable cause operator response is autoAcknowledgeable is latchable activation status\nScala val metadataF: Future[AlarmMetadata] = adminAPI.getMetadata(alarmKey)\nmetadataF.onComplete {\n  case Success(metadata)  => println(s\"${metadata.name}: ${metadata.description}\")\n  case Failure(exception) => println(s\"Error getting metadata: ${exception.getMessage}\")\n}\nNote Inactive alarms will not be taking part in aggregation of severity or health. Alarms are set active or inactive in the alarm configuration file, and not through either API.","title":"getMetadata"},{"location":"/services/alarm.html#getstatus","text":"Gets the status of the alarm which contains fields like:\nlatched severity acknowledgement status shelve status alarm time\nScala val statusF: Future[AlarmStatus] = adminAPI.getStatus(alarmKey)\nstatusF.onComplete {\n  case Success(status)    => println(s\"${status.alarmTime}: ${status.latchedSeverity}\")\n  case Failure(exception) => println(s\"Error getting status: ${exception.getMessage}\")\n}","title":"getStatus"},{"location":"/services/alarm.html#getcurrentseverity","text":"Gets the severity of the alarm.\nScala val severityF: Future[FullAlarmSeverity] = adminAPI.getCurrentSeverity(alarmKey)\nseverityF.onComplete {\n  case Success(severity)  => println(s\"${severity.name}: ${severity.level}\")\n  case Failure(exception) => println(s\"Error getting severity: ${exception.getMessage}\")\n}","title":"getCurrentSeverity"},{"location":"/services/alarm.html#getaggregatedseverity","text":"Gets the aggregated severity for the given alarm/component/subsystem/whole TMT system. Aggregation of the severity represents the most severe alarm amongst the aggregated alarms.\nScala val componentKey                                   = ComponentKey(Prefix(NFIRAOS, \"tromboneassembly\"))\nval aggregatedSeverityF: Future[FullAlarmSeverity] = adminAPI.getAggregatedSeverity(componentKey)\naggregatedSeverityF.onComplete {\n  case Success(severity)  => println(s\"aggregate severity: ${severity.name}: ${severity.level}\")\n  case Failure(exception) => println(s\"Error getting aggregate severity: ${exception.getMessage}\")\n}","title":"getAggregatedSeverity"},{"location":"/services/alarm.html#getaggregatedhealth","text":"Gets the aggregated health for the given alarm/component/subsystem/whole TMT system. Aggregation of health is either Good, ill or Bad based on the most severe alarm amongst the aggregated alarms.\nScala val subsystemKey                 = SubsystemKey(IRIS)\nval healthF: Future[AlarmHealth] = adminAPI.getAggregatedHealth(subsystemKey)\nhealthF.onComplete {\n  case Success(health)    => println(s\"${subsystemKey.subsystem.name} health = ${health.entryName}\")\n  case Failure(exception) => println(s\"Error getting health: ${exception.getMessage}\")\n}","title":"getAggregatedHealth"},{"location":"/services/alarm.html#subscribeaggregatedseveritycallback","text":"Subscribes to the changes of aggregated severity for given alarm/component/subsystem/whole TMT system by providing a callback which gets executed for every change.\nScala val alarmSubscription: AlarmSubscription = adminAPI.subscribeAggregatedSeverityCallback(\n  ComponentKey(Prefix(NFIRAOS, \"tromboneAssembly\")),\n  aggregatedSeverity => { /* do something*/ }\n)\n// to unsubscribe:\nval unsubscribe1F: Future[Done] = alarmSubscription.unsubscribe()","title":"subscribeAggregatedSeverityCallback"},{"location":"/services/alarm.html#subscribeaggregatedseverityactorref","text":"Subscribes to the changes of aggregated severity for given alarm/component/subsystem/whole TMT system by providing an actor which will receive a message of aggregated severity on every change.\nScala val severityActorRef = typed.ActorSystem(behaviour[FullAlarmSeverity], \"fullSeverityActor\")\nval alarmSubscription2: AlarmSubscription =\n  adminAPI.subscribeAggregatedSeverityActorRef(SubsystemKey(NFIRAOS), severityActorRef)\n\n// to unsubscribe:\nval unsubscribe2F: Future[Done] = alarmSubscription2.unsubscribe()","title":"subscribeAggregatedSeverityActorRef"},{"location":"/services/alarm.html#subscribeaggregatedhealthcallback","text":"Subscribe to the changes of aggregated health for given alarm/component/subsystem/whole TMT system by providing a callback which gets executed for every change.\nScala val alarmSubscription3: AlarmSubscription = adminAPI.subscribeAggregatedHealthCallback(\n  ComponentKey(Prefix(IRIS, \"ImagerDetectorAssembly\")),\n  aggregatedHealth => { /* do something*/ }\n)\n\n// to unsubscribe\nval unsubscribe3F: Future[Done] = alarmSubscription3.unsubscribe()","title":"subscribeAggregatedHealthCallback"},{"location":"/services/alarm.html#subscribeaggregatedhealthactorref","text":"Subscribes to the changes of aggregated health for given alarm/component/subsystem/whole TMT system by providing an actor which will receive a message of aggregated severity on every change.\nScala val healthActorRef                        = typed.ActorSystem(behaviour[AlarmHealth], \"healthActor\")\nval alarmSubscription4: AlarmSubscription = adminAPI.subscribeAggregatedHealthActorRef(SubsystemKey(IRIS), healthActorRef)\n\n// to unsubscribe\nval unsubscribe4F: Future[Done] = alarmSubscription4.unsubscribe()","title":"subscribeAggregatedHealthActorRef"},{"location":"/services/alarm.html#technical-description","text":"See Alarm Service Technical Description.","title":"Technical Description"},{"location":"/services/alarm.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source code for examples"},{"location":"/services/time.html","text":"","title":"Time Service"},{"location":"/services/time.html#time-service","text":"The Time Service provides APIs to access time in Coordinated Universal Time (UTC) and International Atomic Time (TAI) time scales with up to nanosecond precision when available. It also provides APIs for scheduling periodic and non-periodic tasks in the future, which are optimized for scheduling at up to 1KHz frequency.\nTMT has standardized on the use of Precision Time Protocol (PTP) as the basis of time to achieve sub-microsecond accuracy and precision between computers. The Time Service provides each participating computer with access to time synchronized by PTP.\nAt the telescope site, the Global Positioning System (GPS) provides an absolute time base, and a PTP grand master clock (a hardware device) synchronized to the GPS broadcasts the PTP protocol. Each computer system participating in the PTP system is synchronized with GPS and each other using the PTP protocol. For higher accuracy in time measurements hardware time stamping is required, and those computers should be fitted with PTP capable Network Interface Cards (NIC).\nIn order to read the time with high precision, the Time Service relies on making native calls to the Linux kernel libraries, since Java 8 supports only millisecond precision. Java Native Access (JNA) is used internally in Time Service to make native calls that return the required precision. The implementation of Time Service Scheduler is based on the Akka Scheduler, which is designed for high-throughput tasks rather than long-term, cron-like scheduling of tasks.","title":"Time Service"},{"location":"/services/time.html#dependencies","text":"The Time Service comes bundled with the Framework, no additional dependency needs to be added to your build.sbt file if using it. To use the Time Service without using the framework, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-time-scheduler\" % \"2.0.0-RC2\"\nNote Applicable only to Linux Users. If your machines are not PTP enabled/synced, you need to set the tai_offset kernel variable on your machine. This value is used in APIs like TAITime.now() or TAITime.offset and in scheduling APIs which take TAI time. To set the tai offset, run the following command. sudo ntptime -T 37\n Install ntp, ntpdate packages to get this command on your machine.","title":"Dependencies"},{"location":"/services/time.html#time-service-api-flavors","text":"There are total three APIs provided by the Time Service:\nTMTTime API: This API provides a way to get current UTC or TAI time. TMTTimeHelper API: This API provides additional time zone related functionality on top of TMTTime. Scheduler API: This API provides various methods to schedule future or periodic tasks.","title":"Time Service API Flavors"},{"location":"/services/time.html#tmttime-api","text":"TMTTime represents an instantaneous point in time with nanosecond precision. It is a wrapper around Instant and provides additional information about the timescale of the instant.\nTMTTime supports two timescales:\nCoordinated Universal Time ( UTCTime ) International Atomic Time ( TAITime )","title":"TMTTime API"},{"location":"/services/time.html#get-current-time","text":"Gets the current UTC/TAI time with nanosecond precision.\nScala // get current UTC time\nval utcTime: UTCTime = UTCTime.now()\n\n// get current TAI time\nval taiTime: TAITime = TAITime.now() Java // get current UTC time\nprivate UTCTime utcTime = UTCTime.now();\n\n// get current TAI time\nprivate TAITime taiTime = TAITime.now();\nNote that time is returned as a UTCTime or TAITime object so that it is possible to determine the time scale of the time value by inspection.","title":"Get Current Time"},{"location":"/services/time.html#creating-custom-time-instances","text":"To create custom time instances, you can use the default constructors of UTC and TAI times.\nScala //creating a UTCTime of an hour ago\nval utcTimeOfHourAgo: UTCTime = UTCTime(Instant.now().minusSeconds(3600))\n\n//creating a TAITime of an hour ago\nval taiTimeOfHourAgo: TAITime = TAITime(Instant.now().minusSeconds(3600)) Java //creating a UTCTime of an hour ago\nprivate UTCTime utcTimeOfHourAgo = new UTCTime(Instant.now().minusSeconds(3600));\n\n//creating a TAITime of an hour ago\nprivate TAITime taiTimeOfHourAgo = new TAITime(Instant.now().minusSeconds(3600));","title":"Creating custom time instances"},{"location":"/services/time.html#converting-from-utc-to-tai-time-and-vice-versa","text":"Each time object provides a way to convert to the other.\nScala // UTC to TAI\nval taiTime: TAITime = utcTime.toTAI\n\n// TAI to UTC\nval utcTime0: UTCTime = taiTime.toUTC Java // UTC to TAI\nTAITime taiTime = utcTime.toTAI();\n\n// TAI to UTC\nUTCTime utcTime = taiTime.toUTC();","title":"Converting from UTC to TAI Time and Vice-versa"},{"location":"/services/time.html#tmttimehelper-api","text":"This API provides additional time zone related functionality on top of TMTTime. It allows users to get a Java ZonedDateTime representation of a TMTTime.","title":"TMTTimeHelper API"},{"location":"/services/time.html#at-local-time-zone","text":"Gets the given TMTTime at the local time zone. The local time zone is fetched from the calling system’s default time zone.\nScala // Get UTCTime at local timezone\nval utcLocalTime: ZonedDateTime = TMTTimeHelper.atLocal(utcTime)\n\n// Get TAITime at local timezone\nval taiLocalTime: ZonedDateTime = TMTTimeHelper.atLocal(taiTime) Java // Get UTCTime at local timezone\nZonedDateTime utcLocalTime = TMTTimeHelper.atLocal(utcTime);\n\n// Get TAITime at local timezone\nZonedDateTime taiLocalTime = TMTTimeHelper.atLocal(taiTime);","title":"At Local Time Zone"},{"location":"/services/time.html#at-hawaii-hst-timezone","text":"Gets the given TMTTime at the Hawaii time zone.\nScala // Get UTCTime at Hawaii (HST) timezone\nval utcHawaiiTime: ZonedDateTime = TMTTimeHelper.atHawaii(utcTime)\n\n// Get TAITime at Hawaii (HST) timezone\nval taiHawaiiTime: ZonedDateTime = TMTTimeHelper.atHawaii(taiTime) Java // Get UTCTime at Hawaii (HST) timezone\nZonedDateTime utcHawaiiTime = TMTTimeHelper.atHawaii(utcTime);\n\n// Get TAITime at Hawaii (HST) timezone\nZonedDateTime taiHawaiiTime = TMTTimeHelper.atHawaii(taiTime);","title":"At Hawaii (HST) Timezone"},{"location":"/services/time.html#at-custom-timezone","text":"Gets the given TMTTime at the specified time zone.\nScala // Get UTCTime at specified timezone\nval utcKolkataTime: ZonedDateTime = TMTTimeHelper.atZone(utcTime, ZoneId.of(\"Asia/Kolkata\"))\n\n// Get TAITime at specified timezone\nval taiKolkataTime: ZonedDateTime = TMTTimeHelper.atZone(taiTime, ZoneId.of(\"Asia/Kolkata\")) Java // Get UTCTime at specified timezone\nZonedDateTime utcKolkataTime = TMTTimeHelper.atZone(utcTime, ZoneId.of(\"Asia/Kolkata\"));\n\n// Get TAITime at specified timezone\nZonedDateTime taiKolkataTime = TMTTimeHelper.atZone(taiTime, ZoneId.of(\"Asia/Kolkata\"));","title":"At Custom Timezone"},{"location":"/services/time.html#scheduler-api","text":"This API provides various methods to schedule periodic or non-periodic, one-shot tasks in the future.\nFor component developers, the scheduler API is provided as a TimeServiceScheduler object in the CswContext object injected into the ComponentHandlers class provided by the framework.\nIf you are not using csw-framework, you can create TimeServiceScheduler using TimeServiceSchedulerFactory as follows:\nScala // create time service scheduler using the factory method\nimplicit val actorSystem: typed.ActorSystem[_]         = ctx.system\nimplicit val scheduler: Scheduler                      = actorSystem.scheduler\nimplicit val executionContext: ExecutionContext        = actorSystem.executionContext\nprivate val timeServiceScheduler: TimeServiceScheduler = new TimeServiceSchedulerFactory().make() Java // create time service scheduler using the factory method\nTimeServiceScheduler scheduler = new TimeServiceSchedulerFactory(actorSystem.scheduler()).make( actorSystem.executionContext());\nFor all scheduler calls, an instance of Cancellable is returned which can be used to cancel the execution of the future tasks.","title":"Scheduler API"},{"location":"/services/time.html#schedule-once","text":"Schedules a task to execute once at the given start time. The startTime is a TMTTime and can be either a UTCTime or TAITime.\nScala timeServiceScheduler.scheduleOnce(utcTime) {\n  // do something\n} Java Runnable task = () -> {/* do something*/};\nscheduler.scheduleOnce(utcTime, task);\nWarning Note that callbacks are asynchronous and can be potentially executed in an unknown thread. Therefore, if there is a need to mutate state when the time expires, it is recommended to send a message to an Actor, and keep any mutable state within the actor where it can be managed safely. This is true for any CSW API with a callback. The schedule once with ActorRef can often be used in this scenario.","title":"Schedule Once"},{"location":"/services/time.html#schedule-once-with-actorref","text":"Schedules sending of the message to the provided actorRef at the given start time. The startTime can be either UTCTime or TAITime.\nScala object SchedulingHandler {\n  def behavior: Behavior[UTCTime] = Behaviors.setup { ctx =>\n    //setup required for the actor\n\n    Behaviors.receiveMessage {\n      case _ => // handle the message to execute the task on scheduled time and return new behavior\n        Behaviors.same\n    }\n  }\n}\n\nprivate val actorRef: ActorRef = ctx.spawnAnonymous(SchedulingHandler.behavior).toClassic\n\ntimeServiceScheduler.scheduleOnce(utcTime, actorRef, UTCTime.now())\n Java static class SchedulingHandler {\n\n    public static Behavior<UTCTime> behavior() {\n        // handle the message to execute the task on scheduled time\n        return null;\n    }\n}\n\nCancellable schedule() {\n    ActorRef actorRef = Adapter.toClassic(ctx.asJava().spawnAnonymous(SchedulingHandler.behavior()));\n\n    return scheduler.scheduleOnce(utcTime, actorRef, UTCTime.now());\n}","title":"Schedule Once With ActorRef"},{"location":"/services/time.html#schedule-periodically","text":"Schedules a task to execute periodically at the given interval. The first task is executed once immediately without any initial delay followed by periodic executions. In case you do not want to start scheduling immediately, you can use the overloaded method for schedulePeriodically() with startTime as shown in the next example.\nScala timeServiceScheduler.schedulePeriodically(Duration.ofMillis(50)) { /* do something*/ } Java Runnable task = () -> {/* do something*/};\nscheduler.schedulePeriodically(Duration.ofMillis(50), task);","title":"Schedule Periodically"},{"location":"/services/time.html#schedule-periodically-with-start-time","text":"Schedules a task to execute periodically at the given interval. The task is executed once at the given start time followed by execution of task at each interval. The startTime can be either UTCTime or TAITime.\nScala timeServiceScheduler.schedulePeriodically(utcTime, Duration.ofMillis(50)) { /* do something*/ } Java Runnable task = () -> {/* do something*/};\nscheduler.schedulePeriodically(utcTime, Duration.ofMillis(50), task);\nAs with the schedule once API, there is also a periodic schedule API that takes a message and ActorRef.","title":"Schedule Periodically with Start Time"},{"location":"/services/time.html#technical-description","text":"See Time Service Technical Description.","title":"Technical Description"},{"location":"/services/time.html#source-code-for-tmttime-examples","text":"Scala Example Java Example","title":"Source code for TMTTime examples"},{"location":"/services/time.html#source-code-for-scheduler-examples","text":"Scala Example Java Example","title":"Source code for Scheduler examples"},{"location":"/services/database.html","text":"","title":"Database Service"},{"location":"/services/database.html#database-service","text":"The Database Service is included in TMT Common Software for use by components that need the features of a relational database. The CSW Database Service provides a TMT-standard relational database and connection library. Databases created by the Database Service will be stored reliably at the site during operations.\nThe Database Service provides an API to manage database connections and access data in the TMT Software System. The service provides PostgreSQL as the underlying database server. It uses the Jooq library underneath to manage database access, connection pooling, etc.\nNote Jooq is a Java library that provides a higher level API for accessing data i.e. DDL support, DML support, fetch, batch execution, prepared statements, safety against sql injection, connection pooling, etc. To know more about Jooq and its features, please refer to this link.\nThe Database Service requires PostgreSQL server to be running on a machine. To start the PostgreSQL server for development and testing purposes, refer to Starting Apps for Development.\nOnce the PostgreSQL database is up and running, the Database Service can be used to connect and access data. It is assumed that there will be more than one user type registered with PostgreSQL i.e. for read access, for write access, for admin access, etc.","title":"Database Service"},{"location":"/services/database.html#dependencies","text":"To include the Database Service in a component, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-database\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/database.html#accessing-database-service","text":"The Database Service is accessed differently than other CSW services in that it is not passed to a component through CswContext/JCswContext in the component’s ComponentHandlers. To access Database Service, developers create a DatabaseServiceFactory. A DatabaseServiceFactory can be created anywhere in the code using an ActorSystem and its creation is explained in next section.\nNote Creating a new DatabaseServiceFactory does not mean a new connection to PostgreSQL server will be created. Database connections are managed in a pool by the underlying Database Service implementation. Hence, creating multiple DatabaseServiceFactory instances per component can be considered pretty cheap and harmless. But it is also possible to save the instance returned by DatabaseServiceFactory and pass it around your component.","title":"Accessing Database Service"},{"location":"/services/database.html#connect-with-read-access","text":"Our access approach is that all components can read any Database Service database, and clients that only need read access use the following factory method. However, a writer will need a special username/password with write access as shown below.\nBy default while connecting to PostgreSQL database, the Database Service will provide read access for data. To achieve that, create an instance of DatabaseServiceFactory and use it as shown below:\nScala val dbFactory = new DatabaseServiceFactory(ctx.system)\n\ndbFactory\n  .makeDsl(locationService, \"postgres\")         // postgres is dbName\n  .foreach((dsl: DSLContext) => this.dsl = dsl) // save returned dsl to a local variable Java dbFactory = new DatabaseServiceFactory(ctx.getSystem());\n\ndbFactory\n        .jMakeDsl(cswCtx.locationService(), \"postgres\") // postgres is dbName\n        .thenAccept((DSLContext dsl) -> this.dsl = dsl);        // save the returned dsl to a local variable\nThe underlying database server is registered with the Location Service. makeDsl/jMakeDsl uses locationService to locate the PostgreSQL server running and connect to it. It connects to the database by the provided dbName. It picks the database username and password for read access profile from TMT-standard environment variables called DB_READ_USERNAME for username and DB_READ_PASSWORD for password, hence it is expected that developers will set these environment variables prior to using DatabaseServiceFactory. PostgreSQL should also be initialized with a read-only user and password that agrees with the values in the environment variables. This approach is used to keep from putting database login information in the source code.\nNote See the PostgreSQL docs or this site for help with creating users, passwords, and roles in PostgreSQL. The psql interactive CLI client is provided with PostgreSQL. It can be used to connect to PostgreSQL and create users (as well as many other maintenance commands). If the Database Service is started with csw-services.sh, the database server is started on port 5432. Eventually, all TMT user logins will all have these environment variables set with the agreed upon read-only user and password.\nmakeDsl/jMakeDsl returns a Jooq type DSLContext. DSLContext provides the mechanism to access the data stored in PostgreSQL using the selected JDBC driver underneath. The usage of DSLContext in component development will be explained in later sections.\nHint Any exception encountered while connecting to PostgreSQL server will be wrapped in DatabaseException.","title":"Connect with Read Access"},{"location":"/services/database.html#connect-with-write-access","text":"In order to connect to PostgreSQL for write access (or any other access other than read), use the DatabaseServiceFactory as shown below with different environment variables:\nScala dbFactory\n  .makeDsl(locationService, \"postgres\", \"DB_WRITE_USERNAME\", \"DB_WRITE_PASSWORD\")\n  .foreach((dsl: DSLContext) => this.dsl = dsl) // save returned dsl to a local variable Java dbFactory\n        .jMakeDsl(cswCtx.locationService(), \"postgres\", \"DB_WRITE_USERNAME\", \"DB_WRITE_PASSWORD\")\n        .thenAccept((DSLContext dsl) -> this.dsl = dsl);        // save the returned dsl to a local variable\nHere the username and password for write access is picked from environment variables. e.g. - IRIS_DB_WRITE_USERNAME & IRIS_DB_WRITE_PASSWORD. Hence, it is expected from developers to set environment variables prior to using this method with the user name and password to use for write access.","title":"Connect with Write Access"},{"location":"/services/database.html#connect-for-development-or-testing","text":"For development and testing purposes, all database connection properties can be provided from application.conf including username and password. This will not require setting any environment variables for credentials as described in previous sections. In order to do so, use the DatabaseServiceFactory as shown below:\nScala dbFactory\n  .makeDsl()\n  .foreach((dsl: DSLContext) => this.dsl = dsl) // save returned dsl to a local variable Java dbFactory\n        .jMakeDsl()\n        .thenAccept((DSLContext dsl) -> this.dsl = dsl);        // save the returned dsl to a local variable\nThe reference for providing database properties is shown below:\nreference.conf csw-database = {\n  hikari-datasource {\n    dataSourceClassName = org.postgresql.ds.PGSimpleDataSource\n//    dataSource {\n      //  serverName = <server_name>\n      //  portNumber = <port_number>\n      //  databaseName = <database_name>\n      //  user = <username>\n      //  password = <password>\n//    }\n\n    // Below are the default properties of HikariCP\n    //autoCommit = true\n    //connectionTimeout = 30000 (30 seconds)\n    //idleTimeout = 600000 (10 minutes)\n    //maxLifetime = 600000 (10 minutes)\n    //maximumPoolSize = 10\n    //minimumIdle = 10 (same as max pool size)\n  }\n}\nIn order to override any property shown above, it needs to be defined in application.conf. For example. a sample application.conf can look as follows:\ncsw-database.hikari-datasource.dataSource {\n  serverName = localhost\n  portNumber = 5432\n  databaseName = postgres\n  user = postgres\n  password = postgres\n}\nNote By default, CSW configures HikariCP connection pool for managing connections with PostgreSQL server. To know more about HikariCP please refer to this link.","title":"Connect for Development or Testing"},{"location":"/services/database.html#using-dslcontext","text":"Once the DSLContext is returned from makeDsl/jMakeDsl, it can be used to provide plain SQL to the Database Service and get it executed on the PostgreSQL server.\nThe following sections show examples of most typical SQL use cases.","title":"Using DSLContext"},{"location":"/services/database.html#create","text":"To create a table, use the DSLContext as follows:\nScala val createQuery: Query = dsl.query(\"CREATE TABLE films (id SERIAL PRIMARY KEY, Name VARCHAR (10) NOT NULL)\")\n\nimport csw.database.scaladsl.JooqExtentions.RichQuery\nval createResultF: Future[Integer] = createQuery.executeAsyncScala()\ncreateResultF.foreach(result => println(s\"Films table created with $result\")) Java Query createQuery = dsl.query(\"CREATE TABLE films (id SERIAL PRIMARY KEY, Name VARCHAR (10) NOT NULL)\");\nCompletionStage<Integer> createResultF = createQuery.executeAsync();\ncreateResultF.thenAccept(result -> System.out.println(\"Films table created with \" + result));","title":"Create"},{"location":"/services/database.html#insert","text":"To insert data in a batch, use the DSLContext as follows:\nScala val movie_2 = \"movie_2\"\n\nval queries = dsl.queries(\n  dsl.query(\"INSERT INTO films(name) VALUES (?)\", \"movie_1\"),\n  dsl.query(\"INSERT INTO films(id, name) VALUES (?, ?)\", \"2\", movie_2)\n)\n\nimport csw.database.scaladsl.JooqExtentions.RichQueries\nval batchResultF: Future[List[Int]] = queries.executeBatchAsync()\nbatchResultF.foreach(results => println(s\"executed queries [$queries] with results [$results]\")) Java String movie_2 = \"movie_2\";\n\nQueries queries = dsl.queries(\n        dsl.query(\"INSERT INTO films(name) VALUES (?)\", \"movie_1\"),\n        dsl.query(\"INSERT INTO films(id, name) VALUES (?, ?)\", 2, movie_2)\n);\n\nCompletableFuture<int[]> batchResultF = JooqHelper.executeBatch(queries);\nbatchResultF.thenAccept(results ->\n        System.out.println(\"executed queries [\" + queries + \"] with results [\" + Arrays.toString(results) + \"]\"));\nNote The insert statements above gets mapped to prepared statements underneath at JDBC layer and values like movie_1, movie_2 and 2 from the example are bound to the dynamic parameters of these generated prepared statements. As prepared statements provide safety against SQL injection, it is recommended to use prepared statements instead of static SQL statements whenever there is a need to dynamically bind values. In the above example, two insert statements are batched together and sent to PostgreSQL server in a single call. executeBatchAsync/executeBatch maps to batch statements underneath at JDBC layer.","title":"Insert"},{"location":"/services/database.html#select","text":"To select data from table, use the DSLContext as follows:\nScala // domain model\ncase class Films(id: Int, name: String) // variable name and type should be same as column's name and type in database\n\n// fetch data from table and map it to Films class\nval selectQuery = dsl.resultQuery(\"SELECT id, name FROM films WHERE id = ?\", \"1\")\n\nimport csw.database.scaladsl.JooqExtentions.RichResultQuery\nval selectResultF: Future[List[Films]] = selectQuery.fetchAsyncScala[Films]\nselectResultF.foreach(names => s\"Fetched names of films $names\") Java // domain model\nclass Films {\n    private Integer id;  // variable name (id) and type (Integer) should be same as column's name and type in database\n    private String name; // variable name (name) and type (String) should be same as column's name and type in database\n};\n\n// fetch data from table and map it to Films class\nResultQuery<Record> selectQuery = dsl.resultQuery(\"SELECT id, name FROM films WHERE id = ?\", 1);\nCompletableFuture<List<Films>> selectResultF = JooqHelper.fetchAsync(selectQuery, Films.class);\nselectResultF.thenAccept(names -> System.out.println(\"Fetched names of films \" + names));\nNote Make sure that variable name and type of Films class is same as column’s name and type in database. This is necessary for successful mapping of table fields to domain model class.","title":"Select"},{"location":"/services/database.html#stored-function","text":"To create a stored function, use the DSLContext as follows:\nScala val functionQuery = dsl\n  .query(\n    \"\"\"\n    |CREATE FUNCTION inc(val integer) RETURNS integer AS $$\n    |BEGIN\n    |RETURN val + 1;\n    |END; $$\n    |LANGUAGE PLPGSQL;\n    \"\"\".stripMargin\n  )\n\nval functionResultF: Future[Integer] = functionQuery.executeAsyncScala()\nfunctionResultF.foreach(result => println(s\"Function inc created with $result\")) Java Query functionQuery = dsl.query(\"CREATE FUNCTION inc(val integer) RETURNS integer AS $$\\n\" +\n        \"BEGIN\\n\" +\n        \"RETURN val + 1;\\n\" +\n        \"END; $$\\n\" +\n        \"LANGUAGE PLPGSQL;\");\n\nCompletionStage<Integer> functionResultF = functionQuery.executeAsync();\nfunctionResultF.thenAccept(result -> System.out.println(\"Function inc created with  \" + result));\nSimilarly, any SQL queries can be written with the help of DSLContext including stored procedures.\nNote If there is a syntax error in SQL queries, the Future/CompletableFuture returned will fail with CompletionException and the CompletionStage will fail with an ExecutionException. But both CompletionException and ExecutionException will have Jooq’s DataAccessException underneath as cause.\nThese examples are just a start. Any SQL statement can be created and executed using the DSLContext.","title":"Stored Function"},{"location":"/services/database.html#technical-description","text":"See Database Service Technical Description.","title":"Technical Description"},{"location":"/services/database.html#source-code-for-examples","text":"Scala Example Java Example","title":"Source code for examples"},{"location":"/services/aas.html","text":"","title":"Authentication and Authorization Service (AAS)"},{"location":"/services/aas.html#authentication-and-authorization-service-aas-","text":"THe Authentication and Authorization Service (AAS) is a suite of libraries (i.e. adapters) provided by CSW to help build an ecosystem of client & server side applications that enforce the authentication and authorization policies for TMT.\nThe backbone of AAS is the Keycloak product. It is where all client and server applications need to be registered and configured. Keycloak comes bundled with csw-services.sh so you don’t have to download & install it manually. It is installed the first time you run csw-services.sh.\nIn the current authentication and authorization model for TMT, users will use browser-based graphical user interfaces to perform authentication through AAS. Sequencers, Assemblies and HCDs execute within the authenticated environment, but do not concern themselves directly with authorization and authentication.\nCore concepts & terms Akka HTTP Adapter - csw-aas-http Installed Auth Adapter - csw-aas-installed Javascript Adapter - csw-aas-js","title":"Authentication and Authorization Service (AAS)"},{"location":"/services/aas.html#technical-description","text":"See Authorization and Authentication Service Technical Description.","title":"Technical Description"},{"location":"/services/sequencer-command-service.html","text":"","title":"Sequencer Command Service"},{"location":"/services/sequencer-command-service.html#sequencer-command-service","text":"The SequencerCommandService provides the ability to send a Sequence of commands to a running Sequencer. A future value of SubmitResponse is returned on execution of the provided Sequence.","title":"Sequencer Command Service"},{"location":"/services/sequencer-command-service.html#dependencies","text":"To use the SequencerCommandService, add this to your build.sbt file:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-command\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/services/sequencer-command-service.html#creating-sequencercommandservice","text":"To create SequencerCommandService, you need to resolve the AkkaLocation of the Sequencer using the Location Service. Pass the resolved location to the SequencerCommandServiceImpl, which will return the handle of a SequencerCommandService.\nScala private val connection             = AkkaConnection(ComponentId(Prefix(Subsystem.CSW, \"sequencer\"), ComponentType.Sequencer))\nprivate val location: AkkaLocation = Await.result(locationService.resolve(connection, 5.seconds), 5.seconds).get\n\nval sequencerCommandService: SequencerCommandService = new SequencerCommandServiceImpl(location)","title":"Creating SequencerCommandService"},{"location":"/services/sequencer-command-service.html#submitting-sequence-to-a-sequencer","text":"To submit a Sequence to a Sequencer, SequencerCommandService provides a submit API which takes a Sequence and returns a Future[SubmitResponse].\nIf the sequencer is idle, the provided sequence is loaded in the sequencer and execution of the sequence starts immediately, and a Started response is returned. If the sequencer is already running another sequence, an Invalid response is returned.\nScala val sequence: Sequence        = Sequence(Setup(Prefix(\"test.move\"), CommandName(\"command-1\"), None))\nimplicit val timeout: Timeout = Timeout(10.seconds)\n\nprivate val initialResponse: SubmitResponse = Await.result(sequencerCommandService.submit(sequence), 5.seconds)\n\nprivate val queryResponse: SubmitResponse = Await.result(sequencerCommandService.query(initialResponse.runId), 5.seconds)\n\nprivate val queryFinalResponse: SubmitResponse =\n  Await.result(sequencerCommandService.queryFinal(initialResponse.runId), 5.seconds)\nquery or queryFinal Apis, as shown above, could be used to query for the sequence result after the sequence is submitted. query returns the current response which could be either final response (eg. Completed) or intermediate response (eg. Started). Whereas queryFinal will wait for the final response of the sequence for the given timeout. This Api will never return an intermediate response.\nIf you are not interested in initial/intermediate response but only in final response, you can use the submitAndWait api which submits the sequence and waits for the final response if the sequence was successfully Started.\nScala private val finalResponse: SubmitResponse = Await.result(sequencerCommandService.submitAndWait(sequence), 5.seconds)","title":"Submitting Sequence to a Sequencer"},{"location":"/commons/apps.html","text":"","title":"Applications"},{"location":"/commons/apps.html#applications","text":"Starting Apps for Development Starting Elastic Logging Aggregator for Development csw-location-server csw-location-agent csw-config-server csw-config-cli csw-event-cli csw-alarm-cli csw-host-config","title":"Applications"},{"location":"/commons/apps.html#starting-apps-for-development","text":"In order to run a component, it is required to run csw-location-server. Moreover, even to start the Event Service or Configuration Service, csw-location-server should be running.\nA shell script has been provided to make the starting of services more convenient for developers. This script will start the Location Service, and then a set of CSW services that a developer may make use of. All services are started by default, but specific services can be started using the command line options (see below).\nAssuming that developer has downloaded csw-apps-<some-version>.zip from csw releases and unzipped it, there are four folders, as follows, in csw-apps-<some-version>\nbin lib logging_aggregator conf\nAll the shell scripts provided by CSW reside in bin folder. The shell script referred in this segment is named as csw-services.sh. Go to the bin folder and type ./csw-services.sh --help. This will list all possible options applicable for the script.\nEnvironment variables which are needed to start CSW services are documented in environment variables.\nNote This shell script will start csw-location-server as the first step regardless of any options provided.\nThe execution of the script is such that it starts csw-location-server, then start all the CSW services, unless one or more of the following options are specified.\n--all | -a if provide, starts all the services mentioned below --config | -c if provided, starts configuration service. --event | -e if provided, starts event service. --alarm | -r if provided, starts alarm service. --database | -d if provided, starts database service. --auth | -k if provided, starts authentication service.\nNote While starting the Database Service, make sure that The PGDATA environment variable is set to the Postgres data directory where Postgres is installed e.g. for mac: “/usr/local/var/postgres” and there is a password set for the valid Postgres user. If not, go to the Postgres shell via psql and run ALTER USER <username> WITH PASSWORD '<mypassword>';.\nTo start all the CSW services, run ./csw-services.sh start --all command.\nWith this, the component code is now ready to connect to the provided services via csw-services.sh.\ncsw-services.sh script additionally support custom CSW version as a command line argument along with others mentioned above. Let’s say you want to start all the services for CSW version of 2.0.0, then you can run following command:\n./csw-services.sh start --all -v 2.0.0\nNote By default csw-services.sh script runs services in the foreground, you can press ctr+c to stop all the services. If you start services in background by running ./csw-services.sh start -a & command, then you can stop these services using csw-services.sh stop command.","title":"Starting Apps for Development"},{"location":"/commons/apps.html#starting-elastic-logging-aggregator-for-development","text":"Elastic stack (Elasticsearch, Logstash, Kibana and Filebeat) is used to aggregate logs generated from TMT applications (Scala/Java/Python/C++/C) and CSW services (mentioned in the previous section). For development purposes, Docker compose is used. Hence, make sure that latest Docker setup is installed and running before starting the Elastic stack. To know more about how Elastic stack works please refer to Logging Aggregator.\nFor the host setup, follow the below given steps:\nInstall Docker version 18.09+ Install Docker Compose version 1.24.0+\nOn distributions which have SELinux enabled out-of-the-box, you will need to either re-context the files or set SELinux into Permissive mode in order for docker-elk to start properly. For example, on Redhat and CentOS, the following will apply the proper context:\n$ chcon -R system_u:object_r:admin_home_t:s0 docker-elk/\nTo know more about running Docker for Mac please refer to this link. For Windows, ensure that the “Shared Drives” feature is enabled for the C: drive (Docker for Windows > Settings > Shared Drives). See Configuring Docker for Windows Shared Drives (MSDN Blog).\nAssuming that the developer has downloaded csw-apps-<some-version>.zip from csw releases and unzipped it, there are four folders, as follows, in csw-apps-<some-version>:\nbin lib logging_aggregator conf\nGo to logging_aggreator/dev and run\ndocker-compose build --no-cache docker-compose up --force-recreate\nThis will start Filebeat, Elasticsearch, Logstash and Kibana in a Docker container. Note that csw-services.sh will generate all log files under /tmp/tmt/logs/csw and Filebeat will watch for them there.\nOnce, the Docker container is up, open an browser and go to http://localhost:5601/ to use Kibana. Go to:\nManagement -> Kibana -> Index Patterns and create an index pattern as per the requirement. Discover -> Select the index pattern created and explore\nTo use a different Elastic Stack version than the one currently available in the repository, simply change the version in logging_aggreator/dev/.env file, and rebuild the stack with:\ndocker-compose build --no-cache docker-compose up --force-recreate\nAlways pay attention to the upgrade instructions for each individual component before performing a stack upgrade.","title":"Starting Elastic Logging Aggregator for Development"},{"location":"/apps/cswlocationserver.html","text":"","title":"csw-location-server"},{"location":"/apps/cswlocationserver.html#csw-location-server","text":"Note: Normally you will not need to start this application manually. The csw-services.sh script does this for you.\nThis application will start a HTTP CSW Location Server on port 7654 which is required for all Location Service consumers who uses HTTP Location client. All components (HCD’s, Assemblies, Services etc.) use a local HTTP Location client which expects the Location Server running at localhost:7654. In a production environment, it is required that all machines running components should have the HTTP Location Server running locally.","title":"csw-location-server"},{"location":"/apps/cswlocationserver.html#prerequisite","text":"The CSW Location Server application can be installed as binaries or constructed from source. To download the application, go to the CSW Release page and follow instructions.\nTo install from source, the command sbt csw-location-server/universal:publishLocal will publish necessary artifacts to run the csw-location-server application. The target of the above command is a zip file titled “csw-location-server.zip” and its path will be printed on console.\nNote: An alternative method is to run sbt publishLocal stage, which installs all the dependencies locally and also installs all the csw applications in target/universal/stage/bin.\nUnzip either the downloaded or constructed zip file and switch current working directory to the extracted folder. Choose appropriate instructions from below based on requirement (i.e. single machine or multiple machines).","title":"Prerequisite"},{"location":"/apps/cswlocationserver.html#starting-location-server-on-a-single-machine","text":"The steps below describe how to run the Location Server on a single machine. This can be a requirement for testing or demo purposes.\nPreparation: Find out the IP address and dedicated port for running the Location Server. Assume that IP is 192.168.1.21 and port is 3552.\nProvisioning: Make sure you set all necessary environment variables.\nRunning: Switch to the application directory and run this command - ./bin/csw-location-server --clusterPort=3552","title":"Starting Location Server on a single machine"},{"location":"/apps/cswlocationserver.html#starting-location-server-on-two-machines","text":"The steps below describe how to run the Location Server on multiple machines, which is the recommended set-up for production usage.\nPreparation: Identify machines which are running the Location Server and whose IP and port are known. Let’s assume they are two for now, and the IP address for machine1 is 192.168.1.21 and for machine2 is 192.168.1.22. Also, they will both have dedicated port 3552 to run the Location Server.\nProvisioning: Make sure you set all necessary environment variables.\nSwitch to application directory and run this command on machine1 and machine2 - ./bin/csw-location-server --clusterPort=3552\nNote : Outside below means any machine not present in this Akka cluster.\nIn production environment, you may need a capability to access protected resources of Location Server and provide Authentication and Authorization for such resources. E.g. ability to register/unregister components(which has to undergo maintenance) from a system operator machine present Outside.\nTo enable this, an additional command line argument is available when starting Location Server Default is local-only mode and such location server will not be accessible from Outside Other option is --publicNetwork, it means start location server in public mode with auth enabled Switch to application directory and run this command on all machines where you want Location Server in local-only mode (authentication and authorization disabled) - ./bin/csw-location-server --clusterPort=3552 Switch to application directory and run this command on all machines where you want Location Server in public mode (authentication and authorization enabled) - ./bin/csw-location-server --clusterPort=3552 --publicNetwork Once Akka cluster is up, start Authentication and Authorization Service on one of the node where Location Server is running in local-only mode, so that it can register itself to this Location Server without the need of authentication and authorization. Other Location Server instances including public mode instances will get location of Authentication and Authorization Service automatically. When any application(E.g. Dashboard application used by Operator to monitor System health) wants to access protected resource of Location Server, it can connect to any public mode Location Server, pass a valid token and access it.","title":"Starting Location Server on two machines"},{"location":"/apps/cswlocationserver.html#help","text":"Use the following command to get help on the options available with this app.\n./bin/csw-location-server --help","title":"Help"},{"location":"/apps/cswlocationserver.html#version","text":"Use the following command to get version information for this app.\n./bin/csw-location-server --version","title":"Version"},{"location":"/apps/cswlocationagent.html","text":"","title":"csw-location-agent"},{"location":"/apps/cswlocationagent.html#csw-location-agent","text":"A utility application that starts a given external (non-CSW) program, registers a comma separated list of services with the Location Service, and unregisters them when the program exits.","title":"csw-location-agent"},{"location":"/apps/cswlocationagent.html#command-line-parameter-options","text":"--names is a required parameter. It needs to be a list of comma separated service names without a space after comma. --command is an optional parameter. The command that starts the target application. Use use %port to specify the port number. If the parameter is not provided, the value $name.command from the config file will be picked up. If the value in the config file is not found, the service names provided will be registered with Location Service. --port the optional port number the application listens on (default: use the value of $name.port from the config file, or use a random, free port.) an optional config file in HOCON format. Will be automatically picked based on –names parameter (Options specified as: $name.command, $name.port, etc.) --delay the number of milliseconds to wait for the app to start before registering itself with the Location Service (default: 1000) --http is an optional parameter. To register services as HTTP with the provided path. (default: none, i.e Services will be registered as Tcp) --no-exit For testing: prevents application from exiting after running command --help Prints the help message. --version Prints the version of the application.","title":"Command line parameter options"},{"location":"/apps/cswlocationagent.html#examples","text":"csw-location-agent --name \"redis\" --command \"redis-server /usr/local/etc/redis.conf\" --port 6379\n Application will start a Redis server on port 6379 (the default Redis port) and will register a TcpConnection for it with the Location Service csw-location-agent --name \"foo\" --command \"sleep 30\"\n Application will sleep for 30 seconds. It will be registered as a service named foo on a random port with the Location Service. After the sleep is over in 30 seconds, it will unregister the foo service. csw-location-agent --name \"myHttpServiceAsTcp\" --command \"python -m SimpleHTTPServer 8080\" --port 8080\n Application will start a simple HTTP service on port 8080. This will register myHttpServiceAsTcp as a TCP service with the Location Service. csw-location-agent --name \"myHttpServiceAsHttp\" --command \"python -m SimpleHTTPServer 8080\" --port 8080 --http \"path\"\n Application will start a simple HTTP service on port 8080. This will register myHttpServiceAsHttp as a HTTP service with the provided path with the Location Service. csw-location-agent --help\n Prints help message csw-location-agent --version\n Prints application version\nNote Before running csw-location-agent, make sure that csw-location-server is running on the local machine at localhost:7654, since the location agent uses a local HTTP location client which expects the Location Server to be running locally.","title":"Examples"},{"location":"/apps/cswonfigserverapp.html","text":"","title":"csw-config-server"},{"location":"/apps/cswonfigserverapp.html#csw-config-server","text":"A HTTP server application that hosts the Configuration Service.","title":"csw-config-server"},{"location":"/apps/cswonfigserverapp.html#prerequisites","text":"The HTTP server hosting the Configuration Service needs to be part of the csw-cluster so that it can be consumed by other components. A required check before starting the Configuration Service is to ensure the csw-cluster is setup and the Location Server are available. Kindly refer to CSW Location Server for more information about the Location Server setup.","title":"Prerequisites"},{"location":"/apps/cswonfigserverapp.html#command-line-parameter-options","text":"--initRepo is an optional parameter. When supplied, the server will try to initialize a repository if it does not exist. --port is an optional parameter. When specified, the HTTP server will start on this port. Default will be 4000. --help prints the help message. --version prints the version of the application.","title":"Command line parameter options"},{"location":"/apps/cswonfigserverapp.html#examples","text":"csw-config-server --initRepo\n Start an HTTP server on default port 4000. Initialize the repository if it does not exist and register it with the Location Service csw-config-server --initRepo --port 4001\n Start an HTTP server on port 4001. Initialize the repository if it does not exist and register it with the Location Service csw-config-server --help\n Prints help message csw-config-server --version\n Prints application version\nNote Before running csw-config-server, make sure that csw-location-server is running on local machine at localhost:7654. As config server uses a local HTTP Location client which expects the Location Server to be running locally.","title":"Examples"},{"location":"/apps/cswconfigcli.html","text":"","title":"csw-config-cli"},{"location":"/apps/cswconfigcli.html#csw-config-cli","text":"A command line application that facilitates interaction with the Configuration Service. It accepts various commands to store, retrieve, list and manage configuration files.","title":"csw-config-cli"},{"location":"/apps/cswconfigcli.html#supported-commands","text":"login logout create update get delete list history setActiveVersion resetActiveVersion getActiveVersion getActiveByTime getMetadata exists getActive","title":"Supported Commands"},{"location":"/apps/cswconfigcli.html#login","text":"create, update, delete, setActiveVersion & resetActiveVersion commands are protected behind authorization and require admin role. In order to use those, you need to login a with an account having admin privileges.\nFor development and testing purposes, AAS comes pre-bundled with a user account for config service\nusername = config-admin\npassword = config-admin\nNote These credentials will not be available in actual production environment.\nFor development and testing purposes, AAS comes pre-bundled with the following user accounts for config service:\nusername: admin password: tmt-admin role: admin username: dev password: tmt-dev role: NONE\nNote These credentials will not be available in actual production environment.","title":"login"},{"location":"/apps/cswconfigcli.html#examples","text":"csw-config-cli login\n This opens up default browser on your machine and asks you to provide username and password. Once you provide valid credentials, AAS will respond with an access token, refresh token etc. which get stored on the local filesystem. So next time when you use any of the above admin protected commands, this access token gets retrieved from local filesystem and is implicitly passed in a request sent to the Config Server. csw-config-cli login --consoleLogin\n Instead of opening the default browser on your machine, this will prompt for username and password on the console. (You do not need to leave console in this case.)","title":"Examples"},{"location":"/apps/cswconfigcli.html#logout","text":"Use this command to logout if you are already logged in or you want to re-login with different credentials.","title":"logout"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli logout\nThis command will delete all the tokens stored in local filesystem.","title":"Example"},{"location":"/apps/cswconfigcli.html#admin-api","text":"The commands listed below will be used by administrators and maintainers of Configuration Service.","title":"Admin API"},{"location":"/apps/cswconfigcli.html#create","text":"Takes an input source file and creates the configuration in the repository at a specified path.\n‘relativeRepoPath’ is path in the repository -i, --in is input file path --annex is optional parameter. Add this option to specify if the input file must be saved to annex store. This is usually the case if file is binary/large(>10 MiB) -c, --comment optional create comment","title":"create"},{"location":"/apps/cswconfigcli.html#examples","text":"csw-config-cli create /path/hcd/trombone.conf -i /Users/admin/configs/trombone.conf -c \"Initial version\"\n Creates a config file at path /path/hcd/trombone.conf, using the local file at /Users/admin/configs/trombone.conf, with Initial version as a comment. csw-config-cli create /path/hcd/trombone.conf -i /Users/admin/large-configs/bigBinary.conf --annex\n Creates a config file at path /path/hcd/trombone.conf, using the local file at /Users/admin/large-configs/bigBinary.conf, file will be stored in annex store.","title":"Examples"},{"location":"/apps/cswconfigcli.html#update","text":"Overwrites the file specified in the repository by the input file.\n‘path’ is path in the repository -i, --in is input file path -c, --comment optional create comment","title":"update"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli update /path/hcd/trombone.conf -i /Users/foo/new_trombone.conf -c \"new conf for next observation\"\nUpdates repository file /path/hcd/trombone.conf, with a local file at /Users/foo/new_trombone.conf, using the specified comment.","title":"Example"},{"location":"/apps/cswconfigcli.html#get","text":"Retrieves a file for a given path and saves it to the output file. The latest file is fetched if neither date nor id is specified.\n‘relativeRepoPath’ is path in the repository. -o, --out is output file path --id optional. if specified this id will be matched --date optional. if specified will get the file matching this date. Format: 2017-04-16T16:15:23.503Z","title":"get"},{"location":"/apps/cswconfigcli.html#examples","text":"1.\ncsw-config-cli get /path/hcd/trombone.conf -o /Users/bar/temp/latest_trombone.conf\nGets repository file /path/hcd/trombone.conf, stores at the local disk location /Users/bar/temp/latest_trombone.conf\n2.\ncsw-config-cli get /path/hcd/trombone.conf -o /Users/bar/temp/old_trombone.conf --id 10\nGets version revision 10 of the repository file /path/hcd/trombone.conf, stores at the local disk location /Users/bar/temp/old_trombone.conf","title":"Examples"},{"location":"/apps/cswconfigcli.html#delete","text":"Deletes the file at the specified path in the repository.\n‘relativeRepoPath’ is path in the repository -c, --comment optional delete comment","title":"delete"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli delete /path/hcd/outdated_trombone.conf -c monthly maintainance activity\nDeletes the repository file /path/hcd/outdated_trombone.conf, if it exists, using a comment","title":"Example"},{"location":"/apps/cswconfigcli.html#list","text":"Lists the files in the repository. Can’t use ‘–annex’ and ‘–normal’ together.\n--annex optional parameter. List all files that are of annex type. --normal optional parameter. List all files that are of normal type. --pattern optional parameter. List all files whose path matches the given pattern. e.g. “/path/hcd/*.*”, “a/b/c/d.*”, “.*.conf”, “.*hcd.*”","title":"list"},{"location":"/apps/cswconfigcli.html#history","text":"Shows the version history of the file in the repository.\n‘relativeRepoPath’ is path in the repository --max optional parameter, indicating the maximum number of files to be retrieved","title":"history"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli history /path/hcd/trombone.conf --max 25\nPrints the history of repository file /path/hcd/trombone.conf, with only 25 entries.","title":"Example"},{"location":"/apps/cswconfigcli.html#setactiveversion","text":"Sets the active version of the file in the repository.\n‘relativeRepoPath’ is path in the repository --id optional parameter, specifying the version ID of the repository file to set as active. -c, --comment optional delete comment","title":"setActiveVersion"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli setActiveVersion /path/hcd/trombone.conf --id 4 -c restoring last successful version.\nSets revision 4 to be active for the repository file /path/hcd/trombone.conf, using a comment.","title":"Example"},{"location":"/apps/cswconfigcli.html#resetactiveversion","text":"Resets the active version to the latest version for the specified file path.\n‘relativeRepoPath’ is path in the repository -c, --comment optional reset comment","title":"resetActiveVersion"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli resetActiveVersion /path/hcd/trombone.conf -c testing most recent config\nSets latest revision to be active for the repository file /path/hcd/trombone.conf, using a comment.","title":"Example"},{"location":"/apps/cswconfigcli.html#getactiveversion","text":"Gets the ID of the active version of the file in the repository.\n‘relativeRepoPath’ is path in the repository","title":"getActiveVersion"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli getActiveVersion /path/hcd/trombone.conf\nGets active version ID for the repository file /path/hcd/trombone.conf.","title":"Example"},{"location":"/apps/cswconfigcli.html#getactivebytime","text":"Gets the file that was active at a specified time.\n‘relativeRepoPath’ is path in the repository -o, --out is output file path --date optional. If specified, will get the active file matching this date. Format: 2017-04-16T16:15:23.503Z","title":"getActiveByTime"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli getActiveByTime /path/hcd/trombone.conf -o /usr/tmp/last_week_trombone.conf --date 2017-05-09T07:29:53.242Z\nGets version of teh repository file /path/hcd/trombone.conf, that was active on 2017-05-09T07:29:53.242Z, and saves it to local disk.","title":"Example"},{"location":"/apps/cswconfigcli.html#getmetadata","text":"Gets the metadata of the Configuration Service server e.g. repository directory, annex directory, min annex file size, max config file size.","title":"getMetadata"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli getMetadata\nPrints the metadata on screen.","title":"Example"},{"location":"/apps/cswconfigcli.html#client-api","text":"The following commands are available for component developers.","title":"Client API"},{"location":"/apps/cswconfigcli.html#exists","text":"Checks if the file exists at specified path in the repository.\n‘relativeRepoPath’ is path in the repository","title":"exists"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli exists /path/hcd/trombone.conf\nTrue if repository file /path/hcd/trombone.conf exists, false otherwise","title":"Example"},{"location":"/apps/cswconfigcli.html#getactive","text":"Retrieves active file for a given path from the Configuration Service and writes it to the output file. * ‘relativeRepoPath’ is path in the repository * -o, --out is output file path","title":"getActive"},{"location":"/apps/cswconfigcli.html#example","text":"csw-config-cli getActive /path/hcd/trombone.conf -o /Users/bar/temp/scheduled_trombone.conf\nGets currently active version of the repository file /path/hcd/trombone.conf, stores to the local disk location /Users/bar/temp/scheduled_trombone.conf","title":"Example"},{"location":"/apps/cswconfigcli.html#about-this-application","text":"","title":"About this application"},{"location":"/apps/cswconfigcli.html#help","text":"Prints the help message.","title":"–help"},{"location":"/apps/cswconfigcli.html#version","text":"Prints the version of the application.\nNote All the above examples require that csw-location-server is running on local machine at localhost:7654. If csw-location-server is running on a remote machine with an IP address or 172.1.1.2, then you need to pass the additional --locationHost 172.1.1.2 command line argument. Example: csw-config-cli getMetadata --locationHost 172.1.1.2","title":"–version"},{"location":"/apps/csweventcli.html","text":"","title":"csw-event-cli"},{"location":"/apps/csweventcli.html#csw-event-cli","text":"A command line application that facilitates interaction with Event Service. It accepts various commands to publish and subscribe to events.","title":"csw-event-cli"},{"location":"/apps/csweventcli.html#supported-commands","text":"inspect get publish subscribe","title":"Supported Commands"},{"location":"/apps/csweventcli.html#inspect","text":"Takes a comma separated list of events and displays each event’s parameter information which includes key name, key type, and unit along with metadata (event key, timestamp & id).\n-e, --events : comma separated list of events to inspect","title":"inspect"},{"location":"/apps/csweventcli.html#examples-","text":"csw-event-cli inspect -e wfos.prog.cloudcover,wfos.prog.filter\nNote inspect command does not display parameter values. To view values, use get command instead.","title":"Examples:"},{"location":"/apps/csweventcli.html#get","text":"Takes a comma separated list of events with nested key paths and displays event information including values either in oneline or JSON format.\n-e, --events comma separated list of events in the form of <event1:key1>,<event2:key2:key3>, use : to separate multiple keys for same event. Ex. -e a.b.c:struct1/ra,x.y.z:struct2/dec:epoch -o, --out output format, default is oneline -t, --timestamp display timestamp --id display event id -u, --units display units","title":"get"},{"location":"/apps/csweventcli.html#examples-","text":"csw-event-cli get -e wfos.prog.cloudcover\n Displays all keys information in oneline form for event wfos.prog.cloudcover csw-event-cli get -e wfos.prog.cloudcover:struct1/ra:epoch -t --id -u\n Displays information of only struct1/ra and epoch keys as well as timestamp, event id and units of provided keys in oneline form for event wfos.prog.cloudcover csw-event-cli get -e wfos.prog.cloudcover:epoch,wfos.prog.filter:ra\n Displays information of epoch of event wfos.prog.cloudcover and ra key of event wfos.prog.filter:ra csw-event-cli get -e wfos.prog.cloudcover:epoch -o json\n Displays event wfos.prog.cloudcover with only epcoh key in JSON format.\nNote -t, --id & --u options are not applicable when -o json option is provided. An Event displayed in JSON format will always have timestamp, event id and units irrespective of whether those options are provided via the CLI.","title":"Examples:"},{"location":"/apps/csweventcli.html#publish","text":"Publishes an event to the Event Server from the provided input data file or CLI params.\n-e, --event event key to publish --data absolute file path which contains event in JSON format --params pipe ‘|’ separated list of params enclosed in double quotes in the form of \"keyName:keyType:unit=values| ...\". unit is optional here. Supported key types are: [i = IntKey | s = StringKey | f = FloatKey | d = DoubleKey | l = LongKey | b = BooleanKey]. You can optionally choose to enclose param values in [, ] brackets. Values of a string key should be provided in single quotes and use backslash to escape string. Ex. \"addressKey:s=['Kevin O\\'Brien','Chicago, USA']|timestampKey:s=['2016-08-05T16:23:19.002']\" -i, --interval interval in milliseconds to publish event. A single event will be published, if not provided -p, --period publish events for this duration in seconds on provided interval. Default is 2147483 seconds.\nNote If --data & --params are provided together, then the Event is generated from both --data file & --params option. --params takes a precedence and overrides params from the Event data file if it is already present in the file. Option -p should be used with -i, otherwise -p is ignored.","title":"publish"},{"location":"/apps/csweventcli.html#examples-","text":"csw-event-cli publish -e wfos.prog.cloudcover --data /path/to/event.json\n Creates event from provided JSON file and publishes it with key wfos.prog.cloudcover to the Event Server. csw-event-cli publish -e wfos.prog.cloudcover --data /path/to/event.json -i 500 -p 60\n Creates an Event from provided JSON file and publishes it every 500ms for duration of 60s. csw-event-cli publish -e wfos.prog.cloudcover --params \"k1:s=['Kevin O\\'Brien','Chicago, USA']|k2:s=['2016-08-05T16:23:19.002']\"\n First fetches already published Event for key wfos.prog.cloudcover from the Event Server and then updates that Event with provided --params If provided keys are already present in existing Event, then those will be updated. Otherwise, new param entries will be added to the Event. If no Event is published in past for the provided key, then the new Event gets created with the provided params and Event key.","title":"Examples:"},{"location":"/apps/csweventcli.html#subscribe","text":"Takes a comma separated list of Events with nested key paths and displays continuous stream of Event information as soon as it receives the Event.\n-e, --events comma separated list of Events in the form of <event1:key1>,<event2:key2:key3>, use : to separate multiple keys for the same Event. Ex. -e a.b.c:struct1/ra,x.y.z:struct2/dec:epoch -i, --interval interval in milliseconds which to receive an Event -o, --out output format, default is oneline -t, --timestamp display timestamp --id display event id -u, --units display units","title":"subscribe"},{"location":"/apps/csweventcli.html#examples-","text":"csw-event-cli subscribe -e wfos.prog.cloudcover\n Subscribes to Event key wfos.prog.cloudcover and displays all key information as soon as there is an Event published for key wfos.prog.cloudcover with the oneline format. csw-event-cli subscribe -e wfos.prog.cloudcover:struct1/ra:epoch -t --id -u\n Subscribes to the Event key wfos.prog.cloudcover and displays information of only the struct1/ra and epoch keys along with timestamp, event id and units of tge provided keys in oneline format as soon as there is an Event published for the key wfos.prog.cloudcover. csw-event-cli subscribe -e wfos.prog.cloudcover -i 500\n Subscribes to the Event key wfos.prog.cloudcover and displays all key information at provided interval <500ms>. Irrespective of whether there are multiple Events published for the key wfos.prog.cloudcover within 500ms interval or not, at every tick (i.e. 500ms), the latest Event information will be displayed on the console. csw-event-cli subscribe -e wfos.prog.cloudcover:epoch -o json\n Subscribes to Event key wfos.prog.cloudcover and displays only epoch key information as soon as there is an Event published for key wfos.prog.cloudcover, in JSON format.\nNote -t, --id & --u options are not applicable when the -o json option is provided. An Event displayed in json format will always have timestamp, event id and units irrespective of whether those options are provided via the CLI.","title":"Examples:"},{"location":"/apps/csweventcli.html#about-this-application","text":"","title":"About this application"},{"location":"/apps/csweventcli.html#help","text":"Prints the help message.","title":"--help"},{"location":"/apps/csweventcli.html#version","text":"Prints the version of the application.\nNote All the above examples require that csw-location-server is running on local machine at localhost:7654. If csw-location-server is running on a remote machine with an IP address of 172.1.1.2, then you need to pass the additional --locationHost 172.1.1.2 command line argument. Example: csw-event-cli get -e wfos.prog.cloudcover --locationHost 172.1.1.2","title":"--version"},{"location":"/apps/csweventcli.html#testing-development","text":"While testing or development, in order to use this CLI application, below prerequisites must be satisfied:\ncsw-location-server application is running. csw-location-agent application is running, which has started the Event Server and registered it to the Location Service.\nPlease refer to Starting Apps for Development section for more details on how to start these applications using csw-services.sh script.","title":"Testing/Development"},{"location":"/apps/csweventcli.html#monitor-statistics","text":"Event Service uses redis as the event store. Using redis-cli, you can monitor continuous stats about the Event Service.\n$ redis-cli --stat\n------- data ------ --------------------- load -------------------- - child -\nkeys       mem      clients blocked requests            connections\n305        20.70M   605     0       1771418 (+0)        615\n305        20.71M   605     0       1825363 (+53945)    615\n305        20.70M   605     0       1877638 (+52275)    615\n305        20.71M   605     0       1910198 (+32560)    615\n305        20.71M   605     0       1960837 (+50639)    615\n305        20.74M   605     0       2001565 (+40728)    615\nIn the above example, a new line is printed every second with useful information, including the difference between current and old data points.\nkeys: Represents all the keys present in the Redis database, which in case of the Event Service are EventKeys clients: Represents total number of clients currently connected to the Redis server requests: Represents total number of Redis commands processed along with a delta between every interval, specified with the -i option (see below) connections: Represents total number of socket connections opened to the Redis server\nThe -i <interval> option in this case works as a modifier in order to change the frequency at which new lines are emitted. The default is one second.\nYou can explicitly pass the hostname and port of the Redis server while running redis-cli\n$ redis-cli -h redis.tmt.org -p 6379\nA detailed list of operations you can perform with redis-cli can be found here","title":"Monitor statistics"},{"location":"/apps/cswalarmcli.html","text":"","title":"csw-alarm-cli"},{"location":"/apps/cswalarmcli.html#csw-alarm-cli","text":"A command line application that facilitates interaction with the Alarm Service. It accepts various commands to load and retrieve alarm data, to subscribe to severity and health activities of alarm, and to change current of the state of alarms.","title":"csw-alarm-cli"},{"location":"/apps/cswalarmcli.html#supported-commands","text":"init list acknowledge unacknowledge activate deactivate shelve unshelve reset severity health","title":"Supported Commands"},{"location":"/apps/cswalarmcli.html#admin-api","text":"The commands listed below will be used by administrators of the Alarm Service.","title":"Admin API"},{"location":"/apps/cswalarmcli.html#init","text":"Loads the alarm data in the alarm store\nfile path - is a required parameter. Can be on local disk or in the Config Service server (by default it will be picked from the Config Service.) --local - this specifies that the config file must be read from the local disk --reset - this is an optional parameter to clear previous data before loading the new one. By default, it is false.","title":"init"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli init /path/allAlarms.conf --local --reset\nClears the alarm store and loads alarm data to the alarm store from /path/allAlarms.conf, which is the path of a local file.","title":"Examples"},{"location":"/apps/cswalarmcli.html#list","text":"Gets the data of alarms from the alarm store. If none of the optional parameters are given, then by default, data of all alarms will be displayed.\n--subsystem - is an optional parameter to get the data of a specific subsystem’s alarms --component - is an optional parameter to get the data of a specific component’s alarms. --subsystem must be specified with this parameter. --name - is an optional parameter to get the data of a specific alarm. --subsystem and --component must be specified with this parameter. --metadata - is an optional parameter to get only the metadata of alarms --status - is an optional parameter to get only the status of the alarms","title":"list"},{"location":"/apps/cswalarmcli.html#examples","text":"1.\ncsw-alarm-cli list --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nDisplays metadata, status and severity of alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.\n2.\ncsw-alarm-cli list --subsystem nfiraos --metadata --status\nDisplays metadata and status of all alarms which belong to the nfiraos subsystem.","title":"Examples"},{"location":"/apps/cswalarmcli.html#operations-specific-to-an-alarm","text":"Commands given below are specific to an alarm. All of commands must be provided with --subsystem,--component and --name as parameters.\n--subsystem - is a parameter to specify the subsystem of alarm --component - is a parameter to specify the component of alarm --name - is a parameter to specify the name of the alarm","title":"Operations specific to an alarm"},{"location":"/apps/cswalarmcli.html#acknowledge","text":"Sets the acknowledgement status of the alarm to Acknowledged","title":"acknowledge"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli acknowledge --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nAcknowledge the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#unacknowledge","text":"Sets the acknowledgement status of the alarm to Unacknowledged","title":"unacknowledge"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli unacknowledge --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nUnacknowledge the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#activate","text":"Sets the activation status of the alarm to Active","title":"activate"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli activate --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nSets activation status to Active of the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#deactivate","text":"Sets the activation status of the alarm to Inactive","title":"deactivate"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli inactivate --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nSets activation status to Inactive of the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#shelve","text":"Sets the shelve status of the alarm to Shelved","title":"shelve"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli shelve --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nShelves the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#unshelve","text":"Sets the shelve status of the alarm to Unshelved","title":"unshelve"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli unshelve --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nUnshelves the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#reset","text":"Resets the alarm status. This will set the acknowledgement Status to Acknowledged and the latched severity to the current severity of the alarm.","title":"reset"},{"location":"/apps/cswalarmcli.html#examples","text":"csw-alarm-cli reset --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nResets the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#severity","text":"Severity command contains 3 sub-commands.","title":"severity"},{"location":"/apps/cswalarmcli.html#get","text":"Used to get the severity of the subsystem, component or alarm. If none of the optional parameters are given, then the severity of whole TMT system will be displayed.\nNote For a single alarm, current severity will be displayed. For a system, subsystem or component, aggregated severity will be displayed.\n--subsystem - is an optional parameter to get the severity of a specific subsystem’s alarms --component - is an optional parameter to get the severity of a specific component’s alarms. --subsystem must be specified with this parameter. --name - is an optional parameter to get the severity of a specific alarm. --subsystem and --component must be specified with this parameter.","title":"get"},{"location":"/apps/cswalarmcli.html#examples","text":"1.\ncsw-alarm-cli severity get --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nDisplays the severity of the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.\n2.\ncsw-alarm-cli severity get --subsystem nfiraos --component trombone\nDisplays the aggregated severity of the component with name trombone of subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#set","text":"Sets the given severity for the specified alarm\nseverity to which given alarm needs to be set --subsystem - is a parameter to specify the subsystem of alarm --component - is a parameter to specify the component of alarm --name - is a parameter to specify the name alarm --refresh - is an optional parameter to refresh severity after every 3 seconds","title":"set"},{"location":"/apps/cswalarmcli.html#examples","text":"1.\ncsw-alarm-cli severity set major --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nSets Major as the severity of the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.\n2.\ncsw-alarm-cli severity set major --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm --refresh\nRefresh Major as the severity of the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos every 3 seconds.","title":"Examples"},{"location":"/apps/cswalarmcli.html#subscribe","text":"Subscribes to the severity changes of the whole TMT system, subsystem, component or an alarm. If none of the optional parameters are given then the severity of the whole TMT system will be displayed.\nNote For a single alarm, the current severity will be displayed. For system, subsystem or component, the aggregated severity will be displayed.\n--subsystem - is an optional parameter to get the severity of a specific subsystem’s alarms --component - is an optional parameter to get the severity of a specific component’s alarms. --subsystem must be specified with this parameter. --name - is an optional parameter to get the severity of a specific alarm. --subsystem and --component must be specified with this parameter.","title":"subscribe"},{"location":"/apps/cswalarmcli.html#examples","text":"1.\ncsw-alarm-cli severity subscribe --subsystem nfiraos --component trombone\nSubscribes to the aggregated severity of a component with name trombone and subsystem nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#health","text":"Health command contains two sub-commands.","title":"health"},{"location":"/apps/cswalarmcli.html#get","text":"Gets the health of the whole TMT system, subsystem, component or alarm. If none of the optional parameters are given, then the health of the whole TMT system will be displayed.\nNote For a single alarm, the current health will be displayed. For the system, subsystem or component, the aggregated health will be displayed.\n--subsystem - is an optional parameter to get the health of a specific subsystem’s alarms --component - is an optional parameter to get the health of a specific component’s alarms. --subsystem must be specified with this parameter. --name - is an optional parameter to get the health of a specific alarm. --subsystem and --component must be specified with this parameter.","title":"get"},{"location":"/apps/cswalarmcli.html#examples","text":"1.\ncsw-alarm-cli health get --subsystem nfiraos --component trombone --name tromboneAxisLowLimitAlarm\nDisplays the health of the alarm with name tromboneAxisLowLimitAlarm of component trombone and subsystem nfiraos.\n2.\ncsw-alarm-cli health get --subsystem nfiraos\nDisplays the aggregated health of the subsystem with name nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#subscribe","text":"Subscribes to the health changes of the whole TMT system, subsystem, component or an alarm. If none of the optional parameters are given, then the health of the whole TMT system will be displayed.\nNote For a single alarm, current health will be displayed. For the system, subsystem or component, the aggregated health will be displayed.\n--subsystem - is an optional parameter to get the health of a specific subsystem’s alarms --component - is an optional parameter to get the health of a specific component’s alarms. --subsystem must be specified with this parameter. --name - is an optional parameter to get the health of a specific alarm. --subsystem and --component must be specified with this parameter.","title":"subscribe"},{"location":"/apps/cswalarmcli.html#examples","text":"1.\ncsw-alarm-cli health subscribe --subsystem nfiraos --component trombone\nSubscribes to the aggregated health of the subsystem with name nfiraos.","title":"Examples"},{"location":"/apps/cswalarmcli.html#about-this-application","text":"","title":"About this application"},{"location":"/apps/cswalarmcli.html#help","text":"Prints the help message.","title":"–help"},{"location":"/apps/cswalarmcli.html#version","text":"Prints the version of the application.\nNote All the above examples require that csw-location-server is running on local machine at localhost:7654. If csw-location-server is running on remote machine having Ip address 172.1.1.2, then you need to pass an additional --locationHost 172.1.1.2 command line argument. Example: csw-alarm-cli list --locationHost 172.1.1.2","title":"–version"},{"location":"/apps/hostconfig.html","text":"","title":"csw-host-config"},{"location":"/apps/hostconfig.html#csw-host-config","text":"This is just a helper to create a host configuration application. A component/subsystem can create their own host configuration application using this helper; to be more precise, every Github repository should have one host configuration application. The reason for having one app per repo is, when you run universal:publish task, it will generate a zip file which will be self contained (will have all the required dependencies) and can be deployed independently on any machine.\nThis application will start multiple containers on a given host machine and each container will have single/multiple components.","title":"csw-host-config"},{"location":"/apps/hostconfig.html#how-to-create","text":"Scala object HostConfigApp extends App {\n\n  HostConfig.start(\"Host-Config-App\", CSW, args)\n\n} Java public class JHostConfigApp {\n\n    public static void main(String args[]) {\n        JHostConfig.start(\"JHost-Config-App\", JSubsystem.CSW, args);\n    }\n\n}\nNote It is not necessary to have the name of the application as HostConfigApp/JHostConfigApp, the user can choose this name.","title":"How to create"},{"location":"/apps/hostconfig.html#command-line-parameter-options","text":"--local is an optional parameter. When supplied, get the host configuration file from local machine located at hostConfigPath, otherwise, fetch it from the Config Service <file> is a mandatory parameter. It specifies Host configuration file path -s, --container-script <script-path> specifies the path of the generated shell script for the container command app from task universal:packageBin (sbt-native-packager task) --help prints the help of the application. --version prints the version of the application.","title":"Command line parameter options"},{"location":"/apps/hostconfig.html#examples","text":"","title":"Examples"},{"location":"/apps/hostconfig.html#pre-requisites","text":"Run sbt project/universal:packageBin command. Here, the project contains HostConfigApp and ContainerCmdApp and it depends on the required components. Ex. Hcd’s, Assembly’s etc. Navigate to project/target/universal directory Unzip the file created with the project’s name Navigate to the bin directory from the unzipped contents\nThe sbt task: sbt project/universal:packageBin creates the following four scripts in the bin directory:\ntrombone-host-config-app : Responsible for starting multiple containers. It takes a hostconfig.conf file as an argument which contains the list of container specifications. trombone-container-cmd-app : Responsible for starting a single container or component in standalone mode. It takes a containerConfig.conf file as an argument which contains single container specifications. trombone-host-config-app.bat : For Windows machine. trombone-container-cmd-app.bat : For Windows machine.\nExamples:\n./trombone-host-config-app hostconfig.conf -s ./trombone-container-cmd-app\n Fetch hostconfig.conf from the Configuration Service which contains a multiple container configuration, then invoke the trombone-container-cmd-app script per the container configuration which spawns the container ./trombone-host-config-app --local hostconfig.conf -s ./trombone-container-cmd-app\n Fetch and parse hostconfig.conf from the current directory which contains a multiple container configuration, then invoke the trombone-container-cmd-app script per the container configuration which spawns the container\nNote In above examples, we are passing argument: -s ./trombone-container-cmd-app to ./trombone-host-config-app. here -s stands for script and following that is the script name, in our case its trombone-container-cmd-app. If you notice, trombone-container-cmd-app does not take a container configuration file. The hostconfig.conf file passed to trombone-host-config-app contains the location of the container configuration files. The Host Config App internally parses hostconfig.conf and passes the container configuration file location to trombone-container-cmd-app. Find more details of ContainerCmd application here.","title":"Pre-requisites"},{"location":"/apps/hostconfig.html#where-does-it-fit-in-overall-deployment-strategy-may-change-","text":"","title":"Where does it fit in overall deployment strategy (may change)"},{"location":"/apps/hostconfig.html#custom-host-configuration","text":"hostconfig.conf # This is a host configuration file which contains list of containers to be spawned by host configuration app\ncontainers: [\n  {\n    # mode can be one of Container or Standalone\n    mode: \"Container\"\n    # path of individual container configuration file\n    configFilePath: \"/Users/salonivithalani/Desktop/tw_tmt/csw/examples/src/main/resources/assemblyContainer.conf\"\n    # provide 'Remote' if file needs to fetched from config service else\n    # provide 'Local' to fetch file from local machine\n    configFileLocation: \"Local\"\n  },\n  {\n    mode: \"Standalone\"\n    configFilePath: \"/Users/salonivithalani/Desktop/tw_tmt/csw/examples/src/main/resources/hcdStandalone.conf\"\n    configFileLocation: \"Local\"\n  }\n]","title":"Custom Host Configuration"},{"location":"/apps/hostconfig.html#help","text":"Use the following command to get help on the options available with this app\n./bin/trombone-host-config-app --help","title":"Help"},{"location":"/apps/hostconfig.html#version","text":"Use the following command to get version information for this app\n./bin/trombone-host-config-app --version\nNote Before running host-config app, make sure that csw-location-server is running on the local machine at localhost:7654. The host config application internally executes the container-cmd application, which uses a local HTTP location client that expects a Location Server to be running locally.","title":"Version"},{"location":"/apps/hostconfig.html#systemd-configuration","text":"Using systemd, you can configure a host configuration application to spawn containers on a machine to be run automatically on system startup.\nFor detailed information on systemd configuration, please refer to readme.md","title":"Systemd configuration"},{"location":"/commons/deployment.html","text":"","title":"Deployment"},{"location":"/commons/deployment.html#deployment","text":"The environment variables needed to be set before starting CSW services are here","title":"Deployment"},{"location":"/deployment/env-vars.html","text":"","title":"Environment variables"},{"location":"/deployment/env-vars.html#environment-variables","text":"List of environment variables which needs to be set for using different Csw services.\nVariable Dev Prod Needs to set before Description CLUSTER_SEEDS Optional Mandatory starting csw services The Host and port of the seed nodes of cluster, Ex. CLUSTER_SEEDS=“127.0.0.1:3552, 127.0.0.2:3552”. INTERFACE_NAME Mandatory Mandatory starting csw services Network interface in which the Akka cluster is formed, Ex. INTERFACE_NAME=en0. MANAGEMENT_PORT Optional Optional starting location service Port on which the Akka provided cluster management service will start (if not provided service won’t start) TMT_LOG_HOME Optional Mandatory starting any Csw service Base path of the directory to hold log files from TMT apps. (Log files will be generated only if file appender is enabled) DB_READ_USERNAME & DB_READ_PASSWORD Optional Mandatory (for components using DB service) starting DB service Needed to create connection with the Database Service with read access.\nNote The environment variable names for database write access and database admin access are not mentioned here because their names will be specific to their corresponding databases.","title":"Environment variables"},{"location":"/deployment/env-vars.html#terminologies","text":"Dev - For starting services using ./csw-services.sh Prod - For starting services by ways other than ./csw-services.sh","title":"Terminologies"},{"location":"/commons/testing.html","text":"","title":"Testing"},{"location":"/commons/testing.html#testing","text":"","title":"Testing"},{"location":"/commons/testing.html#dependencies","text":"To use the CSW Testkit, you must add the following dependency in your project:\nsbt libraryDependencies += \"com.github.tmtsoftware.csw\" %% \"csw-testkit\" % \"2.0.0-RC2\"","title":"Dependencies"},{"location":"/commons/testing.html#introduction","text":"CSW comes with a dedicated csw-testkit module for supporting tests. This module includes following multiple individual testkits:\nLocationTestKit : starts and stops the Location Server ConfigTestKit : starts and stops the Config Server EventTestKit : starts and stops the Event Service (Note : This uses embedded-redis to start redis sentinel and master) AlarmTestKit : starts and stops the Alarm Service (Note : This uses embedded-redis to start redis sentinel and master) FrameworkTestKit : in most of the cases, you will end up using this testkit. FrameworkTestKit is created by composing all the above mentioned testkits. Hence it supports starting and stopping all provided CSW services.\nNote All of the testkits require the Location Server to be up and running. Hence, the first thing all testkits do is to start a Location Server. You do not need to start it explicitly.","title":"Introduction"},{"location":"/commons/testing.html#testkits","text":"When you really want granular level access to testkits, then only you would want to use LocationTestKit|ConfigTestKit|EventTestKit|AlarmTestKit|FrameworkTestKit directly. You can create instance of FrameworkTestKit as shown below:\nScala // create instance of framework testkit\nprivate val frameworkTestKit = FrameworkTestKit()\n\n// starts Config Server and Event Service\noverride protected def beforeAll(): Unit = frameworkTestKit.start(ConfigServer, EventServer)\n\n// stops all services started by this testkit\noverride protected def afterAll(): Unit = frameworkTestKit.shutdown() Java private static FrameworkTestKit frameworkTestKit = FrameworkTestKit.create();\n\n@BeforeClass\npublic static void beforeAll() {\n    frameworkTestKit.start(JCSWService.ConfigServer, JCSWService.EventServer);\n}\n\n@AfterClass\npublic static void afterAll() {\n    frameworkTestKit.shutdown();\n}\nNote Similarly, you can use other testkits. Please refer to API docs for more details.","title":"TestKits"},{"location":"/commons/testing.html#spawning-components","text":"FrameworkTestKit provides an easy way to spawn components in Container or Standalone mode. Use the spawnContainer method provided by FrameworkTestKit to start components in container mode andspawnStandalone method to start a component in standalone mode.\nThe example below shows how to spawn container or component in standalone mode using the Framework testkit.\nScala // starting container from container config using testkit\nframeworkTestKit.spawnContainer(ConfigFactory.load(\"SampleContainer.conf\"))\n\n// starting standalone component from config using testkit\n// val componentRef: ActorRef[ComponentMessage] =\n//   frameworkTestKit.spawnStandaloneComponent(ConfigFactory.load(\"SampleHcdStandalone.conf\"))\n Java // starting container from container config using testkit\nframeworkTestKit.spawnContainer(ConfigFactory.load(\"JSampleContainer.conf\"));\n\n// starting standalone component from config using testkit\n// ActorRef<ComponentMessage> componentRef =\n//      frameworkTestKit.spawnStandaloneComponent(ConfigFactory.load(\"SampleHcdStandalone.conf\"));\nFull source at GitHub\nScala Java","title":"Spawning components"},{"location":"/commons/testing.html#test-framework-integration","text":"","title":"Test framework integration"},{"location":"/commons/testing.html#scalatest","text":"If you are using ScalaTest, then you can extend csw.testkit.scaladsl.ScalaTestFrameworkTestKit to have the Framework testkit automatically start the provided services before running tests and shut them down when the tests are complete. This is done in beforeAll and afterAll from the BeforeAndAfterAll trait. If you override that method you should call super.beforeAll to start services and super.afterAll to shutdown the test kit.","title":"ScalaTest"},{"location":"/commons/testing.html#junit","text":"If you are using JUnit then you can use csw.testkit.javadsl.FrameworkTestKitJunitResource to have the framework test kit automatically start the provided services before running tests and shut them down when the tests are complete.","title":"JUnit"},{"location":"/commons/testing.html#supported-csw-services-by-frameworktestkit","text":"ScalaTestFrameworkTestKit and FrameworkTestKitJunitResource both support starting one or more of the following services.\nCSWService.LocationServer | JCSWService.LocationServer CSWService.ConfigServer | JCSWService.ConfigServer CSWService.EventServer | JCSWService.EventServer CSWService.AlarmServer | JCSWService.AlarmServer\nThe example below shows the usage of ScalaTestFrameworkTestKit and FrameworkTestKitJunitResource and how you can start the above mentioned services as per your need.\nScala import com.typesafe.config.ConfigFactory\nimport csw.testkit.scaladsl.CSWService.{AlarmServer, EventServer}\nimport csw.testkit.scaladsl.ScalaTestFrameworkTestKit\nimport org.scalatest.funsuite.AnyFunSuiteLike\n\nclass ScalaTestIntegrationExampleTest extends ScalaTestFrameworkTestKit(AlarmServer, EventServer) with AnyFunSuiteLike {\n\n  test(\"test spawning component in standalone mode\") {\n    spawnStandalone(ConfigFactory.load(\"SampleHcdStandalone.conf\"))\n\n    // .. assertions etc.\n\n  }\n\n} Java import com.typesafe.config.ConfigFactory;\nimport csw.testkit.javadsl.FrameworkTestKitJunitResource;\nimport csw.testkit.javadsl.JCSWService;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.scalatestplus.junit.JUnitSuite;\n\nimport java.util.Arrays;\n\npublic class JUnitIntegrationExampleTest extends JUnitSuite {\n\n    @ClassRule\n    public static final FrameworkTestKitJunitResource testKit =\n            new FrameworkTestKitJunitResource(Arrays.asList(JCSWService.AlarmServer, JCSWService.EventServer));\n\n    @Test\n    public void testSpawningComponentInStandaloneMode() {\n        testKit.spawnStandalone(ConfigFactory.load(\"JSampleHcdStandalone.conf\"));\n\n         // ... assertions etc.\n    }\n}\nNote You do not need to externally start any services like the Event Service, Config Service, Location Service etc. via csw-services.sh script. Testkits will start required services as a part of initialization. For the Event and Alarm service, it uses an instance of embedded-redis.","title":"Supported CSW Services by FrameworkTestKit"},{"location":"/commons/testing.html#unit-tests","text":"The goal of unit testing is to break your application into the smallest testable units and test them individually, isolating a specific piece of functionality and ensuring it is working correctly. It is always a good idea to write more unit test cases and relatively fewer component and integration tests. If you want to get an idea of how many tests you should have in different types of testing phases (Unit/Component/Integration), refer to this blog\nUnit testing simple Scala/Java classes or objects is straightforward. You can mock external dependencies using Mockito. Refer to the Mockito section for more details.\nThe following links provide guides for testing applications using different modules of Akka:\nAkka Untyped Actors Akka Typed Actors Akka Streams","title":"Unit Tests"},{"location":"/commons/testing.html#multi-jvm-tests","text":"Testing asynchronous distributed systems requires special tooling/framework support. Sbt has a plugin called sbt-multi-jvm which helps to test systems across multiple JVMs or machines. This is especially useful for integration testing where multiple systems communicate with each other.\nYou can find more details on multi-JVM tests here.\nYou can also refer to some examples in CSW for writing your own multi-JVM tests. For example: CommandServiceTest.scala\nIn case you want to run your multi-JVM tests across machines, refer to this multi-node testing guide here.","title":"Multi-JVM Tests"},{"location":"/commons/testing.html#mockito","text":"Mocks are used so that unit tests can be written independent of dependencies. CSW uses Mockito for writing unit tests. ScalaTest comes with the MockitoSugar trait which provides some basic syntax sugar for Mockito.\nFor example: ContainerBehaviorTest.scala\nNote The csw-prod pipeline is responsible for following tasks: build and run csw tests publish binaries to bintray publish paradox documentation publish apps and release notes to Github releases\ncurl -G '$REMOTE_JENKINS_URL/job/$JOB_NAME/buildWithParameters' \\\n    --data-urlencode token=$RELEASE_TOKEN \\\n    --data-urlencode CSW_VERSION=$CSW_VERSION","title":"Mockito"},{"location":"/commons/sbt-tasks.html","text":"","title":"sbt Tasks"},{"location":"/commons/sbt-tasks.html#sbt-tasks","text":"csw uses multiple plugins in the sbt ecosystem to help accomplish a variety of tasks.\nplugin task Description default in sbt test compile and run all tests including scala tests, java tests, multi-jvm/multi-node tests default in sbt test:test compile and run all tests excluding multi-jvm/multi-node tests default in sbt publishLocal install your libraries in the local Ivy repository so they can be added as dependencies to other projects sbt-multi-jvm multi-jvm:test provides support for running applications (objects with main methods) and ScalaTest tests in multiple JVMs at the same time. sbt-site makeSite generates project’s webpage in the target/site directory sbt-ghpages ghpagesPushSite publish project website to GitHub Pages sbt-github-release githubRelease creates Github releases with proper release notes and optional artifact uploading. Releases in Github are first-class objects with changelogs and binary assets that present a full project history beyond Git artifacts. They’re accessible from a repository’s homepage. sbt-bintray publish upload and release artifacts into bintray (command requires proper bintray credentials. Intended for TMT staff.) sbt-native-packager stage locally install your app in target/universal/stage/bin/ so you can run it locally without having the app packaged. sbt-native-packager universal:packageBin Generates a universal zip file","title":"sbt Tasks"},{"location":"/commons/manuals.html","text":"","title":"Manuals"},{"location":"/commons/manuals.html#manuals","text":"Complete user guide on how to use common software platform, Scaladoc and Javadoc API’s can be found below:","title":"Manuals"},{"location":"/commons/manuals.html#","text":"","title":"Javadoc"},{"location":"/commons/manuals.html#","text":"","title":"Scaladoc"},{"location":"/migration_guide/migration-guides.html","text":"","title":"Migration Guides"},{"location":"/migration_guide/migration-guides.html#migration-guides","text":"Migration Guide from 1.0.0 to 2.0.0","title":"Migration Guides"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/migration-guide-1.0.0-to-2.0.0.html","text":"","title":"Migration Guide from 1.0.0 to 2.0.0"},{"location":"/migration_guide/migration_guide_1.0.0_to_2.0.0/migration-guide-1.0.0-to-2.0.0.html#migration-guide-from-1-0-0-to-2-0-0","text":"This guide shows how to migrate from CSW Version 1.0.0 to CSW Version 2.0.0. From the release notes, the following are the important changes from 1.0.0:\nSimplified CommandResponseManager and removed auto-completion of commands Prefix has Subsystem in constructor Log statements have subsystem and prefix along with componentName AlarmKey and ComponentKey is constructed from prefix instead of string TcpLocation and HttpLocation has prefix along with AkkaLocation ComponentType is displayed to snake_case from lowercase Subsystem is displayed in uppercase instead of lowercase ArrayData and MatrixData does not require classtag for creation Admin routes for setting log level and getting log level are now available via gateway JSON contracts for location and command service added in paradox documentation\nDetails on how to work with the updated Prefix type are given here.\nDetails on how to work with the new Command Service changes are given here.","title":"Migration Guide from 1.0.0 to 2.0.0"},{"location":"/commons/contract.html","text":"","title":"CSW Service contract"},{"location":"/commons/contract.html#csw-service-contract","text":"Complete CSW service contract to access Http/Websocket endpoints can be found below:","title":"CSW Service contract"},{"location":"/commons/contract.html#","text":"","title":"Json Contract"},{"location":"/technical/technical.html","text":"","title":"Technical Design Documents"},{"location":"/technical/technical.html#technical-design-documents","text":"Framework Command Params Location Service Configuration Service Logging Service Event Service Alarm Service Time Service Database Service Authentication and Authorization Service","title":"Technical Design Documents"},{"location":"/technical/framework/framework.html","text":"","title":"Framework"},{"location":"/technical/framework/framework.html#framework","text":"","title":"Framework"},{"location":"/technical/framework/framework.html#introduction","text":"The common software framework is a library that provides set of APIs used for:\ncreation of components(Assemblies, HCDs) discovering other components receiving messages from external world sending messages/commands to other components receiving responses from components deploying component in container or standalone mode\nThe CSW framework is implemented using Akka typed actors.\nIMPORTANT!!! Actors provide a single control of execution by processing messages received one by one, but it is still necessary to be careful with actor state. If a Future is spawned inside an actor then on completion of that Future, it is a common mistake to mutate actor state. It can cause critical problems of state corruption because some other message might be in the process of execution and accessing the actor state. The ideal way to handle Futures inside actors is by sending message to self on Future completion. This will make sure the mutation of state happens in order of one by one via messages. Example code can be seen here.","title":"Introduction"},{"location":"/technical/framework/framework.html#creation-of-a-component","text":"A component consists of couple of actors and classes created by framework on behalf of the component and some actors/classes that are expected to be created by component writers using the CSW framework.","title":"Creation of A Component"},{"location":"/technical/framework/framework.html#framework-actors-classes","text":"During component creation the CSW framework creates a Supervisor actor as the first thing when creating any component. Each component has its own Supervisor. That Supervisor then goes on to create the Top Level Actor using the component’s unique Handlers, Pub-Sub Manager actor and Command Response Manager actor as part of the TMT framework supporting a component.\nNote The actors shown in blue are created by CSW framework and actors/classes shown in green are expected to be written by the component developer. The Handlers shown above is implemented by extending ComponentHandlers/ JComponentHandlers framework class. So, the TLA decides when to call a specific handler method or hooks and implementation of ComponentHandlers/JComponentHandlers decides what to do when it is called, for e.g. TLA decides when to call intialize handler and handler provides implementation of how to initialize a component, may be by putting the hardware in default position, etc. From the framework’s viewpoint, the TLA is created with an instance of ComponentBehavior and the Handlers created by the developer. The ComponentBehavior actor has the framework behavior such as lifecycle. It manages incoming messages and calls the Handlers when appropriate.\nTo know more about the responsibility of Supervisor and Top Level Actor, please refer to this section.\nThe interaction between supervisor and Top Level Actor when creating the component is shown below:\nThe code base that implements the creation of Top Level Actor and watching it from its Supervisor can be found here. The code that implements the startup lifecycle and the calling of the intialize handler of Top Level Actor can be found here. An explanation of the Idle state can be found here.\nNote If there is any exception thrown while executing initialize handler then the exception is bubbled up to Supervisor, and it restarts Top Level Actor which in turn calls initialize handler again hoping the error fixes on restart. For this, Supervisor uses a restart strategy with maximum of 3 restarts possible that must be finished within 5 seconds. To know more about the Akka supervision failure strategy please refer to the Akka Fault Tolerance document. The Supervisor code base wiring restart strategy can be found here.\nOnce the handler is spawned it receives an ActorContext and CswContext in its constructor. The ActorContext is used to get the ActorSystem of the component and maybe spawn other actors i.e worker actors for maintaining state. The CswContext can be used to get the handle of all the services provided by CSW. To know more about these services, please refer to this section.","title":"Framework Actors/Classes"},{"location":"/technical/framework/framework.html#component-info-file-for-component-startup","text":"Every component needs to provide a startup config file called the Component Info File that contains component details such as name, type, handler class name, tracking details, etc. The contents of this file is used by Supervisor to create and setup the component as well as some customization. To know more about what is it and how to write the Component Info File, please refer to this section and this sample file.\nThe name of the Component Info File needs to be passed to Container/Standalone app at the time of startup. The file is either fetched from Configuration Service or taken from local path on the machine to parse to a ComponentInfo object. The ComponentInfo object is then passed to Handlers in CswContext.\nA component may be created in standalone or container mode. In standlone mode, the component is created in its own JVM process. In container mode, multiple components may be started together within the same JVM process. When an HCD or Assembly is created, depending on the mode, either the ContainerBehaviorFactory or the SupervisorBehaviorFactory class is used to create the initial TLA behavior.\nIn container mode, a Supervisor actor is created for each component and a single container actor accepts control messages for the container (subtypes of ContainerActorMessage). These container messages can be used to manage the lifecycle of the components in the container.","title":"Component Info File for Component Startup"},{"location":"/technical/framework/framework.html#actorsystem-for-the-component","text":"While creating a component a new ActorSystem is spawned, which means if there are more than one components running in single JVM process there will be more than one ActorSystem created in the single JVM. Having different ActorSystems in an application is not recommended by akka but it is still kept multiple per JVM so that any delay in executing in one component does not affect the execution of other components running in the same JVM. The code base for creating an ActorSystem for each component is written in SupervisorInfoFactory.","title":"ActorSystem for the Component"},{"location":"/technical/framework/framework.html#discovering-other-components","text":"Sometimes one component needs to discover other components. For discovering other components, there are two ways:\nprovide tracking information in the Component Info File as explained here. Whenever there is location update of tracked components onLocationTrackingEvent handler is called in the TLA. track using trackConnection method that is available in the TLA. This allows tracking to start at runtime based on activities of the component.","title":"Discovering Other Components"},{"location":"/technical/framework/framework.html#receiving-messages-from-outside-the-component","text":"Messages sent to the component are first received and handled by Supervisor. It decides which messages should be passed to downstream actors (i.e. Top Level Actor, Command Response Manager actor or Pub-Sub Manager actor).","title":"Receiving Messages from Outside the Component"},{"location":"/technical/framework/framework.html#restart","text":"An external administrative message to the Supervisor can cause the component to restart. An explanation of the Restart state and subsequent actions can be found here. The code base that implements restart can be found here.","title":"Restart"},{"location":"/technical/framework/framework.html#shutdown","text":"An external administrative command to the Supervisor can cause the component to shutdown and exit. The explanation about Shutdown state and subsequent actions can be found here. The code base that implements shutdown can be found here.","title":"Shutdown"},{"location":"/technical/framework/framework.html#changing-log-level","text":"An external messages to change the component’s log level (via SetComponentLogLevel) or get log metadata for a component (via GetComponentLogMetadata) gets handled by Supervisor. The code base implements this message can be found here.","title":"Changing log level"},{"location":"/technical/framework/framework.html#lock","text":"An external client component can restrict messages and allow only messages from itself using the lock command. An explanation of the Lock state can be found here. The code base that implements Lock can be found here.","title":"Lock"},{"location":"/technical/framework/framework.html#sending-commands-from-the-component","text":"Please refer to the section for sending commands to other component","title":"Sending Commands from the Component"},{"location":"/technical/framework/framework.html#receiving-responses-from-components","text":"Please refer to the section for receiving responses from other components","title":"Receiving Responses from Components"},{"location":"/technical/framework/framework.html#deploying-component-in-container-or-standalone-mode","text":"Component(s) can start within a container or a single component can start as a standalone. The code implementing deployment for Container is here and for Standalone here.\nThe name of the Component Information File can be passed as a command line argument to an application using the ContainerCmd class to deploy the component.\nIn a production environment, it is planned that components will be started at boot time using a HostConfig based application.\nSince Akka Typed is used throughout the TMT framework, there are seperate messages understood by Container, Supervisor, Top level Actor and other actors of the CSW framework. The architecture/relations of and between messages can be found here.","title":"Deploying Component in Container or Standalone Mode"},{"location":"/technical/command/command.html","text":"","title":"Command"},{"location":"/technical/command/command.html#command","text":"Commands can be sent to other component and responses can be received in return. To understand the underlying framework of the components and its deployment, please refer to the framework technical doc.","title":"Command"},{"location":"/technical/command/command.html#sending-commands-from-the-component","text":"The types of commands that can be sent by a component are discussed here. In order to send commands to other components, a CommandService helper is needed. CommandService helper is used to send commands to a component in the form of methods instead of sending messages directly to a component’s Supervisor actor. The creation of a CommandService instance can be found here.\nThe operations allowed through CommandService helper are as follows:\nvalidate submit submitAndWait submitAllAndWait oneway onewayAndMatch query queryFinal subscribeCurrentState","title":"Sending Commands from the Component"},{"location":"/technical/command/command.html#receiving-responses-from-components","text":"","title":"Receiving Responses from Components"},{"location":"/technical/command/command.html#submit","text":"To understand the flow of the Submit command, please refer to this section.","title":"Submit"},{"location":"/technical/command/command.html#oneway","text":"To understand the flow of Oneway, please refer to this section.","title":"Oneway"},{"location":"/technical/command/command.html#validate","text":"To understand the flow of the Validate command, please refer to this section and the code base for the implementation can be found here.","title":"Validate"},{"location":"/technical/command/command.html#command-response-manager","text":"Each component contains a Command Response Manager (CRM) with the sole purpose of helping to manage responses for long-running comamnds. If an Assembly sends a command to an HCD that is long-running, the HCD returns the Started response. The framework in the HCD notices the Started response and makes an entry in its CRM that a sender actor is may be expecting a final response for the command with the associated runId. It also tracks the most recent SubmitResponse for the command, which will always be Started when entered into the CRM.\nIf the Assembly (or any component) issues a query, the message is passed by the HCD’s Supervisor to the CRM, which returns the most recent response associated with the runId. If the Assembly (or any component) issues a queryFinal the HCD’s Supervisor forwards the request to the CRM, which makes an entry to remember that there is an Actor that needs to be updated with the final response for the runId. If the current response is already a final response, the Actor is updated immediately and no entry is made in the CRM.\nWhen the actions in the HCD completes, the HCD uses the updateCommand method of the CRM. When this happens, the CRM updates the most recent response for the runId and then checks it’s table to determine if there are any Actors waiting for the runId’s final response. If there are waiting Actors, each of them is sent the final response, and each is then removed from the CRM tables. If there are no waiting Actors, only the current response for the runId is updated ready for the possibility that there will be a future query or queryFinal.\nThe CRM remembers roughly 10 recent commands, so query and queryFinal may be used successfully with commands that have completed for some amount of time. If there is no entry in the CRM for a provided runId, the CRM returns an Error response indicating it does not know the runId.\nThe CRM also provides a utility method called queryFinalAll. This is just a wrapper around the Future.sequence call that allows components sending sub-commands to wait for all the sub-commands to complete.\nThe Assembly worker can communicate with CommandResponseManagerActor using CommandResponseManager coming via CswContext.","title":"Command Response Manager"},{"location":"/technical/command/command.html#current-state-pub-sub-functionality","text":"The framework provides a way, based only on Akka, for one component to subscribe to CurrentState events provided in another component. This can be used by an HCD to keep an Assembly up to date on its internal state asynchronously outside of commands. This can also be coupled with the use of Oneway commands that do not provide completion responses to the sender as is done for Submit. The provider of CurrentState can use CurrentStatePublisher to publish its state from CswContext and the sender component can receive state using subscribeCurrentState from CommandService.\nThe Current State Pub/Sub is implemented in PubSubBehavior","title":"Current State Pub/Sub Functionality"},{"location":"/technical/params/params.html","text":"","title":"Params"},{"location":"/technical/params/params.html#params","text":"The csw-params project contains models like Events, Commands, ParameterSet etc which are cross-compiled to Scala and Scala.js. CSW Framework and CSW Services like Command Service, Event Service, Alarm Service etc rely on this project for these models. Since these models are needed across platforms like C/Cpp, Scala/Java, Python etc this project also contains serialization support for these models. Cbor is used as default for Serialization and Deserialization of these models. Comparisons were done with other popular formats like Protocol Buffers, Message Pack before finalizing on Cbor. Cbor was finalized as it offers better performance and has much lesser overhead of maintenance as compared to protocol buffers.","title":"Params"},{"location":"/technical/params/params.html#cbor","text":"We use scala Borer library for cbor. CborSupport object in csw-params contains codecs (encoders and decoders) for all the models defined in this project that need to be serialized.\nCborSupport needs to be imported wherever you need to encode/decode a model. This makes all the implicit codecs defined in CborSupport available in the current scope. When any new models are added, their corresponding codecs also need to be added in the CborSupport.\nCbor offers Array-Based Encoding and Map-Based Encoding. Map-based encoding carries keys/labels along with values unlike array-based encoding which encodes only values. We use the Map-Based encoding since it makes it easy to debug the problems if any in the encoded cbor object. Though the size of map based encoded cbor objects is slightly more than that of array-based, it does not affect the performance as much.\nAdtCbor is a helper trait to encode and decode ADTs (Abstract Data Types) with Cbor. This is needed due to the way ADTs are encoded in Borer library. Having this abstraction ensures all the types in an ADT (Base types plus concrete types) are encoded in a similar manner. In Borer, encoding of base-types carries the type information which differs than the way concrete-types are encoded (without type information). The problem is described in detail in the issue here. Going via AdtCbor ensures the Adt types(base as well concrete) are all encoded as Base-type containing type information. So if you are writing new types which are ADTs, make sure you add your own helper next to EventCbor or CommandCbor to ensure proper encoding and decoding of ADTs.\nUsage of Cbor for Event encoding and decoding is shown below:\nScala //Key\nval raDecKey = RaDecKey.make(\"raDecKey\")\n\n//values\nval raDec1 = RaDec(10.20, 40.20)\nval raDec2 = RaDec(11.20, 50.20)\n\n//parameters\nval param = raDecKey.set(raDec1, raDec2).withUnits(arcmin)\n\nval prefix = Prefix(\"tcs.pk\")\nval name   = EventName(\"targetCoords\")\n//events\nval observeEvent: ObserveEvent = ObserveEvent(prefix, name).add(param)\nval systemEvent1: SystemEvent  = SystemEvent(prefix, name).add(param)\nval systemEvent2: SystemEvent  = SystemEvent(prefix, name).add(param)\n\n//convert events to cbor bytestring\nval byteArray2 = EventCbor.encode(observeEvent)\nval byteArray3 = EventCbor.encode(systemEvent1)\nval byteArray4 = EventCbor.encode(systemEvent2)\n\n//convert cbor bytestring to events\nval pbObserveEvent: ObserveEvent = EventCbor.decode(byteArray2)\nval pbSystemEvent1: SystemEvent  = EventCbor.decode(byteArray3)\nval pbSystemEvent2: SystemEvent  = EventCbor.decode(byteArray4) Java //prefixes\nPrefix prefix1 = Prefix.apply(JSubsystem.TCS, \"pk\");\nEventName name1 = new EventName(\"targetCoords\");\nPrefix prefix2 = Prefix.apply(JSubsystem.TCS, \"cm\");\nEventName name2 = new EventName(\"guiderCoords\");\n\n//Key\nKey<RaDec> raDecKey = JKeyType.RaDecKey().make(\"raDecKey\", JUnits.NoUnits);\n\n//values\nRaDec raDec1 = new RaDec(10.20, 40.20);\nRaDec raDec2 = new RaDec(11.20, 50.20);\n\n//parameters\nParameter<RaDec> param = raDecKey.set(raDec1, raDec2).withUnits(JUnits.arcmin);\n\n//events\nObserveEvent observeEvent = new ObserveEvent(prefix1, name1).add(param);\nSystemEvent systemEvent1 = new SystemEvent(prefix1, name1).add(param);\nSystemEvent systemEvent2 = new SystemEvent(prefix2, name2).add(param);\n\n//convert events to cbor bytestring\nbyte[] byteArray2 = EventCbor$.MODULE$.encode(observeEvent);\nbyte[] byteArray3 = EventCbor$.MODULE$.encode(systemEvent1);\nbyte[] byteArray4 = EventCbor$.MODULE$.encode(systemEvent2);\n\n//convert cbor bytestring to events\nObserveEvent pbObserveEvent = EventCbor$.MODULE$.decode(byteArray2);\nSystemEvent pbSystemEvent1 = EventCbor$.MODULE$.decode(byteArray3);\nSystemEvent pbSystemEvent2 = EventCbor$.MODULE$.decode(byteArray4);","title":"Cbor"},{"location":"/technical/params/params.html#tooling","text":"A number of tools are available around Cbor for diagnostic purposes. This link contains list of all the tools available. A couple of useful utilities are available here. Follow the readme for details regarding installation and usage.\nUtilities like cbor2json or cbor2diag could be very helpful while debugging. Often in development/debugging we wish to look at the actual encoded object, but since cbor is a binary format, looking at it does not help in debugging. That’s when these tools come in handy.\nFor example:\nThis will return the json representation of the given Cbor object.\ncbor2json.rb csw-params/shared/src/test/cbor/data/event.cbor\nA lot of other similar utilities are present in the same package which you can play with.\nCbor is schema-less. But you could define a schema (.cddl) and validate it against your cbor object. This is very similar to the json format. Json does not have a predefined schema, but you can define your schema and validate an object against it.\nCDDL tool allows you to give a schema and a cbor/json object, and it will validate and tell you if the object adheres to the given schema.\ncddl csw-params/shared/src/test/cbor/schema/event_command.cddl validate csw-params/shared/src/test/cbor/data/event.cbor\nCddl schema for events and commands has been defined here.","title":"Tooling"},{"location":"/technical/location/location.html","text":"","title":"Location Service"},{"location":"/technical/location/location.html#location-service","text":"","title":"Location Service"},{"location":"/technical/location/location.html#introduction","text":"The Location Service is used to register and find CSW services in the TMT network. A service can be an Akka actor (including CSW components, such as assemblies and HCDs), an HTTP server or a server running on some TCP host and port.\nFor information on using the Location Service, see these sections:\nLocation Service Location Server Location Agent","title":"Introduction"},{"location":"/technical/location/location.html#location-service-implementation","text":"Core implementation of location service uses\nAkka Cluster Conflict Free Replicated Data Types (CRDTs) Akka Http.\nYou can find more details on how CSW is using this in csw-location-server.\nNote See here for some background on how the choice was made to use an Akka cluster with CRDTs.\nThe implementation of the Location Service is split into following four sub-modules:\ncsw-location-api - common API implemented by the csw-location-server and csw-location-client csw-location-server - think of it as a agent which runs on every machine and exposes HTTP routes which underneath uses akka cluster and distributed data. csw-location-client - lightweight HTTP client for location service csw-location-agent - an application used to register non-csw services (Http/Tcp).","title":"Location Service Implementation"},{"location":"/technical/configuration/configuration.html","text":"","title":"Configuration Service"},{"location":"/technical/configuration/configuration.html#configuration-service","text":"","title":"Configuration Service"},{"location":"/technical/configuration/configuration.html#introduction","text":"The Configuration Service maintains all the configuration files in version control system (Subversion repository) which tracks the history of file and directory changes over time. We access Subversion programmatically using SVNKit which is a pure Java Subversion client library.\nFor information on using the Configuration Service, see these sections:\nConfiguration Service Config CLI","title":"Introduction"},{"location":"/technical/configuration/configuration.html#architecture","text":"Configuration service is divided into following modules:\nServer It exposes HTTP rest end points to store/retrieve configuration files from central repository. Internally server maintains two types of repositories: SVN Repository: store for normal/small files Annex File Repository: store for large files Client Config client is a convenient lightweight wrapper for accessing the CSW Configuration HTTP Server. It is build using Akka Http’s request level client API. CLI It is a command line application which internally uses Config Client and hides HTTP interaction with Config server. This CLI also provides additional login/logout commands to authenticate and authorize user to perform administrator actions such as create/update/delete etc. operations.","title":"Architecture"},{"location":"/technical/configuration/configuration.html#configuration-server","text":"Configuration server is made up of following three main components:\nHTTP Routes/Layer SVN Service/Repo Annex Service","title":"Configuration Server"},{"location":"/technical/configuration/configuration.html#http-routes-layer","text":"Configuration server exposes set of HTTP routes to write or read configuration files to/from subversion(svn) or local file system(annex).","title":"HTTP Routes/Layer"},{"location":"/technical/configuration/configuration.html#write-routes","text":"create update delete setActive resetActive\nThese are admin protected which uses utilities provided by csw-aas-http adapter for protection. csw-aas-http adapter internally uses Keycloak for authentication and authorization.\nAdmin protected routes expects Access Token to be passed in HTTP requests header field called Authorization: Bearer ****. This token then used to retrieve RPT(Requesting Party Token) from keycloak. RPT contains user’s permissions. Once RPT is received, then it is validated to see if user has valid admin client role. If user has valid admin role, then only request gets served otherwise authentication/authorization failed response is sent back to user.","title":"Write Routes"},{"location":"/technical/configuration/configuration.html#read-routes","text":"get get active get metadata list history\nThese routes are read only and not protected. Anyone with valid HTTP request can access these routes.","title":"Read Routes"},{"location":"/technical/configuration/configuration.html#svn-service-repo","text":"Primary store for small or normal configuration files is Subversion. SVNKit is used to perform all the read’s and write’s operations programmatically.\nThere is a notion of active files in configuration service. Whenever user creates new file using create API provided by service, there are following two files gets created in svn behind the scenes.\nNormal file which user has requested with provided content {name}.$active file which keeps track of current active version of file\nOnce new file is created, user can keep updating it as and when required but {name}.$active file will still point to initial version of file. When user is done with changes and wants respective components to pick these new changes, he can mark specific version as active version by using setActiveVersion API. These will update {name}.$active file to point to provided version.","title":"SVN Service/Repo"},{"location":"/technical/configuration/configuration.html#annex-service","text":"Large/Binary files are stored in annex repo. There are two ways file gets archived in annex store:\nFiles of size more than 10 MB If user explicitly specifies annex=true while creating new file\nWhenever new annex file has to be created, there are following three files gets created in svn and annex store behind the scenes:\nactual large file which user has requested with provided content in annex store while creating large file, it’s sha gets calculated based on file content and new file {name}.$sha1 with this sha gets created in svn repo {name}.$active file which keeps track of current active version of file","title":"Annex Service"},{"location":"/technical/configuration/configuration.html#internals","text":"Important classes involved in configuration service are:\nConfigServiceRoute: contains all the read and write http routes SvnConfigService: responsible for all the interactions with SvnRepo and AnnexFileService to perform CRUD operations SvnRepo: responsible for all the CRUD operations on svn repository using SVNKit AnnexFileService: responsible for calculating SHA1 based on file content and interacting with AnnexFileRepo to perform CRUD operations AnnexFileRepo: represents file based repository for large/binary/annex files\nBelow sequence diagram indicates how these classes are involved in creation of annex file:","title":"Internals"},{"location":"/technical/logging/logging.html","text":"","title":"Logging Service"},{"location":"/technical/logging/logging.html#logging-service","text":"The Logging Service library provides an advanced logging facility for CSW components and services. The Logger API for scala and java allows logging at different levels viz., trace, debug, info, warn and error.\nThe library allows separate log levels for the logging API, Akka logging, and Slf4J. Also, it provides ability to set different log levels for different components. To understand how to use the API, refer to this doc.","title":"Logging Service"},{"location":"/technical/logging/logging.html#implementation-details","text":"Following is a diagram showing the interaction of various classes, components and services in the Logging Service ecosystem.\nFor each container/standalone component, a new csw.logging.client.internal.LoggingSystem is created and started with the help of LoggingSystemFactory.\nLoggingSystem on creation performs various things:\nSpawning csw.logging.client.internal.LogActor. LogActor is the central component of Logging Service. All the messages of changing log level at runtime and logging at different levels go through the LogActor. Even external loggers csw.logging.client.compat.AkkaLogger and csw.logging.client.compat.Slf4jAppender send messages to LogActor in order to log messages to the configured appenders (StdOutAppender and FileAppender). Creating csw.logging.client.internal.LoggerImpl which is a concrete implementation of Logger API. In order to do logging of messages sent by services/components, LoggerImpl also sends messages to LogActor. Load the default logging configurations from logging.conf and initialize csw.logging.client.internal.LoggingState which is a singleton object keeping track of the current logging state (levels of logging API, akka, slf4j and various components) of the application. LoggingSystem changes this LoggingState to change logging level of services/components. And LoggerImpl picks up from this state inorder to decide if the message needs to be logged or not depending on the current level set.","title":"Implementation Details"},{"location":"/technical/logging/logging.html#changing-the-log-level-of-components","text":"The above figure also demonstrates the flow of a request for changing the log level of a component. Such a request would be initiated by some admin dashboard which will access the Admin Http Server. On receiving the request, Admin Http Server will resolve the component using Location Service and send that component a SetComponentLogLevel message. This message is received by the component’s Supervisor (SupervisorBehavior) which then changes the global LoggingState. On any following logs of that component, the new LoggingState will be used in LoggerImpl to decide whether it needs to be logged or not.","title":"Changing the Log Level of Components"},{"location":"/technical/logging/logging.html#external-loggers","text":"External loggers like AkkaLogger and Slf4jAppender are wired up such that any logs from those libraries (Akka and others that implement Slf4j like log4j, logback, tinylog etc) will go through our facade of AkkaLogger and Slf4jAppender respectively. For AkkaLogger, there is a configuration akka.loggers specified in logging.conf which does this wiring and for Slf4jAppender logback.xml is configured. AkkaLogger and Slf4jAppender then forward these logs to the LogActor.","title":"External Loggers"},{"location":"/technical/logging/logging.html#hierarchy-of-loggerfactories","text":"All the services for e.g. Event Service, Alarm Service etc, create their own instances of LoggerFactory which inherit from LoggerFactory provided by csw-logging-client.\nThe below diagram shows the hierarchy of the LoggerFactory and where do the Service Loggers stand in that tree.","title":"Hierarchy of LoggerFactories"},{"location":"/technical/event/event.html","text":"","title":"Event Service"},{"location":"/technical/event/event.html#event-service","text":"","title":"Event Service"},{"location":"/technical/event/event.html#introduction","text":"Event Service is a PubSub service which allows publishing of and subscription to CSW Events based on an Event Key, which is a combination of a component’s Prefix and Event Name. Event Service is optimized for the high performance requirements of events as demands with varying rates, for ex. 100 Hz, 50 Hz etc., but can also be used with events that are published infrequently or when values change. The end-to-end latency of events assured by Event Service is 5 milliseconds in typical observatory conditions. It also ensures ordered delivery of events with no event loss within performance specification.\nIn the TMT control system, events are used in a variety of circumstances. Events may be created as the output of a calculation by one component for the input to a calculation in one or more other components. These demand events are usually published at a specific rate and are usually calculated for a specific use or device. Other events indicate the occurence of a specific event in the control system that is of interest to other systems. For instance, the opening/closing of a shutter in a detector system is marked with an event.","title":"Introduction"},{"location":"/technical/event/event.html#technology-choices","text":"There were two good candidates for the backend of Event Service - Apache Kafka and Redis. The Event Service API is implemented with both these backends and performance testing was done to select one particular backend which would best satisfy the service latency requirements. Results of the performance tests can be found here. Redis was the best choice for the backend as it turned out to be better at providing low latency unlike Kafka which is more suited for high throughput systems. Hence you can see 2 implementations of the API in the Event Service client. The code is structured in a way that it is easy to switch the implementations. The Kafka implementation is being retained for possible future use.","title":"Technology Choices"},{"location":"/technical/event/event.html#implementation-details","text":"EventServiceFactory in csw-event-client is the entry point in the Event Service. It provides low-level APIs to make a new EventService from Scala and IEventService from Java. It takes an EventStore which could be either RedisStore or KafkaStore.\nThese APIs are not needed by component developers. The CSW framework provides the correct service implementation.\nDepending on which store is provided to the EventServiceFactory, an implementation of EventService is returned which could be either KafkaEventService or RedisEventService. The default store is set to RedisStore. Hence the service returned by default is RedisEventService.\nBelow is a sequence diagram showing the important entities involved in the implementation of Event Service. It captures the flow of the code from creation of EventService via EventServiceFactory through the publishing/subscription of events to the underlying Redis implementation product.\nEventServiceFactory provides overloads of the make method that allow creation of EventService using host-port as well as using LocationService to resolve the implementation product. The EventService provides APIs to create an EventPublisher and an EventSubscriber, which allow users to publish and subscribe to events. It provides both APIs:\nto make new instances of publisher/subscriber to use a default instance of publisher/subscriber\nWhen to use which API is documented in this section of the Event Service programming documentation.\nEvent Service uses Redis’ PubSub for publishing and subscribing to events. And to support the feature of getting the latest event on a subscription, the set operation of Redis DB is used.","title":"Implementation Details"},{"location":"/technical/event/event.html#romaine","text":"At a lower level, we have created a library called “Romaine” to communicate more efficiently with Redis as is shown in the following figure.\nRomaine is a Scala library built over the Java Redis client library called Lettuce. Romaine provides additional rich APIs over the existing functionality offered by Lettuce including these additional APIs useful to CSW:\nAsync API: Provides asynchronous API (romaine.async.RedisAsyncApi) for various redis commands like get, set, publish etc. Reactive API: Provides API for Subscription and Pattern-Subscription (romaine.reactive.RedisSubscriptionApi). On subscription, it returns an Akka Stream of Events which on execution materializes to RedisSubscription instance which gives handle to unsubscribe to events. Keyspace API: Provides APIs to watch Keyspace Notifications (romaine.keyspace.RedisKeySpaceApi). This is a rich API built on Akka Streams which provides not just the change events that happen on keys (for eg: Update, Removal etc.) but also the old and new values corresponding to those keys.\nEvent Service uses Async API for publishing and setting the latest event, and Reactive API for subscribing to events and patterns. Keyspace API is used in Alarm Service.","title":"Romaine"},{"location":"/technical/event/event.html#event-publishing","text":"Publishing of events in the Event Service client involves two things in Redis:\nPublish the event in Redis resulting in distribution of the event to all subscribers Setting the value of event against the event key in Redis. This provides persistence of the most recent event and allows a client to get the most recently published event. This also supports the need to provide a client with the most recently published event whenever a new subscription happens.\nIn case the Event Service is not available (i.e., Redis is not available or crashes), the Publish APIs will fail with an exception EventServerNotAvailable. If due to any other reasons, the publishing of events fail, the publish APIs would throw a PublishFailure","title":"Event Publishing"},{"location":"/technical/event/event.html#event-subscription","text":"A subscription to one or more event keys returns an Akka Stream of events. Subscriptions to concrete event keys as well as to glob-style patterns is supported. With pattern or glob-style subscriptions, the subscriber receives all the events with event-keys that match the provided pattern. In both cases, the subscriber gets a handle to an instance of EventSubscription that can be used to unsubscribe.\nThe subscription API supports subscribing with different modes to control the rate of events you receive. Two modes are provided - RateAdapterMode and RateLimiter mode. Details of when to use which mode could be found here.\nSubscriber API also provides a get API which could be used to fetch the latest events for the specified event keys.\nIn case, when the underlying event implementation is not available, the Subscribe APIs would fail with an exception EventServiceNotAvailable","title":"Event Subscription"},{"location":"/technical/event/event.html#architecture","text":"In order to allow components to discover Event Service, it is necessary to register it with the Location Service including the underlying product, which here refers to the Redis instance (particularly Redis Sentinel).\nFor high availability of Event Service, we use the Redis Sentinel along with master and slave instances of Redis. Master and slave are configured in “replication” mode.\nThe Sentinel’s responsibility is to promote the slave as master when master goes down. It is important to note that when master goes down, the “location” of Event Service remains the same because the location of Event Service is the location of Sentinel and not of master or slave. The master and slave Redis instances are dedicated for events, however Sentinel is shared across CSW Redis-based services. As Sentinel caters to more than one master, we need to specify which master to connect to for Event Service. That is configured in reference.conf in csw-event-client project.\ncsw-event {\n  redis {\n    masterId = \"eventServer\"\n  }\n}\nOnce the location is registered, components and the event CLI can resolve the Event Service location and start publishing/subscribing.","title":"Architecture"},{"location":"/technical/alarm/alarm.html","text":"","title":"Alarm Service"},{"location":"/technical/alarm/alarm.html#alarm-service","text":"","title":"Alarm Service"},{"location":"/technical/alarm/alarm.html#introduction","text":"Alarm Service in TMT software is used by components to raise alarms. Alarms are used exclusively to notify the operator of conditions that require operator intervention. Each alarms includes a severity that must be one of the supported severities such as Warning, Critical, etc. The Alarm Service also provides mechanisms to monitor the health of all components and subsystems in TMT.","title":"Introduction"},{"location":"/technical/alarm/alarm.html#technology","text":"Alarm Service uses Redis for persistence. Redis provides Keyspace Notifications which allows clients to subscribe to Pub/Sub channels in order to receive events affecting the Redis data set in some way.\nWe have created a layer i.e. “Romaine” which converts redis events into an akka stream. Romaine internally uses a java redis library called Lettuce.","title":"Technology"},{"location":"/technical/alarm/alarm.html#severities","text":"An alarm can have one of the following severities at a given time\nSeverity Name Severity Level Okay 0 Warning 1 Major 2 Indeterminate 3 Disconnected 4 Critical 5\nSeverity can be for a single alarm or aggregated for a component or subsystem.","title":"Severities"},{"location":"/technical/alarm/alarm.html#health","text":"Health is a higher level abstraction created on top of severities. One or more alarms can be combined to calculate one of the following healths at a given time.\nGood Ill Bad\nHealth is calculated based on severity. The current mapping between health and severities is shown in the figure below. Health can be for aggregated for a component, subsystem, or the entire TMT system.","title":"Health"},{"location":"/technical/alarm/alarm.html#severity-health-aggregations","text":"A severity can be associated with an alarm, a component or a sub-system.\nUnlike alarms, which has a direct association with severity, components and sub-systems don’t have a direct association with severities. They have aggregated severities based on severities of their children.\nA components aggregated severity is equal to severity of its child alarm with “maximum” severity. Similarly, a sub-system’s aggregated severity is determined by its child component with maximum severity.\nHealth aggregation works on top of severity aggregation. A components aggregated health is calculated by first calculating its aggregated severity and then mapping it to health. Sub-System health aggregation works in the same way.","title":"Severity & Health Aggregations"},{"location":"/technical/alarm/alarm.html#redis-storage","text":"","title":"Redis Storage"},{"location":"/technical/alarm/alarm.html#metadata","text":"Metadata of an alarm is static information about an alarm such as name, description, subsystem, component, etc. This information is not changed during runtime. Since the metadata is static, entire metadata of an alarm is stored against a single redis key in json format for easy retrieval. The static information is contained in a configuration file and loaded when Alarm Service starts up. The key name is formed with pattern: metadata-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME] e.g. metadata-nfiraos-trombone-tromboneaxislowlimitalarm.\nThe sample JSON below, shows structure of typical alarm metadata from the configuration file\n{\n   \"subsystem\":\"tcs\",\n   \"component\":\"corrections\",\n   \"name\":\"outOfRangeOffload\",\n   \"description\":\"Another system has sent an out of range offload that has caused the system to go into a bad state!\",\n   \"location\":\"Computer Room\",\n   \"alarmType\":\"absolute\",\n   \"supportedSeverities\":[\n      \"warning\",\n      \"major\",\n      \"indeterminate\",\n      \"okay\"\n   ],\n   \"probableCause\":\"Bad software in NFIRAOS or WFOS\",\n   \"operatorResponse\":\"Reset the software system and hope\",\n   \"isAutoAcknowledgeable\":false,\n   \"isLatchable\":true,\n   \"activationStatus\":\"active\"\n}","title":"Metadata"},{"location":"/technical/alarm/alarm.html#acknowledgement-status","text":"Indicates Acknowledgement status of an alarm. The key name is formed with pattern ackstatus-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME]. e.g. ackstatus-nfiraos-enclosure-temphighalarm\nPossible values of this key are:\nacknowledged unacknowledged","title":"Acknowledgement Status"},{"location":"/technical/alarm/alarm.html#severity","text":"Indicates the current severity of an alarm. The key name is formed with pattern severity-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME]. e.g. severity-nfiraos-trombone-tromboneaxislowlimitalarm\nThis key can contain one of the following values at a given time -\nokay warning major indeterminate critical\nIf the value is not present, it is considered to have disconnected severity.","title":"Severity"},{"location":"/technical/alarm/alarm.html#latched-severity","text":"Indicates the severity on which the current alarm is latched. A latched severity is the max severity of an alarm since last reset operation. The key name is formed with pattern latchedseverity-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME]. e.g. latchedseverity-tcs-tcspk-cpuexceededalarm\nPossible values of this key are:\nokay warning major indeterminate critical disconnected","title":"Latched Severity"},{"location":"/technical/alarm/alarm.html#shelve-status","text":"Indicates shelved status of an alarm. The key name is formed with pattern shelvestatus-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME]. e.g. shelvestatus-nfiraos-trombone-tromboneaxislowlimitalarm\nThis key can have one of the following values\nshelved unshelved\nIf no value is present, it is inferred as unshelved","title":"Shelve Status"},{"location":"/technical/alarm/alarm.html#alarm-time","text":"Indicates the time of the last severity change of an alarm. The key name is formed with pattern alarmtime-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME]. e.g. alarmtime-nfiraos-beamsplitter-splitterlimitalarm\nThe value is stored in the format: 2019-04-03T11:09:28.404143Z","title":"Alarm Time"},{"location":"/technical/alarm/alarm.html#initializing","text":"Indicates whether alarm is initializing or its initialization is finished. It contains a boolean value either true or false\nThe key name is formed with pattern initializing-[SUBSYSTEM_NAME]-[COMPONENT_NAME]-[ALARM_NAME]. e.g. initializing-lgsf-tcspkinactive-cpuidlealarm","title":"Initializing"},{"location":"/technical/alarm/alarm.html#alarm-disconnection","text":"All the severities except Disconnected, need to be set explicitly. Disconnected is a special severity in the sense that it can never be set by a component explicitly and is always inferred when there is no severity value has been set. Alarm service uses Heartbeat pattern.\nWhenever a component calls SetSeverity, severity get stored in redis for that alarm with certain TTL. This TTL is configurable. For our example, let’s assume it is 5 seconds. A TTL of 5 second means that if another call to SetSeverity is not made within 5 seconds, the current severity will get deleted from redis. Absence of severity is inferred as “Disconnected” severity.\nSo to avoid disconnection of an alarm, the component will need to ensure that a SetSeverity call (aka heartbeat) is made with appropriate severity at-least once in every 5 seconds.\nNote Actual TLL depends on refresh-interval & max-missed-refresh-counts configurations. For example, if refresh-interval is set to 3s (3 seconds) and max-missed-refresh-counts is set to 2, alarm will get disconnected after 6 seconds (3 * 2)\nUsing the heartbeat pattern allows us to detect dead or disconnected components automatically without polling them at regular intervals.","title":"Alarm Disconnection"},{"location":"/technical/alarm/alarm.html#severity-latching","text":"Alarm metadata JSON has a boolean attribute called “isLatchable”. This determines the latching behaviour of an alarm. If an alarm is latchable, its latched severity sticks to the last highest severity reached until the alarm is reset. Reset operation is provided the admin api of alarm service.\nFor, alarms which are not latchable, latched severity will always be equal to severity.\nSetting severity and latched severity both is done by SetSeverity api. As described in previous sections, when a component dies, it can’t call SetSeverity and the severity key in redis, expires. However the “Latched Severity” is still not updated; which is a problem. To solve this problem, the future Alarm Server subscribes to all severity changes (using akka stream api of alarm service) and whenever it detects a key has expired, it updates its respective “latchedSeverity” value in Redis to disconnected.","title":"Severity Latching"},{"location":"/technical/alarm/alarm.html#shelving-alarms","text":"Alarms can be shelved and un-shelved using the Alarm Service API. A shelved alarm will contribute to a subsystem’s aggregate severity and health. An alarm can only be shelved for a predefined time. The time can be configured in Alarm Service configuration. For example:\nshelve-timeout = \"8:00:00 AM\" // format -> h:m:s a\nWhen changing the shelve status to shelved it is set using setex operation of redis with an appropriate TTL so that it gets expired on next shelve-timeout. Once expired, it is inferred as unshelved.\nAlarms can also be un-shelved explicitly before next shelve-timeout occurs. unshelve api sets unshelved value in redis explicitly without any TTL.","title":"Shelving Alarms"},{"location":"/technical/alarm/alarm.html#acknowledging-alarms","text":"Alarms can be in either acknowledged or unacknowledged state. See Acknowledgement Status for more details. The state can be changed to acknowledged by simply using acknowledge api of alarm service. Apart from this api, setting severity can also change the Acknowledgement Status of an alarm.\nAlarm can either be auto-acknowledgeable or not auto-acknowledgeable. This behavior is driven from alarm metadata. When an alarm is not auto-acknowledgeable, whenever it’s severity changes to anything except Okay, it’s Acknowledgement Status becomes acknowledged. If it is changing from any severity to Okay, Acknowledgement Status remains same.\nWhen the alarm is auto-acknowledgeable, whenever it’s severity changes to Okay, it’s Acknowledgement Status becomes acknowledged. If it changes to anything else, Acknowledgement Status remains same.","title":"Acknowledging Alarms"},{"location":"/technical/alarm/alarm.html#api-and-implementation-structure","text":"The alarm functionality divided into two services AlarmService and AlarmAdminService\nAlarmService is meant for “Components” that can only set their current alarm severity and hence for components, Alarm Service only exposes one api i.e. setSeverity\nAlarmAdminService is meant for admin operations.\nThe Alarm Service implementation is further divided into four internal modules.\n1. SeverityServiceModule\nSeverityServiceModule allows reading, subscribing and modifying severity of alarms and subsystems.\n2. MetadataServiceModule\nMetadataServiceModule allows read-only access to alarm metadata. Initialisation of alarms is also performed using this module.\n3. StatusServiceModule\nStatusServiceModule allows read-write access to alarm status via operations such as getStatus, shelveAlarm, reset, acknowledge, unshelve, etc.\nAlarmStatus is a logical entity which contains:\nacknowledgementStatus latchedSeverity shelveStatus alarmTime initializing\n4. HealthServiceModule\nHealthServiceModule allows reading and subscribing to alarm and subsystem healths.\nNote These modules are interdependent on each other and use self-type feature of scala\nAlarmAdminService is consumed from alarm cli and Alarm Server. This API is also used by administrative clients such as the future Alarm Server.\nThe API doc for Alarm Service can be found here and here.\nDetailed documentation about how to use these APIs is available here.","title":"API and Implementation Structure"},{"location":"/technical/alarm/alarm.html#setting-severity-execution-workflow","text":"The following diagram show execution sequence triggered when a component calls setSeverityApi","title":"Setting severity execution workflow"},{"location":"/technical/alarm/alarm.html#architecture","text":"Important At the time of writing this documentation, the Alarm Server does not exist. It will be developed in future as part of ESW.HCMS. Its description and scope of work is subjected to change\nFor alarms to function, it is necessary that the Alarm Service’s Redis instance be registered with Location Service. Redis here is configured similar to other services in CSW. There is a master Redis instance and a slave Redis instance. They are configured in “replication” mode. There is a Sentinel instance who’s responsibility it is to promote the slave as master when master goes down. It is important to note that when master goes down, the “location” of Alarm Service remains the same because the location of Alarm Service is the location of Sentinel and not of master or slave. The master and slave Redis instances are dedicated for alarm, however Sentinel is shared across CSW Redis-based services.\nOnce location is registered, components, alarm CLI & Alarm Server can resolve Redis location and start interacting with it using the component alarm API & alarm admin API. While the interaction of components with Alarm Service is limited to the setSeverity API, Alarm CLI can perform all admin operations as discussed above.\nAlarm Server is part of ESW.HCMS and is not yet built. When built, its functionality will be to watch all severity changes using the admin API and latch appropriate alarms to disconnected severity. Apart from this, it will also be responsible for logging alarms and generating alarm events for archiving in DMS.ENG. It provides an HTTP interface for various UI layer ESW.HCMS applications for alarm and health visualisations.","title":"Architecture"},{"location":"/technical/time/time.html","text":"","title":"Time Service"},{"location":"/technical/time/time.html#time-service","text":"","title":"Time Service"},{"location":"/technical/time/time.html#introduction","text":"The time service implementation provides API for managing time across various components within the TMT architecture. TMT has standardised on the use of Precision Time Protocol (PTP), an IEEE 1588 standard, as the basis of observatory time to achieve sub-microsecond accuracy and precision. The PTP grand master clock (a hardware device) is synchronized to the Global Positioning System (GPS) time. Each computer system participating in the PTP network synchronizes to Observatory Time using the PTP protocol. For higher accuracy in time measurements, hardware time stamping is recommended and the network entities should be fitted with PTP capable Network Interface Cards (NIC).\nThe TMT time service relies on making native calls (Linux kernel C methods invocation) to get the nanosecond precise time overcoming the limitations of Scala and Java libraries which support only microsecond precision till date. These native calls are provided as wrapper over Java and Scala APIs for easy use by component developers familiar with Java/Scala. The TMT time service is responsible for primarily providing time in Coordinated Universal Time (UTC) and International Atomic Time (TAI) time scales. The handling of leap second event is taken care by the TMT time service automatically(without human intervention) as the PTP grandmaster distributes the accurate information when received through GPS.\nThe time service also allows for scheduling tasks either periodically or once using both UTC and TAI time. These schedulers are optimised for handling scheduled tasks at 1KHz frequency or 1 task every 1 millisecond. However, there can be jitters due to JVM garbage collection, CPU loads and concurrent task execution.","title":"Introduction"},{"location":"/technical/time/time.html#summary-of-relevant-time-metrics","text":"Second – A second is defined as 9,192,631,770 cycles of radiation corresponding to the transition between two hyperfine levels of the ground state of caesium 133 Leap Second – A second which is introduced to compensate for slight error in Earth’s rotation.\nTime zones TAI – International Atomic Time (Temps Atomique International) is defined as the weighted average of the time kept by about 200 atomic clocks in over 50 national laboratories worldwide. TAI was introduced in Jan 1, 1958. UT – Universal Time (UT1 or UT) is a time standard based on Earth’s rotation. It is a modern continuation of Greenwich Mean Time (GMT) i.e., mean solar time. UTC – Coordinated Universal Time (UTC) is the primary time standard by which the world regulates clocks and time. It is always within 1 second of UT. UTC differs from TAI by an integral number of seconds (currently 37). When needed, leap seconds are updated in UTC. UTC was introduced in 1972. GPS – Global Positioning System Time is a uniformly counting time scale beginning at 00:00 of January 6, 1980. No leap seconds are inserted into GPS time. GPS time is always 19 seconds behind TAI. Various clock quality factors determine the effectiveness of a time implementation.","title":"Summary of relevant time metrics"},{"location":"/technical/time/time.html#overview-of-ieee-1588","text":"The IEEE 1588 standard describes a protocol, PTP, used to synchronize clocks throughout a computer network using either UDP or TCP network packets. On an ethernet based local area network, it achieves clock accuracy in the sub-microsecond range, making it suitable for measurement and control systems. PTP is currently employed in diverse distributed domains and networks that require precise timing but lack access to satellite navigation signals. The recommended timescale of PTP is the same as TAI. PTP provides two mechanisms namely, hardware time stamping and software time stamping to achieve such a high accuracy.","title":"Overview of IEEE 1588"},{"location":"/technical/time/time.html#tmt-time-service-architecture","text":"The IEEE 1588 standards describe a hierarchical master-slave architecture for clock distribution. TMT Time service architecture follows the same topology as shown in the figure.\nUnder this architecture, a time distribution system consists of one or more network segments, and one or more clocks. The standard offers an automatic way for the configuration of the synchronization network, based on the nodes pre-programmed capabilities. A synchronization network consists one or more of the following entities:\nOrdinary Clock - An ordinary clock is a device with a single network connection and is either the source of (master) or destination for (slave) a synchronization reference. Grandmaster - The root timing reference of a distributed network is called the grandmaster. The grandmaster transmits synchronization information to the clocks residing on its network segment. Boundary Clock (Optional) - A boundary clock has multiple network connections and can accurately synchronize one network segment to another. Once a boundary clock residing in a network segment having a grandmaster is synchronized it can then relay accurate time to the other connected network segments. Transparent Clock (Optional) - The transparent clock modifies PTP messages as they pass through the device. Timestamps in the messages are corrected for time spent traversing the network equipment. This scheme improves distribution accuracy by compensating for delivery variability across the network. Refer this for more details on various clocks.\nFor TMT, a simplified PTP system consisting of ordinary clocks connected to a single network, and no boundary clocks can be used. A grandmaster is elected and all other clocks synchronize directly to it. The grandmaster is a special entity which is configured to receive the accurate GPS time information (TAI time, TAI-UTC offset etc.) through a TMT GPS receiver. A master clock has the capability to serve the other nodes in the network when a grandmaster has failed. Such a master clock is elected as a grandmaster using the Best Master Clock Algorithm. A clock in the network distribution can be ‘*slave only*’ which will enforce it to be a slave, always. In case a new network segment is introduced a boundary clock can be set up to synchronize with the TMT grandmaster time. Synchronization and management of a PTP system is achieved through the exchange of protocol messages across the network segments.","title":"TMT Time service architecture"},{"location":"/technical/time/time.html#internals-of-a-network-component","text":"In case of TMT most of the synchronization components will be based on Linux operating system. For understanding the internal working of Linux clock one can refer to this short description or follow ‘Chapter 6’ of the detailed document. The block diagram of an ordinary clock is shown in the following figure.\nA reference time is transmitted through the Ethernet LAN from a master/grandmaster clock. To achieve sub-microsecond accuracy and precision, the Network Interface Card (NIC) of the component must have the capability to perform hardware timestamping, and an internal PTP Hardware Clock(PHC). Here’s a list which mentions of such capable NICs while there are various new additions in the market. Most of the current NICs are able to handle PTP protocol by default. Run the following command to find the details of a NIC.\nsudo lshw -class network\nThe PHC API is an application programming interface which serves as a simplified tool to access and program the on–board clocks on the network adapters. This API is a part of the Linux kernel and synchronizes Linux clock with external time. The PTP Daemon protocol stack implements the specified rules of the synchronization by the IEEE 1588:2008 standard. The reliable and popular LinuxPTP project is utilised as the PTP Daemon for TMT since it meets the general requirements of supporting PTP synchronization across the network. LinuxPTP updates the PHC API to set the correct time on the Linux kernel as well as the system wall clock.\nNote The ports 319 and 320 should be open for PTP to transmit messages. Run the following commands to update iptables.  iptables -I INPUT -p udp -m udp --dport 319 -j ACCEPT\n iptables -I INPUT -p udp -m udp --dport 320 -j ACCEPT\n \n firewall-cmd --permanent --zone=public --add-port=319/udp\n firewall-cmd --permanent --zone=public --add-port=320/udp\nThe recommendation is to utilise the PTP Hardware Timestamping(available at the PHY layer in a network stack) mechanism for TMT to meet the nanosecond requirement. Various tests are performed and documented to support the sub-microsecond capability of IEEE 1588 standard using H/W timestamping. In contrast, the PTP software timestamping makes use of the application layer timestamp having a latency overhead.","title":"Internals of a Network Component"},{"location":"/technical/time/time.html#usage-of-ptp-time","text":"The PTP synchronization assures that the whole TMT system is following an accurate UTC/TAI time with high precision. All the applications running inside TMT should make use of the PTP time to perform time related operations like logging, scheduling etc.","title":"Usage of PTP time"},{"location":"/technical/time/time.html#extracting-time","text":"The CSW provides the time service APIs written in Scala and Java for the component developers. Once the Linux kernel is set with PTP time, these APIs use Java Native Access (JNA) based kernel calls to fetch the time information. The class TimeLibrary is responsible for loading the Linux kernel C libraries and declare the native methods relevant for extracting time information. The clock id determines the clock to fetch time.\nScala val ClockRealtime = 0  // system-wide realtime clock. Its time represents seconds and nanoseconds since the Epoch\nval ClockTAI      = 11 // It is basically defined as CLOCK_REALTIME(UTC) + tai_offset.\nThe following snippet defines the usage of JNA kernel call to fetch UTC/TAI time and tai_offset(kernel variable) in the Scala API.\nScala private def now(clockId: Int): Instant = {\n  val timeSpec = new TimeSpec()\n  TimeLibrary.clock_gettime(clockId, timeSpec)\n  Instant.ofEpochSecond(timeSpec.seconds.longValue(), timeSpec.nanoseconds.longValue())\n}\n\noverride def offset: Int = {\n  val timeVal = new NTPTimeVal()\n  TimeLibrary.ntp_gettimex(timeVal)\n  if (timeVal.tai == 0) printWarning()\n  timeVal.tai\n}\nNote Based on the comparison between JNA and JNI and stakeholder agreement, JNA was fixed to make the low level kernel calls.","title":"Extracting time"},{"location":"/technical/time/time.html#scheduling","text":"TMT time service provides scheduler for scheduling tasks once or periodically. The schedulers can be used to handle tasks at 1KHz frequency or 1 task every millisecond. However, jitters and JVM garbage collection can degrade the performance of the schedulers at such a high frequency. Internally, the scheduler APIs makes use of ‘Akka Scheduler Library’. The results published in here compares some of the options and makes strong recommendation to use the Akka Schedulers. The snippets in the API documentation defines the schedule APIs in the CSW library.\nOne of the major challenge in designing the scheduler was to handle the leap second event. The leap second plays an important role in affecting the UTC time. Therefore, a scheduler accepting future UTC time as a start time can behave unpredictably and less accurately. Time service scheduler APIs provide capability to schedule tasks on the absolute TAI time synchronized using the IEEE 1588 standard. This approach eliminates anomaly of a leap second event and provide robustness in the scheduling mechanism. Depending on developer’s choice, scheduled tasks are queued in either UTC or TAI time encapsulated inside TMTTime.","title":"Scheduling"},{"location":"/technical/database/database.html","text":"","title":"Database Service"},{"location":"/technical/database/database.html#database-service","text":"","title":"Database Service"},{"location":"/technical/database/database.html#introduction","text":"The Database Service is included in TMT Common Software for use by components that need the features of a relational database. CSW Database Service provides a TMT-standard relational database and connection library. Databases created by Database Service will be stored reliably at the site during operations.\nDatabase Service provides access to the TMT-standard PostgeSQL database server. It is a minimal service focused on providing access to the database server through Location Service and standardizing on access libraries and drivers. The user documentation here provides much of what is needed to understand the Database Service.","title":"Introduction"},{"location":"/technical/database/database.html#code-outline","text":"The Database Service code exists in the csw-database project within the CSW source code repository. Database Service consists of 8 files in 4 packages written entirely in Scala. The Database Service depends on the PostgreSQL driver and the Jooq library.\nThe following table provides an overview of the packages and files in the Database Service with links to the source code.\nPackage File Description commons DatabaseLogger An internal class to allow Database Service to log to Logging Service commons DatabaseServiceConnection DatabaseServiceConnection is a wrapper over predefined TcpConnection representing Database Service. It is used to resolve the Database Service location. commons DatabaseServiceLocationResolver Provides the connection information of Database Service by resolving the location. exception DatabaseException Represents an exception while connecting to the database server e.g. in case of providing incorrect username or password. javadsl AsyncHelper A Java helper to schedule and execute blocking operations on a dedicated thread pool. This mechanism will prevent any blocking operation to be scheduled on a thread pool designated for async operations. javadsl JooqHelper A Java helper wrapping some of the Jooq operations. scaladsl JooqExtensions A Scala extension, extending few of the Jooq operations. DatabaseServiceFactory DatabaseServiceFactory provides a mechanism to connect to the database server and get the handle of Jooq’s DSLContext.\nFor all other Database Service information see the user documentation.","title":"Code Outline"},{"location":"/technical/aas/aas.html","text":"","title":"Authentication and Authorization Service"},{"location":"/technical/aas/aas.html#authentication-and-authorization-service","text":"","title":"Authentication and Authorization Service"},{"location":"/technical/aas/aas.html#introduction","text":"Authentication and Authorization Service provides suit of libraries which help to enforce authentication and authorization for web applications and cli applications in TMT ecosystem.\nIt provides following libraries (aka adapters). For information on using these adapters, refer to following sections:\nAkka HTTP Adapter - csw-aas-http - This is adapter which enables security for akka http server applications Installed Auth Adapter - csw-aas-installed - This is adapter which enables security for cli applications Javascript Adapter - csw-aas-js - This is adapter which enables security for javascript react applications\nThese adapters are written on top of client adapters provided by keycloak.\nFor more information on internals of these adapters, refer to following sections:\nAkka HTTP Adapter - csw-aas-http Installed Auth Adapter - csw-aas-installed Javascript Adapter - csw-aas-js","title":"Introduction"},{"location":"/technical/aas/csw-aas-installed.html","text":"","title":"csw-aas-http - Installed App Auth Adapter"},{"location":"/technical/aas/csw-aas-installed.html#csw-aas-http-installed-app-auth-adapter","text":"Installed App Auth Adapter is library provided to support authentication and authorisation in native applications. It provides methods to login, getTokens, logout. It is mainly used in Config service cli.\nIn order for native app to utilize installed app auth adapter, it has to be registered as keycloak client. After registering to keycloak, all client are given client secrets which needed for client verification. Please refer to keycloak documentation for details.","title":"csw-aas-http - Installed App Auth Adapter"},{"location":"/technical/aas/csw-aas-installed.html#technology","text":"Keycloak comes with its own adapter for native applications. Csw InstalledAppAuthAdapter is wrapper on top of keycloak provided adapter with added support for local storage for tokens.","title":"Technology"},{"location":"/technical/aas/csw-aas-installed.html#login-flow","text":"When login method of the adapter is called, then following steps are performed.\nIt opens login page in default browser to accept the credentials of the user. As the user enters credentials, it sends a request to the keycloak server. For valid user credentials, keycloak returns a Verification code to the browser. Verification code is passed back to native app. Native app sends verification code and client secrets it to keycloak to get the Access token and Refresh token . Keycloak verifies the client secrets and and verification code and gives back access and refresh token. Adapter saves the received tokens into local store.","title":"Login flow"},{"location":"/technical/aas/csw-aas-installed.html#using-tokens","text":"While sending request to server following stops are performed.\nNative app retrieves the access token from local store and sends it along with the request. Server verifies the token and user permissions. For this verification, server gets RPT (requesting party token) from keycloak which have all the information about the user roles and permissions. User with right permissions are allowed to perform action otherwise request is rejected with appropriate response code.","title":"Using Tokens"},{"location":"/technical/aas/csw-aas-installed.html#logout-flow","text":"When logout method of the adapter is called, then following steps are performed.\nIt removes tokens from the local store. It removes the user session from keycloak.\nFollowing diagram shows the flow of the Installed App Auth Adapter.","title":"Logout flow"},{"location":"/technical/aas/csw-aas-http.html","text":"","title":"csw-aas-http - Akka HTTP Adapter"},{"location":"/technical/aas/csw-aas-http.html#csw-aas-http-akka-http-adapter","text":"This is security adapter for akka http server applications. It exposes security directives e.g. sGet, sPost etc which enforce authentication and authorization based on authorization policies.\nIn order for akka http server to utilize keycloak it has to be registered as keycloak client. Please refer to keycloak documentation for details.","title":"csw-aas-http - Akka HTTP Adapter"},{"location":"/technical/aas/csw-aas-http.html#types-of-tokens","text":"","title":"Types of tokens"},{"location":"/technical/aas/csw-aas-http.html#id-token-","text":"The ID Token is a JSON Web Token (JWT) that contains user profile information (such as the user’s name and email) which is represented in the form of claims.","title":"ID Token -"},{"location":"/technical/aas/csw-aas-http.html#access-token-","text":"An Access Token is a credential that can be used by an application to access an API. Access Tokens is JSON web token digitally signed using JSON web signature (JWS). They inform the API that the bearer of the token has been authorized to access the API and perform specific actions specified by the scope that has been granted. Access token contains all the information that ID token has. Additionally it has information related to realm and resource roles associated to user. This information is used for authorization based on clientRole and RealmRole policy.","title":"Access Token -"},{"location":"/technical/aas/csw-aas-http.html#requesting-party-token-","text":"A requesting party token (RPT) is a JSON web token (JWT) digitally signed using JSON web signature (JWS). The token is built based on the OAuth2 access token previously issued by Keycloak to a specific client acting on behalf of a user or on its own behalf. RPT contains all the information which access token has, additionally it has information related to permissions granted. permissions are directly related with the resources/scopes you are protecting. This is used for authorization based on permission policy.","title":"Requesting Party Token -"},{"location":"/technical/aas/csw-aas-http.html#request-flow","text":"When request comes to secure akka http server, it performs following steps.\nAuthentication\nAuthentication involves token verification and decoding. Secure http endpoints expect access token to be provided in request header. For verification it uses api provided by keycloak-adapter-core. For decoding csw-aas-http uses third party library - jwt-play-json. If “enable-permissions” flag is enabled, it involves additional call to keycloak server for fetching RPT using access token provided in request header. RPT is then decoded using jwt-play-json. Authentication process verifies access token string and decode it into AccessToken domain model.\nAuthorization\nAuthorization involves applying specified AuthorizationPolicy against AccessToken. Foe example, role based authorization involves checking access to secure api against roles information present in access token. Permission based authorization involves checking access to secure api against permissions present in RPT.\nFollowing diagrams shows request flow for secure akka http server.\nThe core of this adapter is the SecurityDirectives class. Following diagram shows request flow through different core classes of this adapter. SecurityDirecives provided by this adapter are used while writing akka http server. For detailing following diagram shows akka http server and security directives as separate. Diagram depicts happy flow of POST request secured using realm role based authorization policy where server returns 200 OK code. When authentication and authorization is successful, application logic is executed and 200 OK is returned by server. e.g. create config file if user is authenticated and have admin role. In case of errors, server return 401 (unauthorized) or 403 (forbidden) e.g. - If token verification fails for invalid or expired token, TokenVerifier returns appropriate TokenVerificationFailure and akka http server returns 401. If authorization policy check fails then AccessToken returns false and then akka http server return 403.","title":"Request flow"},{"location":"/technical/aas/csw-aas-http.html#asynchronous-nature-of-akka-http-routing-layer","text":"csw-aas-http uses authenticateOAuth2Async and authorizeAsync which are async variants of akka-http security directives. This allows it to run without blocking routing layer of Akka HTTP, freeing it for other requests. Similarly to maintain asynchronous nature csw-aas-http also wraps blocking calls for keycloak adapter token verifier and call for fetching RPT from keycloak inside Future.","title":"Asynchronous nature of Akka-HTTP Routing layer"},{"location":"/technical/location/location-client.html","text":"","title":"Location Client"},{"location":"/technical/location/location-client.html#location-client","text":"The csw-location-client project provides a convenient lightweight wrapper for accessing the CSW Location HTTP Server.\nLocation service client is build using Akka Http’s request level client API.\nNote Lifecycle of all the components registered using location service client is tied up with the actor system used to create client. These components gets unregistered when this actor system is terminated.\nThe client API implements the same LocationService trait as the server API. The core implementation is in the LocationServiceClient class, which can be conveniently instantiated via the HttpLocationServiceFactory class.","title":"Location Client"},{"location":"/technical/location/location-client.html#java-api","text":"The Java location client API is implemented in Scala as a thin wrapper class: JHttpLocationServiceFactory. It delegates to the private class JLocationServiceImpl, which converts the returned Future and Option types to their Java equivalent.","title":"Java API"},{"location":"/technical/location/location-client.html#tests","text":"There are only a few tests in the csw-location-client project. Most of the features are actually tested in the csw-location-server project.","title":"Tests"},{"location":"/services/aas/core-concepts-and-terms.html","text":"","title":"Core concepts and terms"},{"location":"/services/aas/core-concepts-and-terms.html#core-concepts-and-terms","text":"These are some common terms used in AAS. More information is available in the Keycloak documentation","title":"Core concepts and terms"},{"location":"/services/aas/core-concepts-and-terms.html#users","text":"Users are entities that are able to log into your system. They can have attributes associated with themselves like email, username, address, phone number, and birthday. They can be assigned group membership and have specific roles assigned to them.","title":"Users"},{"location":"/services/aas/core-concepts-and-terms.html#roles","text":"Roles identify a type or category of user. Admin, user, manager, and employee are all typical roles that may exist in an organization. Applications often assign access to specific roles rather than individual users as dealing with users can be too fine grained and hard to manage.","title":"Roles"},{"location":"/services/aas/core-concepts-and-terms.html#permissions","text":"Permissions are another way of specifying access in the form of a Scope and a Resource. For example, “scope: Sell; resource: Vehicle” combined specifies that the user with this scope and resource combination can “sell vehicles”.","title":"Permissions"},{"location":"/services/aas/core-concepts-and-terms.html#realms","text":"A realm manages a set of users, credentials, roles, and groups. A user belongs to and logs into a realm. Realms are isolated from one another and can only manage and authenticate the users that they control.","title":"Realms"},{"location":"/services/aas/core-concepts-and-terms.html#clients","text":"Clients are entities that can request AAS to authenticate a user. Most often, clients are applications and services that want to use AAS to secure themselves and provide a single sign-on solution. Clients can also be entities that just want to request identity information or an access token so that they can securely invoke other services on the network that are secured by AAS.","title":"Clients"},{"location":"/services/aas/core-concepts-and-terms.html#client-roles","text":"Clients can define roles that are specific to them. This is basically a role namespace dedicated to the client.","title":"Client Roles"},{"location":"/technical/location/location-api.html","text":"","title":"Location API"},{"location":"/technical/location/location-api.html#location-api","text":"The csw-location-api project provides a common API for the the csw-location-server and csw-location-client implementations.\nThis also includes the model classes used for connections, registration and component types as well as the TrackingEvent class that clients receive whenever new location information is received.","title":"Location API"},{"location":"/technical/location/location-api.html#java-api","text":"The Java API is defined partly in Scala code with traits like ILocationService, and partly in Java, where classes like JComponentType are needed to provide easy access to Scala objects constants.","title":"Java API"}]}